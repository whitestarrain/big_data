# 1. å¼€å§‹

## 1.1. ç›¸å…³

- å‘å±•ï¼š
  - äººå·¥æ™ºèƒ½ï¼šæœ€æ—©å‡ºç°çš„æ¦‚å¿µï¼ŒæŒ‡ä¸€äº›è‡ªåŠ¨åŒ–çš„ç¨‹åº
  - æœºå™¨å­¦ä¹ ï¼šå¯¹æ•°æ®è¿›è¡Œè‡ªåŠ¨åˆ†æè·å¾—è§„å¾‹
  - æ·±åº¦å­¦ä¹ ï¼šæœºå™¨å­¦ä¹ ä¸­è¾ƒä¸ºå¤æ‚çš„ç®—æ³•ï¼Œæ¯”å¦‚å›¾åƒè¯†åˆ«ç­‰
    > ![](./image/history.jpg)
- å½±å“å› ç´ ï¼›
  - ç¡¬ä»¶è®¡ç®—æ°´å¹³
  - æ•°æ®é‡å¤šå°‘
  - ç®—æ³•å‘å±•
- åº”ç”¨ï¼š

  - è‡ªç„¶è¯­è¨€å¤„ç†
  - å›¾åƒè¯†åˆ«
  - ä¼ ç»Ÿé¢„æµ‹

- æœºå™¨å­¦ä¹ åº“å’Œæ¡†æ¶

  - å‰é¢éƒ¨åˆ†ä½¿ç”¨ï¼šscikit learn
  - åé¢éƒ¨åˆ†ä½¿ç”¨ï¼štensorflow(ç”¨çš„æœ€ç«)
  - æ¡†æ¶åˆ—è¡¨ï¼š
    > ![](./image/ML_frame.jpg)

- ä¹¦ç±æ¨è

  > ![](./image/books.jpg)

- æ•°æ®é›†ï¼š

  > ![](./image/data-set.jpg)

- æ•°æ®é›†ç»“æ„ï¼š

  - ç‰¹å¾å€¼ï¼Œå³ x
  - ç›®æ ‡å€¼ï¼Œå³ y

- åŸºæœ¬æµç¨‹ï¼š
  > ![](./image/Basic_process.jpg)


## 1.2. è¯¯å·®æ¥æº

çœ‹é‚£ä¸ªå°æ¹¾çš„é‚£ä¸ª

ä¸€ä¸ªé›†æˆæ¨¡å‹(f)åœ¨æœªçŸ¥æ•°æ®é›†(D)ä¸Šçš„æ³›åŒ–è¯¯å·®E(f;D)ï¼Œç”±æ–¹å·®(var)ï¼Œåå·®(bais)å’Œå™ªå£°(Îµ)å…±åŒå†³å®šã€‚

![](./image/wucha.png)

![](./image/model-1.1.png)


# 2. scikit-learn

## 2.1. ä¼˜ç¼ºç‚¹

- sklearn
  - æœ‰ç‚¹ï¼šå°è£…å¥½ï¼Œå»ºç«‹æ¨¡å‹ç®€å•ï¼Œé¢„æµ‹ç®€å•
  - ç¼ºç‚¹ï¼šç®—æ³•çš„è¿‡ç¨‹ï¼Œéƒ¨åˆ†å‚æ•°éƒ½åœ¨å†…éƒ¨è‡ªè¡Œä¼˜åŒ–
  - æ³¨æ„ï¼šsklearnå½“ä¸­ç‰¹å¾çŸ©é˜µå¿…é¡»æ˜¯äºŒç»´
- tensorflow:å°è£…é«˜ä½çš„apiéƒ½æœ‰ï¼Œå¯ä»¥è‡ªå·±å®ç°ç®—æ³•

## 2.2. æ•°æ®æŠ½å–

### 2.2.1. å­—å…¸æ•°æ®æŠ½å–

#### 2.2.1.1. DictVectorizer

å°†å­—å…¸è½¬æ¢ä¸ºçŸ©é˜µ

```py
# å¯¼åŒ…
from sklearn.feature_extraction import DictVectorizer

def main():
    # å®ä¾‹åŒ–æ•°æ®æŠ½å–å¯¹è±¡
    # é»˜è®¤sparseä¸ºtrueï¼Œä¼šå°†çŸ©é˜µè½¬æ¢ä¸ºsparse
    dictVector = DictVectorizer(sparse=False)

    # è°ƒç”¨fit_transform
    rdata=dictVector.fit_transform(
        [
          {
              "name": 1,
              "value": "value1"
          },
          {
              "name": 2,
              "value": "value2"
          },
          {
              "name": 3,
              "value": "value3"
          }
        ]
    )
    print(dictVector.get_feature_names())
    print((rdata))
    print(type(rdata))
    """
    è¾“å‡ºï¼š
    ['name', 'value=value1', 'value=value2', 'value=value3']
    [[1. 1. 0. 0.]
    [2. 0. 1. 0.]
    [3. 0. 0. 1.]]
    <class 'numpy.ndarray'>

    æ•°å­—ä¸ºå€¼çš„ç¬¬ä¸€åˆ—ä¸ºname;å­—ç¬¦ä¸²ä¸ºå€¼çš„valueä¼šè½¬æ¢ä¸ºç¬¬äºŒï¼Œä¸‰ï¼Œå››åˆ—ï¼Œtrueä¸º1ï¼Œfalseä¸º0(one-hotç¼–ç )
    é»˜è®¤ä¼šé€šè¿‡sparseå‹ç¼©ï¼Œæ‰€ä»¥ä¸å¿…æ‹…å¿ƒå†…å­˜

    sparseçŸ©é˜µï¼š
      (0, 0)        1.0
      (0, 1)        1.0
      (1, 0)        2.0
      (1, 2)        1.0
      (2, 0)        3.0
      (2, 3)        1.0
    """

if __name__ == "__main__":
    main()

```

### 2.2.2. æ–‡æœ¬ç‰¹å¾æŠ½å–

#### 2.2.2.1. Count

> ç»Ÿè®¡å•è¯å‡ºç°é¢‘ç‡

```py
# å¯¼å…¥ç‰¹å¾æŠ½å–ç±»
from sklearn.feature_extraction.text import CountVectorizer

def main():
    cv = CountVectorizer()
    # å®ä¾‹åŒ–
    rdata = cv.fit_transform(
        ["life is short,I use python", "life is too long,i don't use python"])
        # ä¼ å…¥ä¸¤ç¯‡æ–‡ç« 
    print(rdata)
    print(cv.get_feature_names())
    print((rdata.toarray()))
    pass

if __name__ == "__main__":
    main()
"""
ç»“æœï¼š
  (0, 2)        1
  (0, 1)        1
  (0, 5)        1
  (0, 7)        1
  (0, 4)        1
  (1, 2)        1
  (1, 1)        1
  (1, 7)        1
  (1, 4)        1
  (1, 6)        1
  (1, 3)        1
  (1, 0)        1
['don', 'is', 'life', 'long', 'python', 'short', 'too', 'use']
[[0 1 1 0 1 1 0 1]
 [1 1 1 1 1 0 1 1]]

1. ç»Ÿè®¡æ–‡ç« ä¸­æ‰€æœ‰å‡ºç°çš„è¯ï¼Œé‡å¤è¯ä¹‹çœ‹ä½œä¸€æ¬¡ï¼Œä½œä¸ºfeature_names
2. ä¸€è¡Œå¯¹åº”ä¸€ç¯‡æ–‡ç« ï¼Œå¯¹æ¯ç¯‡æ–‡ç« ï¼Œç»Ÿè®¡å‡ºè¯çš„å‡ºç°æ¬¡æ•°
3. å•ä¸ªå­—æ¯ä¸ç»Ÿè®¡
4. ä¸­æ–‡é»˜è®¤ä¸æ”¯æŒç‰¹å¾æŠ½å–ï¼Œå› ä¸ºä¸èƒ½è‡ªåŠ¨åˆ†è¯ï¼Œé™¤éç”¨ç©ºæ ¼åˆ’å¼€ã€‚å¯ä»¥ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯
"""
```

#### 2.2.2.2. tf-idf

> å·²è¿‡æ—¶

- tf(term frequency):å•è¯é¢‘ç‡
- idf(inverse document frequency):é€†æ–‡æ¡£é¢‘ç‡

  > `log(æ€»æ–‡æ¡£æ•°é‡/æ”¹è¯å‡ºç°çš„æ–‡æ¡£æ•°+1)`<br>
  > æ¯”å¦‚`æˆ‘ä»¬`ï¼Œ`ä»Šå¤©`è¿™äº›å¸¸ç”¨è¯ä¸åº”è¯¥è®¡å…¥è€ƒè™‘ï¼Œå¯ä»¥é€šè¿‡é€†æ–‡æ¡£é¢‘ç‡åˆ¤æ–­é‡è¦æ€§

- è®¡ç®—å…¬å¼
  > ![](./image/tf-idf.jpg)

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def main():
    str1 = "1ã€ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œ ä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚"
    str2 = "2ã€æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œ è¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚"
    str3 = "3ã€å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚ äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"
    str1_cut = jieba.cut(str1)
    str2_cut = jieba.cut(str2)
    str3_cut = jieba.cut(str3)
    str1_s = " ".join(str1_cut)
    str2_s = " ".join(str2_cut)
    str3_s = " ".join(str3_cut)
    tifid = TfidfVectorizer()
    rdata = tifid.fit_transform([str1_s, str2_s, str3_s])
    print(tifid.get_feature_names())
    print(rdata.toarray())
    print(rdata)

if __name__ == "__main__":
    main()

"""
['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿™æ ·']
[[0.         0.         0.21821789 0.         0.         0.
  0.43643578 0.         0.         0.         0.         0.
  0.21821789 0.         0.21821789 0.         0.         0.
  0.         0.21821789 0.21821789 0.         0.43643578 0.
  0.21821789 0.         0.43643578 0.21821789 0.         0.
  0.         0.21821789 0.21821789 0.         0.         0.        ]
 [0.         0.         0.         0.2410822  0.         0.
  0.         0.2410822  0.2410822  0.2410822  0.         0.
  0.         0.         0.         0.         0.         0.2410822
  0.55004769 0.         0.         0.         0.         0.2410822
  0.         0.         0.         0.         0.48216441 0.
  0.         0.         0.         0.         0.2410822  0.2410822 ]
 [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891
  0.         0.         0.         0.         0.15698297 0.15698297
  0.         0.15698297 0.         0.15698297 0.15698297 0.
  0.1193896  0.         0.         0.15698297 0.         0.
  0.         0.15698297 0.         0.         0.         0.31396594
  0.15698297 0.         0.         0.15698297 0.         0.        ]]

  # ä¸€ä¸ªæ•°ç»„å¯¹åº”ä¸€ç¯‡æ–‡ç« ï¼Œæ•°å¤§å°å¯¹åº”è¯åœ¨è¯¥ç¯‡æ–‡ç« ä¸­çš„çš„é‡è¦æ€§
"""
```




## 2.3. æ•°æ®é¢„å¤„ç†

### 2.3.1. æ¦‚è¿°

- å®šä¹‰ï¼šé€šè¿‡ç‰¹å®šçš„ç»Ÿè®¡æ–¹æ³•ï¼ˆæ•°å­¦æ–¹æ³•ï¼‰å°†æ•°æ®è½¬æ¢æˆç®—æ³•è¦æ±‚çš„æ•°æ®

- å¤„ç†ï¼š

  - æ•°å€¼å‹æ•°æ®ï¼šæ ‡å‡†ç¼©æ”¾ï¼š
    - 1ã€å½’ä¸€åŒ–
    - 2ã€æ ‡å‡†åŒ–
    - 3ã€ç¼ºå¤±å€¼(pandas å’Œ numpy éƒ½å¯ä»¥è¿›è¡Œå¤„ç†)
  - ç±»åˆ«å‹æ•°æ®ï¼šone-hot ç¼–ç  ï¼ˆå­—å…¸ç±»å‹æ•°æ®æŠ½å–ï¼‰
  - æ—¶é—´ç±»å‹ï¼šæ—¶é—´çš„åˆ‡åˆ†(æ ¹æ®æƒ…å†µå¤„ç†å­—ç¬¦ä¸²)

- apiï¼š`sklearn.preprocessing`

### 2.3.2. å½’ä¸€åŒ– Normalization

> ä¸æ€ä¹ˆä½¿ç”¨

- ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®æ˜ å°„åˆ°ä¸€å®šèŒƒå›´(é»˜è®¤ä¸º[0,1])ä¹‹é—´
- ç›®çš„ï¼šé¿å…å› æ•°å€¼è€Œå½±å“ç‰¹å¾çš„ã€‚
  > æ¯”å¦‚èº«é«˜å’Œä½“é‡ ä¸ å¥åº·ç¨‹åº¦çš„å…³ç³»ï¼Œå› ä¸ºæœ¬èº«å°±ä¸æ˜¯åŒä¸€èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨çš„è¯ï¼Œæ‰€å æƒä¹Ÿä¸åŒ
- ç¼ºç‚¹ï¼šå—å¼‚å¸¸ç‚¹å½±å“æ‰“ï¼ˆç‰¹åˆ«æ˜¯æœ€å¤§æœ€å°å€¼å—å½±å“æ—¶ï¼‰ï¼Œé²æ£’æ€§è¾ƒå·®ã€‚**åªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®**

  - é²æ£’æ€§ Robustnessï¼Œä¹Ÿå°±æ˜¯å¥å£®æ€§

- å…¬å¼ï¼š

  > ![](./image/Normalization.jpg)

- api:`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - MinMaxScalar.fit_transform(X)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array

```py
from sklearn.preprocessing import MinMaxScaler

def main():
    mm = MinMaxScaler()
    # é»˜è®¤å°±æ˜¯ mm = MinMaxScaler(feature_range=(0,1))
    rdata = mm.fit_transform( [[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])
    print(mm.feature_range)
    print(rdata)
    pass

if __name__ == "__main__":
    main()
```

### 2.3.3. æ ‡å‡†åŒ– standardization

- ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º 0,æ–¹å·®ä¸º 1 èŒƒå›´å†…
  > åœ¨å·²æœ‰æ ·æœ¬è¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹æ¯”è¾ƒç¨³å®šï¼Œé€‚åˆç°ä»£å˜ˆæ‚å¤§æ•°æ®åœºæ™¯
- å…¬å¼ï¼š

  > ![](./image/standardization.jpg)

- api:`sklearn.preprocessing.StandardScaler`
  - å¤„ç†ä¹‹åæ¯åˆ—æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨
    - å‡å€¼:0 é™„è¿‘
    - æ–¹å·®ä¸º 1
  - StandardScaler.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array
  - StandardScaler.mean\_
    - åŸå§‹æ•°æ®ä¸­æ¯åˆ—ç‰¹å¾çš„å¹³å‡å€¼
  - StandardScaler.std\_
    - åŸå§‹æ•°æ®æ¯åˆ—ç‰¹å¾çš„æ–¹å·®

```py
from sklearn.preprocessing import StandardScaler

def main():
    stdv = StandardScaler()
    rdata = stdv.fit_transform( [[ 1., -1., 3.], [ 2., 4., 2.], [ 4., 6., -1.]])
    print(rdata)

if __name__ == "__main__":
    main()
```

```
å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ï¼Œä¼šé€‰æ‹©StandardScaleræ¥è¿›è¡Œç‰¹å¾ç¼©æ”¾ï¼Œ
å› ä¸ºMinMaxScalerå¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿã€‚åœ¨PCAï¼Œèšç±»ï¼Œé€»è¾‘å›å½’ï¼Œæ”¯æŒå‘é‡æœºï¼Œ
ç¥ç»ç½‘ç»œè¿™äº›ç®—æ³•ä¸­ï¼ŒStandardScalerå¾€å¾€æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚
MinMaxScaleråœ¨ä¸æ¶‰åŠè·ç¦»åº¦é‡ã€æ¢¯åº¦ã€åæ–¹å·®è®¡ç®—ä»¥åŠæ•°æ®éœ€è¦è¢«å‹ç¼©åˆ°ç‰¹å®šåŒºé—´æ—¶ä½¿ç”¨å¹¿æ³›ï¼Œ
æ¯”å¦‚æ•°å­—å›¾åƒå¤„ç†ä¸­é‡åŒ–åƒç´ å¼ºåº¦æ—¶ï¼Œéƒ½ä¼šä½¿ç”¨MinMaxScalerå°†æ•°æ®å‹ç¼©äº[0,1]åŒºé—´ä¹‹ä¸­ã€‚
å»ºè®®å…ˆè¯•è¯•çœ‹StandardScalerï¼Œæ•ˆæœä¸å¥½æ¢MinMaxScaler
```



### 2.3.4. ç¼ºå¤±å€¼å¤„ç†

> å•çº¯ç”¨ numpy å’Œ pandas å°±èƒ½å®Œæˆ

- æ’è¡¥ï¼šå¯ä»¥é€šè¿‡ç¼ºå¤±å€¼æ¯è¡Œæˆ–è€…æ¯åˆ—çš„å¹³å‡å€¼ã€ä¸­ä½æ•°æ¥å¡«å……
- api:`sklearn.preprocessing.Imputer`
  - Imputer(missing_values='NaN',Â strategy='mean',Â axis=0)
    - å®Œæˆç¼ºå¤±å€¼æ’è¡¥
    - strategy:
      - mean:é»˜è®¤å‡å€¼
      - median:ä¸­ä½æ•°
      - constantï¼š0
      - most_frequent:ä¼—æ•°
    - fill_valueï¼šå½“å‚æ•°startegyä¸ºâ€constant"çš„æ—¶å€™å¯ç”¨ï¼Œå¯è¾“å…¥å­—ç¬¦ä¸²æˆ–æ•°å­—è¡¨ç¤ºè¦å¡«å……çš„å€¼ï¼Œå¸¸ç”¨0
    - copy=true: é»˜è®¤ä¸ºTrueï¼Œå°†åˆ›å»ºç‰¹å¾çŸ©é˜µçš„å‰¯æœ¬ï¼Œåä¹‹åˆ™ä¼šå°†ç¼ºå¤±å€¼å¡«è¡¥åˆ°åŸæœ¬çš„ç‰¹å¾çŸ©é˜µä¸­

  - Imputer.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array

> **Imputerç±»å·²ç»è¿‡æ—¶ï¼Œ0.22ç‰ˆæœ¬ä¼šè¢«ç§»é™¤ï¼Œä¸‹é¢ä½¿ç”¨äº†sklearn.impute.SimpleImputeræ›¿æ¢äº†åŸä»£ç ä¸­çš„Imputer**


### 2.3.5. æ•°å€¼å‹-->æ•°å­—å‹

- sklearn.preprocessing.LabelEncoderï¼šæ ‡ç­¾ä¸“ç”¨ï¼Œèƒ½å¤Ÿå°†åˆ†ç±»è½¬æ¢ä¸ºåˆ†ç±»æ•°å€¼
  - inverse_transform():å¤åŸ
- sklearn.preprocessing.OrdinalEncoderï¼šç‰¹å¾ä¸“ç”¨ï¼Œèƒ½å¤Ÿå°†åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºåˆ†ç±»æ•°å€¼
  > OrdinalEncoderå¯ä»¥ç”¨æ¥å¤„ç†æœ‰åºå˜é‡ï¼Œä½†å¯¹äºåä¹‰å˜é‡ï¼Œæˆ‘ä»¬åªæœ‰ä½¿ç”¨å“‘å˜é‡çš„æ–¹å¼æ¥å¤„ç†ï¼Œæ‰èƒ½å¤Ÿå°½é‡å‘ç®—æ³•ä¼ è¾¾æœ€å‡†ç¡®çš„ä¿¡æ¯
- sklearn.preprocessing.OneHotEncoderï¼šç‹¬çƒ­ç¼–ç ï¼Œåˆ›å»ºå“‘å˜é‡**é‡è¦â€»**
  - categories='auto'ï¼šè‡ªåŠ¨åˆ†ç»„
- sklearn.preprocessing.Binarizer:æ ¹æ®é˜ˆå€¼å°†æ•°æ®äºŒå€¼åŒ–ï¼ˆå°†ç‰¹å¾å€¼è®¾ç½®ä¸º0æˆ–1ï¼‰
  - å¤§äºé˜ˆå€¼çš„å€¼æ˜ å°„ä¸º1ï¼Œè€Œå°äºæˆ–ç­‰äºé˜ˆå€¼çš„å€¼æ˜ å°„ä¸º0ã€‚
  - é»˜è®¤é˜ˆå€¼ä¸º0æ—¶ï¼Œç‰¹å¾ä¸­æ‰€æœ‰çš„æ­£å€¼éƒ½æ˜ å°„åˆ°1
- sklearn.preprocessing.KBinsDiscretizer:å¯ä»¥å°†è¿ç»­å‹å˜é‡åˆ’åˆ†ä¸ºå¤šä¸ªåˆ†ç±»å˜é‡çš„ç±»ï¼Œèƒ½å¤Ÿå°†è¿ç»­å‹å˜é‡æ’åºåæŒ‰é¡ºåºåˆ†ç®±åç¼–ç 
  - æ¯”å¦‚å¯¹å¹´é¾„è¿›è¡Œç‹¬çƒ­ç­‰å®½åˆ†ç®±
    - "uniform"ï¼šè¡¨ç¤ºç­‰å®½åˆ†ç®±ï¼Œå³æ¯ä¸ªç‰¹å¾ä¸­çš„æ¯ä¸ªç®±çš„æœ€å¤§å€¼ä¹‹é—´çš„å·®ä¸º(ç‰¹å¾.max() - ç‰¹å¾.min())/(n_bins)
    - "quantile"ï¼šè¡¨ç¤ºç­‰ä½åˆ†ç®±ï¼Œå³æ¯ä¸ªç‰¹å¾ä¸­çš„æ¯ä¸ªç®±å†…çš„æ ·æœ¬æ•°é‡éƒ½ç›¸åŒ
    - "kmeans"ï¼šè¡¨ç¤ºæŒ‰èšç±»åˆ†ç®±ï¼Œæ¯ä¸ªç®±ä¸­çš„å€¼åˆ°æœ€è¿‘çš„ä¸€ç»´kå‡å€¼èšç±»çš„ç°‡å¿ƒå¾—è·ç¦»éƒ½ç›¸åŒ

## 2.4. ç‰¹å¾å·¥ç¨‹

- ç»´åº¦ï¼šç‰¹å¾çš„æ•°é‡

### 2.4.1. ç‰¹å¾é€‰æ‹©

#### 2.4.1.1. è¯´æ˜

- åŸå› ï¼š

  - å†—ä½™ï¼šéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆè€—è®¡ç®—æ€§èƒ½
  - å™ªå£°ï¼šéƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“

- å®šä¹‰ï¼š
  ```
  ç‰¹å¾é€‰æ‹©å°±æ˜¯å•çº¯åœ°ä»æå–åˆ°çš„æ‰€æœ‰ç‰¹å¾ä¸­é€‰æ‹©éƒ¨åˆ†ç‰¹å¾ä½œä¸ºè®­ç»ƒé›†ç‰¹å¾ï¼Œ
  ç‰¹å¾åœ¨é€‰æ‹©å‰å’Œé€‰æ‹©åå¯ä»¥æ”¹å˜å€¼ã€ä¹Ÿä¸æ”¹å˜å€¼ï¼Œä½†æ˜¯é€‰æ‹©åçš„ç‰¹å¾ç»´æ•°è‚¯
  å®šæ¯”é€‰æ‹©å‰å°ï¼Œæ¯•ç«Ÿæˆ‘ä»¬åªé€‰æ‹©äº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†ç‰¹å¾ã€‚
  ```
- ä¸»è¦æ–¹æ³•:
  - **Filter(è¿‡æ»¤å¼):VarianceThreshold**
    > variance:æ–¹å·®<br>
    > threshold:å…¥å£ï¼›é—¨æ§›ï¼›å¼€å§‹ï¼›æé™ï¼›ä¸´ç•Œå€¼
  - **Embedded(åµŒå…¥å¼)ï¼šæ­£åˆ™åŒ–ã€å†³ç­–æ ‘**:åé¢ç®—æ³•æ—¶å†è®²
  - Wrapper(åŒ…è£¹å¼)ï¼šåŸºæœ¬ä¸ç”¨
- å…¶ä»–æ–¹æ³•ï¼š
  - **ç¥ç»ç½‘ç»œ**ï¼Œä¹‹åå†è¯´
  - çº¿æ€§åˆ¤åˆ«åˆ†æ LDAï¼ˆåŸºæœ¬ä¸ç”¨ï¼‰

#### 2.4.1.2. è¿‡æ»¤å¼

##### 2.4.1.2.1. æ–¹å·®è¿‡æ»¤ï¼šVarianceThreshold

- api:`sklearn.feature_selection.VarianceThreshold`
  - VarianceThreshold(threshold = 0.0)
    > é»˜è®¤ threshold=0.0ã€‚è¦æ ¹æ®å®é™…æƒ…å†µå–å€¼
    - åˆ é™¤æ‰€æœ‰ä½æ–¹å·®ç‰¹å¾
  - Variance.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè®­ç»ƒé›†å·®å¼‚ä½äº threshold çš„ç‰¹å¾å°†è¢«åˆ é™¤ã€‚
    - é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾ï¼Œå³åˆ é™¤æ‰€æœ‰æ ·æœ¬
    - ä¸­å…·æœ‰ç›¸åŒå€¼çš„ç‰¹å¾ã€‚

```py
from sklearn.feature_selection import VarianceThreshold

def main():
    vt = VarianceThreshold()
    rdata = vt.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])
    print(rdata)

if __name__ == "__main__":
    main()
    """
    ç»“æœï¼š
    [[2 0]
    [1 4]
    [1 1]]

    å³åˆ é™¤äº†ä¸€æ ·çš„åˆ—
    """
```

##### 2.4.1.2.2. ç›¸å…³æ€§è¿‡æ»¤

###### 2.4.1.2.2.1. å¡æ–¹è¿‡æ»¤

```
å¡æ–¹è¿‡æ»¤æ˜¯ä¸“é—¨é’ˆå¯¹ç¦»æ•£å‹æ ‡ç­¾ï¼ˆå³åˆ†ç±»é—®é¢˜ï¼‰çš„ç›¸å…³æ€§è¿‡æ»¤ã€‚
å¡æ–¹æ£€éªŒç±»feature_selection.chi2è®¡ç®—æ¯ä¸ªéè´Ÿç‰¹å¾å’Œæ ‡ç­¾ä¹‹é—´çš„å¡æ–¹ç»Ÿè®¡é‡ï¼Œå¹¶ä¾ç…§å¡æ–¹ç»Ÿè®¡é‡ç”±é«˜åˆ°ä½ä¸ºç‰¹å¾æ’åã€‚
å†ç»“åˆfeature_selection.SelectKBestè¿™ä¸ªå¯ä»¥è¾“å…¥â€è¯„åˆ†æ ‡å‡†â€œæ¥é€‰å‡ºå‰Kä¸ªåˆ†æ•°æœ€é«˜çš„ç‰¹å¾çš„ç±»ï¼Œ
æˆ‘ä»¬å¯ä»¥å€Ÿæ­¤é™¤å»æœ€å¯èƒ½ç‹¬ç«‹äºæ ‡ç­¾ï¼Œä¸æˆ‘ä»¬åˆ†ç±»ç›®çš„æ— å…³çš„ç‰¹å¾ã€‚
```

- api:
  ```py
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2
  
  #å‡è®¾éœ€è¦300ä¸ªç‰¹å¾
  X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)
  X_fschi.shape
  ```

###### 2.4.1.2.2.2. äº’ä¿¡æ¯è¿‡æ»¤

> äº†è§£ 

```
äº’ä¿¡æ¯æ³•æ˜¯ç”¨æ¥æ•æ‰æ¯ä¸ªç‰¹å¾ä¸æ ‡ç­¾ä¹‹é—´çš„ä»»æ„å…³ç³»ï¼ˆåŒ…æ‹¬çº¿æ€§å’Œéçº¿æ€§å…³ç³»ï¼‰çš„è¿‡æ»¤æ–¹æ³•ã€‚
äº’ä¿¡æ¯æ³•ä¸è¿”å›på€¼æˆ–Få€¼ç±»ä¼¼çš„ç»Ÿè®¡é‡ï¼Œå®ƒè¿”å› æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„äº’ä¿¡æ¯é‡çš„ä¼°è®¡ ï¼Œè¿™ä¸ªä¼°è®¡é‡åœ¨[0,1]ä¹‹é—´å–å€¼ï¼Œ
ä¸º0åˆ™è¡¨ç¤ºä¸¤ä¸ªå˜é‡ç‹¬ç«‹ï¼Œä¸º1åˆ™è¡¨ç¤ºä¸¤ä¸ªå˜é‡å®Œå…¨ç›¸å…³ã€‚
ä½¿ç”¨äº’ä¿¡æ¯æ³•é€‰å–æ•°æ®ç‰¹å¾ã€‚
```
- api
  ```python
  from sklearn.feature_selection import mutual_info_classif as MIC
  result = MIC(X_fsvar,y)#äº’ä¿¡æ¯æ³•
  ```


#### 2.4.1.3. åµŒå…¥å¼

##### 2.4.1.3.1. ä»‹ç»å’Œapi

- åµŒå…¥æ³•æ˜¯ä¸€ç§è®©ç®—æ³•è‡ªå·±å†³å®šä½¿ç”¨å“ªäº›ç‰¹å¾çš„æ–¹æ³•ï¼Œå³ç‰¹å¾é€‰æ‹©å’Œç®—æ³•è®­ç»ƒåŒæ—¶è¿›è¡Œ

- æ­¥éª¤ï¼š
  - åœ¨ä½¿ç”¨åµŒå…¥æ³•æ—¶ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨æŸäº›æœºå™¨å­¦ä¹ çš„ç®—æ³•å’Œæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°å„ä¸ªç‰¹å¾çš„æƒå€¼ç³»æ•°ï¼Œæ ¹æ®æƒå€¼ç³»æ•°ä»å¤§åˆ°å°é€‰æ‹©ç‰¹å¾ã€‚
    - è¿™äº›æƒå€¼ç³»æ•°å¾€å¾€ä»£è¡¨äº†ç‰¹å¾å¯¹äºæ¨¡å‹çš„æŸç§è´¡çŒ®æˆ–æŸç§é‡è¦æ€§ï¼Œ
    - æ¯”å¦‚å†³ç­–æ ‘å’Œæ ‘çš„é›†æˆæ¨¡å‹ä¸­çš„feature_importances_å±æ€§ï¼Œå¯ä»¥åˆ—å‡ºå„ä¸ªç‰¹å¾å¯¹æ ‘çš„å»ºç«‹çš„è´¡çŒ®
  - æˆ‘ä»¬å°±å¯ä»¥åŸºäºè¿™ç§è´¡çŒ®çš„è¯„ä¼°ï¼Œæ‰¾å‡ºå¯¹æ¨¡å‹å»ºç«‹æœ€æœ‰ç”¨çš„ç‰¹å¾ã€‚
- å…¶ä»–ï¼š
  - åµŒå…¥æ³•å¼•å…¥äº†ç®—æ³•æ¥æŒ‘é€‰ç‰¹å¾ï¼Œå› æ­¤å…¶è®¡ç®—é€Ÿåº¦ä¹Ÿä¼šå’Œåº”ç”¨çš„ç®—æ³•æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚
  - å¦‚æœé‡‡ç”¨è®¡ç®—é‡å¾ˆå¤§ï¼Œè®¡ç®—ç¼“æ…¢çš„ç®—æ³•ï¼ŒåµŒå…¥æ³•æœ¬èº«ä¹Ÿä¼šéå¸¸è€—æ—¶è€—åŠ›ã€‚
  - å¹¶ä¸”ï¼Œåœ¨é€‰æ‹©å®Œæ¯•ä¹‹åï¼Œæˆ‘ä»¬è¿˜æ˜¯éœ€è¦è‡ªå·±æ¥è¯„ä¼°æ¨¡å‹ã€‚

![](./image/2020-11-07-10-56-52.png)


- api:ä¸»è¦æ˜¯thresholdï¼ˆé˜ˆå€¼ï¼‰çš„é€‰æ‹©
  ```py
  class sklearn.feature_selection.SelectFromModel(
    estimator, # ä½¿ç”¨çš„æ¨¡å‹è¯„ä¼°å™¨ï¼Œåªè¦æ˜¯å¸¦feature_importances_æˆ–è€…coef_å±æ€§ï¼Œæˆ–å¸¦æœ‰l1å’Œl2æƒ©ç½šé¡¹çš„æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨
    threshold=None, # ç‰¹å¾é‡è¦æ€§çš„é˜ˆå€¼ï¼Œé‡è¦æ€§ä½äºè¿™ä¸ªé˜ˆå€¼çš„ç‰¹å¾éƒ½å°†è¢«åˆ é™¤
    prefit=False, # é»˜è®¤Falseï¼Œåˆ¤æ–­æ˜¯å¦å°†å®ä¾‹åŒ–åçš„æ¨¡å‹ç›´æ¥ä¼ é€’ç»™æ„é€ å‡½æ•°ã€‚
                  # å¦‚æœä¸ºTrueï¼Œåˆ™å¿…é¡»ç›´æ¥è°ƒç”¨fitå’Œtransformï¼Œä¸èƒ½ä½¿ç”¨fit_transformï¼Œ
                  # å¹¶ä¸”SelectFromModelä¸èƒ½ä¸cross_val_scoreï¼ŒGridSearchCVå’Œå…‹éš†ä¼°è®¡å™¨çš„ç±»ä¼¼å®ç”¨ç¨‹åºä¸€èµ·ä½¿ç”¨ã€‚
    norm_order=1, # kå¯è¾“å…¥éé›¶æ•´æ•°ï¼Œæ­£æ— ç©·ï¼Œè´Ÿæ— ç©·ï¼Œé»˜è®¤å€¼ä¸º1ã€‚åœ¨è¯„ä¼°å™¨çš„coef_å±æ€§é«˜äºä¸€ç»´çš„æƒ…å†µä¸‹ï¼Œç”¨äºè¿‡æ»¤ä½äºé˜ˆå€¼çš„ç³»æ•°çš„å‘é‡çš„èŒƒæ•°çš„é˜¶æ•°
    ax_features=None #åœ¨é˜ˆå€¼è®¾å®šä¸‹ï¼Œè¦é€‰æ‹©çš„æœ€å¤§ç‰¹å¾æ•°ã€‚è¦ç¦ç”¨é˜ˆå€¼å¹¶ä»…æ ¹æ®max_featuresé€‰æ‹©ï¼Œè¯·è®¾ç½®threshold = -np.inf
  )
  ```

##### 2.4.1.3.2. â€»é€šè¿‡å­¦ä¹ æ›²çº¿è¿›è¡Œé€‰æ‹©(é€šç”¨)â€»

- é€šè¿‡å®½æ³›çš„å­¦ä¹ æ›²çº¿è¿›è¡Œé€‰æ‹©
  ```py
  # RFC_:éšæœºæ£®æ—å¯¹è±¡
  RFC_.fit(X,y).feature_importances_
  
  threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20) # ä»0åˆ°æœ€é«˜ç‰¹å¾é‡è¦æ€§ï¼Œlinspaceå¾—åˆ°20ä¸ªã€‚ä½œä¸ºé˜ˆå€¼æŸ¥çœ‹åˆ†æ•°
  
  score = []
  for i in threshold:
      X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)  # ç­›é€‰å‡ºç‰¹å¾
      once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()  # 5æŠ˜äº¤å‰éªŒè¯
      score.append(once)  # å¾—åˆ°åˆ†æ•°
  plt.plot(threshold,score)
  plt.show()
  ```

- é€šè¿‡ç»†åŒ–çš„å­¦ä¹ æ›²çº¿è¿›è¡Œé€‰æ‹©
  ```py
  score2 = []
  for i in np.linspace(0,0.00134,20):
      X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)
      once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()
      score2.append(once)
  plt.figure(figsize=[20,5])
  plt.plot(np.linspace(0,0.00134,20),score2)
  plt.xticks(np.linspace(0,0.00134,20))
  plt.show()
  ```


#### 2.4.1.4. åŒ…è£…å¼

- åŒ…è£…æ³•ä¹Ÿæ˜¯ä¸€ä¸ªç‰¹å¾é€‰æ‹©å’Œç®—æ³•è®­ç»ƒåŒæ—¶è¿›è¡Œçš„æ–¹æ³•ï¼Œä¸åµŒå…¥æ³•ååˆ†ç›¸ä¼¼ï¼Œå®ƒä¹Ÿæ˜¯ä¾èµ–äºç®—æ³•è‡ªèº«çš„é€‰æ‹©ï¼Œ
  - æ¯”å¦‚coef_å±æ€§æˆ–feature_importances_å±æ€§æ¥å®Œæˆç‰¹å¾é€‰æ‹©
  - ä»å½“å‰çš„ä¸€ç»„ç‰¹å¾ä¸­ä¿®å‰ªæœ€ä¸é‡è¦çš„ç‰¹å¾ã€‚åœ¨ä¿®å‰ªçš„é›†åˆä¸Šé€’å½’åœ°é‡å¤è¯¥è¿‡ç¨‹ï¼Œç›´åˆ°æœ€ç»ˆåˆ°è¾¾æ‰€éœ€æ•°é‡çš„è¦é€‰æ‹©çš„ç‰¹å¾
  - åŒºåˆ«äºè¿‡æ»¤æ³•å’ŒåµŒå…¥æ³•çš„ä¸€æ¬¡è®­ç»ƒè§£å†³æ‰€æœ‰é—®é¢˜ï¼ŒåŒ…è£…æ³•è¦ä½¿ç”¨ç‰¹å¾å­é›†è¿›è¡Œå¤šæ¬¡è®­ç»ƒï¼Œå› æ­¤å®ƒæ‰€éœ€è¦çš„è®¡ç®—æˆæœ¬æ˜¯æœ€é«˜çš„
  - åŒ…è£…æ³•çš„æ•ˆæœæ˜¯æ‰€æœ‰ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸­æœ€åˆ©äºæå‡æ¨¡å‹è¡¨ç°çš„ï¼Œå®ƒå¯ä»¥ä½¿ç”¨å¾ˆå°‘çš„ç‰¹å¾è¾¾åˆ°å¾ˆä¼˜ç§€çš„æ•ˆæœã€‚
  - é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨ç‰¹å¾æ•°ç›®ç›¸åŒæ—¶ï¼ŒåŒ…è£…æ³•å’ŒåµŒå…¥æ³•çš„æ•ˆæœèƒ½å¤ŸåŒ¹æ•Œï¼Œä¸è¿‡å®ƒæ¯”åµŒå…¥æ³•ç®—å¾—æ›´è§ç¼“æ…¢ï¼Œæ‰€ä»¥ä¹Ÿä¸é€‚ç”¨äºå¤ªå¤§å‹çš„æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒ…è£…æ³•æ˜¯æœ€èƒ½ä¿è¯æ¨¡å‹æ•ˆæœçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚

![](./image/2020-11-07-11-15-06.png)

- æœ€å…¸å‹çš„ç›®æ ‡å‡½æ•°æ˜¯é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•ï¼ˆRecursive feature elimination, ç®€å†™ä¸ºRFEï¼‰ã€‚
  - å®ƒæ˜¯ä¸€ç§è´ªå©ªçš„ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨æ‰¾åˆ°æ€§èƒ½æœ€ä½³çš„ç‰¹å¾å­é›†ã€‚
  - å®ƒåå¤åˆ›å»ºæ¨¡å‹ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£æ—¶ä¿ç•™æœ€ä½³ç‰¹å¾æˆ–å‰”é™¤æœ€å·®ç‰¹å¾ï¼Œ
  - ä¸‹ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œå®ƒä¼šä½¿ç”¨ä¸Šä¸€æ¬¡å»ºæ¨¡ä¸­æ²¡æœ‰è¢«é€‰ä¸­çš„ç‰¹å¾æ¥æ„å»ºä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Œç›´åˆ°æ‰€æœ‰ç‰¹å¾éƒ½è€—å°½ä¸ºæ­¢ã€‚
  - ç„¶åï¼Œå®ƒæ ¹æ®è‡ªå·±ä¿ç•™æˆ–å‰”é™¤ç‰¹å¾çš„é¡ºåºæ¥å¯¹ç‰¹å¾è¿›è¡Œæ’åï¼Œæœ€ç»ˆé€‰å‡ºä¸€ä¸ªæœ€ä½³å­é›†ã€‚

- api:
  ass sklearn . feature _ selection . RFE ( estimator , n features to select = None , step = 1 , verbose

### 2.4.2. ç‰¹å¾é™ç»´ 

#### 2.4.2.1. æ¦‚å¿µ

åœ¨é™ç»´çš„è¿‡ç¨‹ä¸­ï¼Œèƒ½å¤Ÿå³å‡å°‘ç‰¹å¾çš„æ•°é‡ï¼Œåˆä¿ç•™å¤§éƒ¨åˆ†æœ‰æ•ˆä¿¡æ¯â€”â€”å°†é‚£äº›å¸¦æœ‰é‡å¤ä¿¡æ¯çš„ç‰¹å¾åˆå¹¶ï¼Œ
å¹¶åˆ é™¤é‚£äº›å¸¦æ— æ•ˆä¿¡æ¯çš„ç‰¹å¾ç­‰ç­‰â€”â€”é€æ¸åˆ›é€ å‡ºèƒ½å¤Ÿä»£è¡¨åŸç‰¹å¾çŸ©é˜µå¤§éƒ¨åˆ†ä¿¡æ¯çš„ï¼Œç‰¹å¾æ›´å°‘çš„ï¼Œæ–°ç‰¹å¾çŸ©é˜µ


é™ç»´ç®—æ³•çš„è®¡ç®—é‡å¾ˆå¤§ï¼Œè¿è¡Œæ¯”è¾ƒç¼“æ…¢ï¼Œä½†æ— è®ºå¦‚ä½•ï¼Œå®ƒä»¬çš„åŠŸèƒ½æ— å¯æ›¿ä»£


- å’Œç‰¹å¾é€‰æ‹©åŒºåˆ«ï¼š
  - ç‰¹å¾é€‰æ‹©æ˜¯ä»å·²å­˜åœ¨çš„ç‰¹å¾ä¸­é€‰å–æºå¸¦ä¿¡æ¯æœ€å¤šçš„ï¼Œé€‰å®Œä¹‹åçš„ç‰¹å¾ä¾ç„¶å…·æœ‰å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬ä¾ç„¶çŸ¥é“è¿™ä¸ªç‰¹å¾åœ¨åŸæ•°æ®çš„å“ªä¸ªä½ç½®ï¼Œä»£è¡¨ç€åŸæ•°æ®ä¸Šçš„ä»€ä¹ˆå«ä¹‰ã€‚
  - è€ŒPCAï¼Œæ˜¯å°†å·²å­˜åœ¨çš„ç‰¹å¾è¿›è¡Œå‹ç¼©ï¼Œé™ç»´å®Œæ¯•åçš„ç‰¹å¾ä¸æ˜¯åŸæœ¬çš„ç‰¹å¾çŸ©é˜µä¸­çš„ä»»ä½•ä¸€ä¸ªç‰¹å¾ï¼Œè€Œæ˜¯é€šè¿‡æŸäº›æ–¹å¼ç»„åˆèµ·æ¥çš„æ–°ç‰¹å¾ã€‚
  - é€šå¸¸æ¥è¯´ï¼Œåœ¨æ–°çš„ç‰¹å¾çŸ©é˜µç”Ÿæˆä¹‹å‰ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥æ™“PCAéƒ½å»ºç«‹äº†æ€æ ·çš„æ–°ç‰¹å¾å‘é‡ï¼Œæ–°ç‰¹å¾çŸ©é˜µç”Ÿæˆä¹‹åä¹Ÿä¸å…·æœ‰å¯è¯»æ€§ï¼Œæˆ‘ä»¬æ— æ³•åˆ¤æ–­æ–°ç‰¹å¾çŸ©é˜µçš„ç‰¹å¾æ˜¯ä»åŸæ•°æ®ä¸­çš„ä»€ä¹ˆç‰¹å¾ç»„åˆè€Œæ¥ï¼Œæ–°ç‰¹å¾è™½ç„¶å¸¦æœ‰åŸå§‹æ•°æ®çš„ä¿¡æ¯ï¼Œå´å·²ç»ä¸æ˜¯åŸæ•°æ®ä¸Šä»£è¡¨ç€çš„å«ä¹‰äº†ã€‚
  - ä»¥PCAä¸ºä»£è¡¨çš„é™ç»´ç®—æ³•å› æ­¤æ˜¯ç‰¹å¾åˆ›é€ ï¼ˆfeature creationï¼Œæˆ–feature constructionï¼‰çš„ä¸€ç§ã€‚
  - å¯ä»¥æƒ³è§ï¼ŒPCAä¸€èˆ¬ä¸é€‚ç”¨äºæ¢ç´¢ç‰¹å¾å’Œæ ‡ç­¾ä¹‹é—´çš„å…³ç³»çš„æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å›å½’ï¼‰ï¼Œå› ä¸ºæ— æ³•è§£é‡Šçš„æ–°ç‰¹å¾å’Œæ ‡ç­¾ä¹‹é—´çš„å…³ç³»ä¸å…·æœ‰æ„ä¹‰

#### 2.4.2.2. PCA(ä¸»æˆåˆ†åˆ†æ)


> å½“ç‰¹å¾æˆç™¾ä¸Šåƒæ—¶ï¼Œå°±éœ€è¦è€ƒè™‘äº†

```
PCAä½¿ç”¨çš„ä¿¡æ¯é‡è¡¡é‡æŒ‡æ ‡ï¼Œå°±æ˜¯æ ·æœ¬æ–¹å·®ï¼Œåˆç§°å¯è§£é‡Šæ€§æ–¹å·®ï¼Œæ–¹å·®è¶Šå¤§ï¼Œç‰¹å¾æ‰€å¸¦çš„ä¿¡æ¯é‡è¶Šå¤šã€‚
é™ç»´å®Œæˆä¹‹åï¼ŒPCAæ‰¾åˆ°çš„æ¯ä¸ªæ–°ç‰¹å¾å‘é‡å°±å«åšâ€œä¸»æˆåˆ†â€ï¼Œè€Œè¢«ä¸¢å¼ƒçš„ç‰¹å¾å‘é‡è¢«è®¤ä¸ºä¿¡æ¯é‡å¾ˆå°‘ï¼Œè¿™äº›ä¿¡æ¯å¾ˆå¯èƒ½å°±æ˜¯å™ªéŸ³ã€‚
```

> **åŸç†è‡ªå·±æŸ¥**

- æœ¬è´¨ï¼šPCA æ˜¯ä¸€ç§åˆ†æã€ç®€åŒ–æ•°æ®é›†çš„æŠ€æœ¯ã€‚
- ç›®çš„ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯ã€‚
  > æ¯”å¦‚ feature1 å’Œ featrue2 é—´ä¸ºçº¿æ€§å…³ç³»ï¼Œé‚£ä¹ˆå°±å¯ä»¥åˆ é™¤å…¶ä¸­ä¸€ä¸ª
- ä½œç”¨ï¼šå¯ä»¥å‰Šå‡å›å½’åˆ†ææˆ–è€…èšç±»åˆ†æä¸­ç‰¹å¾çš„æ•°é‡
- api:`sklearn.decomposition.PCA`
  - PCA
    - n_components=None
      > mle:ç¼ºç‚¹è®¡ç®—é‡å¤§
      > å°æ•°ï¼Œ0~1 çš„ç™¾åˆ†æ¯”ï¼Œè¡¨ç¤ºä¿¡æ¯çš„æŸå¤±é‡ï¼Œä¸€èˆ¬ä¸º 0.9~0.95<br>
      > æ•´æ•°ï¼šå‡å°‘åˆ°çš„ç‰¹å¾æ•°é‡ã€‚**ä¸€èˆ¬ä¸ç”¨**
    - svd_solver
      > åœ¨é™ç»´è¿‡ç¨‹ä¸­ï¼Œç”¨æ¥æ§åˆ¶çŸ©é˜µåˆ†è§£çš„ä¸€äº›ç»†èŠ‚çš„å‚æ•°ã€‚æœ‰å››ç§æ¨¡å¼å¯é€‰ "auto"æ˜¯é»˜è®¤å€¼
      - "auto"ï¼šåŸºäºX.shapeå’Œn_componentsçš„é»˜è®¤ç­–ç•¥æ¥é€‰æ‹©åˆ†è§£å™¨ï¼šå¦‚æœè¾“å…¥æ•°æ®çš„å°ºå¯¸å¤§äº500x500ä¸”è¦æå–çš„ç‰¹å¾æ•°å°äºæ•°æ®æœ€å°ç»´åº¦min(X.shape)çš„80ï¼…ï¼Œå°±å¯ç”¨æ•ˆç‡æ›´é«˜çš„â€randomizedâ€œæ–¹æ³•ã€‚å¦åˆ™ï¼Œç²¾ç¡®å®Œæ•´çš„SVDå°†è¢«è®¡ç®—ï¼Œæˆªæ–­å°†ä¼šåœ¨çŸ©é˜µè¢«åˆ†è§£å®Œæˆåæœ‰é€‰æ‹©åœ°å‘ç”Ÿ
      - "full"ï¼šä»scipy.linalg.svdä¸­è°ƒç”¨æ ‡å‡†çš„LAPACKåˆ†è§£å™¨æ¥ç”Ÿæˆç²¾ç¡®å®Œæ•´çš„SVDï¼Œé€‚åˆæ•°æ®é‡æ¯”è¾ƒé€‚ä¸­ï¼Œè®¡ç®—æ—¶é—´å……è¶³çš„æƒ…å†µ
      - "arpack"ï¼šå¯ä»¥åŠ å¿«è¿ç®—é€Ÿåº¦ï¼Œé€‚åˆç‰¹å¾çŸ©é˜µå¾ˆå¤§çš„æ—¶å€™ï¼Œä½†ä¸€èˆ¬ç”¨äºç‰¹å¾çŸ©é˜µä¸ºç¨€ç–çŸ©é˜µçš„æƒ…å†µï¼Œæ­¤è¿‡ç¨‹åŒ…å«ä¸€å®šçš„éšæœºæ€§ã€‚
      - "randomized"ï¼šé€‚åˆç‰¹å¾çŸ©é˜µå·¨å¤§ï¼Œè®¡ç®—é‡åºå¤§çš„æƒ…å†µã€‚

  - PCA.fit_transform(X)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åæŒ‡å®šç»´åº¦çš„ array
  - å±æ€§ï¼š
    - explained_variance_ï¼ŒæŸ¥çœ‹é™ç»´åæ¯ä¸ªæ–°ç‰¹å¾å‘é‡ä¸Šæ‰€å¸¦çš„ä¿¡æ¯é‡å¤§å° ï¼ˆå¯è§£é‡Šæ€§æ–¹å·®çš„å¤§å°ï¼‰
    - **explained_variance_ratio**ï¼ŒæŸ¥çœ‹é™ç»´åæ¯ä¸ªæ–°ç‰¹å¾å‘é‡æ‰€å çš„ä¿¡æ¯é‡å åŸå§‹æ•°æ®æ€»ä¿¡æ¯é‡çš„ç™¾åˆ†æ¯”åˆå«åšå¯è§£é‡Šæ–¹å·®è´¡çŒ®ç‡
      > é™ç»´å‰è¦çœ‹çœ‹
    - explained_variance_ratio_.sum() ä¿ç•™ä¿¡æ¯æ€»å’Œ
    - components_:æ–°ç‰¹å¾ç©ºé—´
  - PCA.inverse_transform()
    > æ•°æ®æ¢å¤

```py
from sklearn.decomposition import PCA

def main():
    pca = PCA(n_components=0.9)
    pca.fit([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])
    print(pca.explained_variance_ratio)  # å½“æœ‰å¿…è¦æ—¶ï¼Œæ‰è¿›è¡Œè½¬æ¢ã€‚è´¡çŒ®åº¦éƒ½å·®ä¸å¤šï¼Œå°±åˆ«é™ç»´äº†
    rdata = pca.transform();
    print(rdata)

if __name__ == "__main__":
    main()
```

#### 2.4.2.3. SVD

```
SVDä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£æ¥æ‰¾å‡ºç©ºé—´Vï¼Œå…¶ä¸­Î£ä¹Ÿæ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œä¸è¿‡å®ƒå¯¹è§’çº¿ä¸Šçš„å…ƒç´ æ˜¯å¥‡å¼‚å€¼ï¼Œè¿™ä¹Ÿæ˜¯SVDä¸­ç”¨æ¥è¡¡é‡ç‰¹å¾ä¸Šçš„ä¿¡æ¯é‡çš„æŒ‡æ ‡ã€‚
```

##### 2.4.2.3.1. é™ç»´æ¡ˆä¾‹

[æ•°æ®](https://www.kaggle.com/c/instacart-market-basket-analysis/data)



## 2.5. æ•°æ®é›†

> scikit-learnæœ‰å†…ç½®æ•°æ®é›†

- å¯å°†æ•°æ®é›†åˆ’åˆ†ä¸ºåˆ’åˆ†ä¸ºï¼š
  - è®­ç»ƒæ•°æ®é›†
  - æµ‹è¯•æ•°æ®é›†

> ![](./image/data_set.jpg)

- api:
  - è·å–ï¼š
    - sklearn.datasets
      > åŠ è½½è·å–æµè¡Œæ•°æ®é›†
      - `datasets.load_*()`
        > è·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œ
      - `datasets.fetch_*(data_home=None)`
        > è·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½ï¼Œå‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯data_homeï¼Œè¡¨ç¤ºæ•°æ®é›†ä¸‹è½½çš„ç›®å½•,é»˜è®¤æ˜¯ ~/scikit_learn_data/<br>
        > subset: 'train'æˆ–è€…'test','all'ï¼Œå¯é€‰ï¼Œé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†. è®­ç»ƒé›†çš„â€œè®­ç»ƒâ€ï¼Œæµ‹è¯•é›†çš„â€œæµ‹è¯•â€ï¼Œä¸¤è€…çš„â€œå…¨éƒ¨â€
    - æ•°æ®æ ¼å¼ï¼›
      - load*å’Œfetch*è¿”å›çš„æ•°æ®ç±»å‹datasets.base.Bunch(å­—å…¸æ ¼å¼)
      - dataï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯ [n_samples * n_features] çš„äºŒç»´ numpy.ndarray æ•°ç»„
      - targetï¼šç›®æ ‡å€¼æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
      - DESCRï¼šæ•°æ®æè¿°
      - feature_namesï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
      - target_namesï¼šæ ‡ç­¾å,å›å½’æ•°æ®é›†æ²¡æœ‰
  - åˆ’åˆ†ï¼š`sklearn.model_selection.train_test_split(*arrays,Â **options)`
    - x:  æ•°æ®é›†çš„ç‰¹å¾å€¼
    - y:  æ•°æ®é›†çš„æ ‡ç­¾å€¼
    - test_size      æµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸ºfloat
    - random_state        éšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœº é‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚
    - return  è®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒæ ‡ç­¾ï¼Œæµ‹è¯•æ ‡ç­¾ (é»˜è®¤éšæœºå–)	
  - æ¸…æ¥šï¼š
    - `datasets.clear_data_home(data_home=None)`
      > æ¸…é™¤ç›®å½•ä¸‹çš„æ•°æ®

## 2.6. è½¬æ¢å™¨ä¸ä¼°è®¡å™¨

### 2.6.1. è½¬æ¢å™¨

> ç‰¹å¾å·¥ç¨‹ï¼Œæ•°æ®å¤„ç†æ‰€ç”¨api

- é€šç”¨ï¼š
  - fit():è¾“å…¥æ•°æ®ï¼Œä½†ä¸è¿›è¡Œå¤„ç†ã€‚ä½†ä¼šè®¡ç®—äº›å¹³å‡å€¼å’Œæ–¹å·®ï¼Œæˆ–è€…è¯åˆ—è¡¨ç­‰å‰ææ“ä½œï¼Œè¿™äº›å€¼ä¼šå­˜å‚¨åˆ°å¤„ç†å¯¹è±¡ä¸­ï¼Œæ¯”å¦‚StandScaler
    > **æ‰€ä»¥ä¸èƒ½fitå®Œä¸€ä¸ªæ•°æ®ï¼Œå†transformå¦ä¸€ä¸ªæ•°æ®ï¼Œä¼šä¹±å¥—**
  - transform():æ•°æ®å¤„ç†ã€‚æ ¹æ®fit()å¾—åˆ°çš„ç»“æœï¼Œå†è¿›è¡Œå¤„ç†
  - fit_transform():è¾“å…¥æ•°æ®ï¼Œç›´æ¥è½¬æ¢ã€‚**ä¸€èˆ¬ä½¿ç”¨è¿™ä¸ªå³å¯ï¼Œä¸Šé¢ä¸¤ä¸ªåŸºæœ¬ä¸ç”¨**
    > ç­‰äº fit()+transform()


### 2.6.2. ä¼°è®¡å™¨

- å®šä¹‰ï¼šåœ¨sklearnä¸­ï¼Œä¼°è®¡å™¨(estimator)æ˜¯ä¸€ä¸ªé‡è¦çš„è§’è‰²ï¼Œåˆ†ç±»å™¨å’Œå›å½’å™¨éƒ½å±äºestimatorï¼Œæ˜¯ä¸€ç±»å®ç°äº†ç®—æ³•çš„API
  > å®ç°ç®—æ³•api

- ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨
  - sklearn.neighbors	k-è¿‘é‚»ç®—æ³•
  - sklearn.naive_bayes      è´å¶æ–¯
  - sklearn.linear_model.LogisticRegression     é€»è¾‘å›å½’
- ç”¨äºå›å½’çš„ä¼°è®¡å™¨
  - sklearn.linear_model.LinearRegression     çº¿æ€§å›å½’
  - sklearn.linear_model.Ridge      å²­å›å½’ 

- é€šç”¨ï¼š
  - fit()ï¼šè¿›è¡Œè®­ç»ƒ
  - prediec():é¢„ä¼°ç»“æœ
  - score():å‡†ç¡®ç‡

## 2.7. apiæ€»ç»“

### 2.7.1. apiæ€»ç»“

- æ•°æ®æŠ½å–ï¼š
  - å­—å…¸ç±»å‹æ•°æ®æŠ½å–:`sklearn.feature_extraction.DictVectorizer`
  - æ–‡æœ¬ç‰¹å¾æ•°æ®æŠ½å–:
    - count:`sklearn.feature_extraction.text.CountVectorizer`
    - tf-idf:`sklearn.feature_extraction.text.TfidfVectorizer`
- æ•°æ®é¢„å¤„ç†
  - å½’ä¸€åŒ–ï¼š`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - æ ‡å‡†åŒ–:`sklearn.preprocessing.StandardScaler`
  - ç¼ºå¤±å€¼:`sklearn.preprocessing.Imputer`
- æ•°æ®é™ç»´
  - ç‰¹å¾é€‰æ‹©:`sklearn.feature_selection.VarianceThreshold`
  - PCA:`sklearn.decomposition.PCA`


### 2.7.2. apiå¤§è‡´ä½¿ç”¨æµç¨‹

![](./image/sklearn_api_line.jpg)

# 3. åŸºæœ¬ç®—æ³•

> æ­¤å¤„åªä¼šè®²è§£åŸºæœ¬æ¦‚å¿µå’Œapiä½¿ç”¨ã€‚ç®—æ³•å®ç°ä¸åšæ€»ç»“

## 3.1. ç›‘ç£å­¦ä¹ 

### 3.1.1. åˆ†ç±»

#### 3.1.1.1. k-è¿‘é‚»ç®—æ³•

> åŸºæœ¬ä¸å’‹ç”¨

- çœ‹ã€Šç®—æ³•å›¾è§£ã€‹
- éœ€è¦åšæ ‡å‡†åŒ–å¤„ç†
- api:sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
  - n_neighborsï¼šint,å¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œk_neighborsæŸ¥è¯¢é»˜è®¤ä½¿ç”¨çš„é‚»å±…æ•° 
  - algorithmï¼š{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼šâ€˜ball_treeâ€™å°†ä¼šä½¿ç”¨ BallTreeï¼Œâ€˜kd_treeâ€™å°†ä½¿ç”¨ KDTreeã€‚â€˜autoâ€™å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•ã€‚ (ä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡)

- ä¼˜ç‚¹ï¼š
  - ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€ä¼°è®¡å‚æ•°ï¼Œæ— éœ€è®­ç»ƒ
- ç¼ºç‚¹ï¼š
  - æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
  - å¿…é¡»æŒ‡å®šKå€¼ï¼ŒKå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯

  ```py
  import numpy as np
  import pandas as pd
  from sklearn import neighbors
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler

  # è·å–æ•°æ®
  iris_data = load_iris()

  # æ•°æ®æè¿°
  print(iris_data.DESCR)

  # æŸ¥çœ‹éƒ¨åˆ†æ•°æ®
  print(iris_data.data[:5, :])

  # å½’ä¸€åŒ–
  mm = MinMaxScaler()
  x_data = mm.fit_transform(iris_data.data)

  # æƒé‡å¤„ç†(å°è¯•)
  # x_data[:,[0]] = x_data[:,[0]]/(1-0.7826)
  # x_data[:,[1]] = x_data[:,[0]]/(1+0.4194)
  # x_data[:,[2]] = x_data[:,[0]]/(1-0.9490)

  # æ•°æ®åˆ’åˆ†
  x_train, x_test, y_train, y_test = train_test_split(
      x_data, iris_data.target, test_size=0.25)

  # å®ä¾‹åŒ–knnå¯¹è±¡
  knn = neighbors.KNeighborsClassifier(n_neighbors=5,)

  # å°è¯•æ·»åŠ æƒé‡
  # knn = neighbors.KNeighborsClassifier(n_neighbors=5, metric="wminkowski", metric_params={
  #                                      "w": [0.7826, -0.4194, 0.9490, 0.9565]})

  # åŠ è½½æ•°æ®
  knn.fit(x_train, y_train)

  # é¢„æµ‹å€¼
  knn.predict(x_test)

  # å‡†ç¡®ç‡
  knn.score(x_test, y_test)
  ```


#### 3.1.1.2. æœ´ç´ è´å¶æ–¯åˆ†ç±»

[è®²è§£](https://zhuanlan.zhihu.com/p/26262151)

  ```py
  import numpy as np
  import pandas as pd
  from sklearn.datasets import fetch_20newsgroups
  from sklearn.model_selection import train_test_split
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.naive_bayes import MultinomialNB

  news = fetch_20newsgroups(subset='all')
  x_train,x_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25)

  # æ•°æ®å¤„ç†
  tf = TfidfVectorizer()
  x_train = tf.fit_transform(x_train)
  x_test = tf.transform(x_test) # æ³¨æ„ï¼Œè¿™é‡Œç”¨trainsformï¼Œæ²¡æœ‰fitï¼Œè¦ä»¥trainä¸ºæ ‡å‡†

  # åˆ›å»ºæœ´ç´ è´å¶æ–¯apiå¯¹è±¡
  mlt = MultinomialNB(alpha=1.0)

  # è¿›è¡Œè®­ç»ƒ
  mlt.fit(x_train,y_train)

  # é¢„æµ‹å€¼
  mlt.predict(x_test)

  # å‡†ç¡®ç‡
  mlt.score(x_test,y_test)
  ```

- ç‰¹ç‚¹
  - æ— æ³•ä¼ å…¥å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œè®­ç»ƒé›†å½±å“å¤§
  - ä¸éœ€è¦è°ƒå‚
  - è®­ç»ƒé›†è¯¯å·®ä¼šå¤§å¤§å½±å“ç»“æœ

- ä¼˜ç‚¹ï¼š
  - æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
  - å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
  - åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«
- ç¼ºç‚¹ï¼š
  - ç”±äºä½¿ç”¨äº†æ ·æœ¬å±æ€§ç‹¬ç«‹æ€§çš„å‡è®¾ï¼Œæ‰€ä»¥å¦‚æœæ ·æœ¬å±æ€§æœ‰å…³è”æ—¶ å…¶æ•ˆæœä¸å¥½

> ç¥ç»ç½‘ç»œçš„æ•ˆæœè¦æ¯”æœ´ç´ è´å¶æ–¯è¦å¥½

#### 3.1.1.3. åˆ†ç±»æ¨¡å‹è¯„ä¼°

> åœ¨åˆ†ç±»ä»»åŠ¡ä¸‹ï¼Œé¢„æµ‹ç»“æœ(Predicted Condition)ä¸æ­£ç¡®æ ‡è®°(True Condition)ä¹‹é—´å­˜åœ¨å››ç§ä¸åŒçš„ç»„åˆï¼Œæ„æˆæ··æ·†çŸ©é˜µ(é€‚ç”¨äºå¤šåˆ†ç±»)

![](./image/confusion_matrix.jpg)


- æ··æ·†çŸ©é˜µ
  - ç²¾ç¡®ç‡:é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹æ ·æœ¬ä¸­çœŸå®ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å¾—å‡†ï¼‰
    > ![](./image/precision.jpg)
  - å¬å›ç‡:çœŸå®ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥çš„å…¨ï¼Œå¯¹æ­£æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ï¼‰
    > ![](./image/recall.jpg)
  - å…¶ä»–åˆ†ç±»æ ‡å‡†ï¼ŒF1-scoreï¼Œåæ˜ äº†æ¨¡å‹çš„ç¨³å¥å‹
    > ![](./image/f1-score.jpg)

- api:`sklearn.metrics.classification_report `
  - y_trueï¼šçœŸå®ç›®æ ‡å€¼ 
  - y_predï¼šä¼°è®¡å™¨é¢„æµ‹ç›®æ ‡å€¼ 
  - target_namesï¼šç›®æ ‡ç±»åˆ«åç§° 
  - returnï¼šæ¯ä¸ªç±»åˆ«ç²¾ç¡®ç‡ä¸å¬å›ç‡

#### 3.1.1.4. æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

##### 3.1.1.4.1. äº¤å‰éªŒè¯

> ä¸€èˆ¬å’Œç½‘æ ¼æœç´¢æ­é…ã€‚

- ç›®çš„ï¼šäº¤å‰éªŒè¯ï¼šä¸ºäº†è®©è¢«è¯„ä¼°çš„æ¨¡å‹æ›´åŠ å‡†ç¡®å¯ä¿¡
- è¿‡ç¨‹ï¼š
  ```
  äº¤å‰éªŒè¯ï¼šå°†æ‹¿åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼š
  å°†æ•°æ®åˆ†æˆ5ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä½œä¸ºéªŒè¯é›†ã€‚
  ç„¶åç»è¿‡5æ¬¡(ç»„)çš„æµ‹è¯•ï¼Œæ¯æ¬¡éƒ½æ›´æ¢ä¸åŒçš„éªŒè¯é›†ã€‚
  å³å¾—åˆ°5ç»„æ¨¡å‹çš„ç»“æœï¼Œå–å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆç»“æœã€‚åˆç§°5æŠ˜äº¤å‰éªŒè¯ã€‚
  ```
  > ![](./image/cross_validation.jpg)

- api:
  - `sklearn.model_selection.cross_val_score(estimator,data,target,cv,[scoring])`:è·å–cvæŠ˜äº¤å‰éªŒè¯çš„åˆ†æ•°
  - `sklearn.model_selection.cross_val_predict(estimator,data,target,cv,[scoring])`:è·å–cvæŠ˜äº¤å‰éªŒè¯çš„åˆ†æ•°
  ```python
  # ä¾‹
  regressor = DecisionTreeRegressor(random_state=0)
  score_arr = cross_val_score(regressor, boston.data, boston.target, cv=10,
                    scoring = "neg_mean_squared_error")
  ```


##### 3.1.1.4.2. ç½‘æ ¼æœç´¢

> ä¸€èˆ¬ä¸äº¤å‰éªŒè¯æ­é…

- è¯´æ˜ï¼š
  ```
  é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œ
  è¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„
  åˆã€‚æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»º
  ç«‹æ¨¡å‹ã€‚
  ```
- è¿‡ç¨‹ï¼š
  - å°±æ˜¯ä¸€ä¸ªç©·ä¸¾ã€‚æ¯ä¸ªå‚æ•°å€¼éƒ½ç”¨äº¤å‰éªŒè¯å¾—åˆ°æ¨¡å‹
  - å‚æ•°é—´ç›¸äº’ç»„åˆ
  - é€‰å–æœ€å¥½çš„æ¨¡å‹

##### 3.1.1.4.3. ä¸¤è€…æ­é…

- api:`sklearn.model_selection.GridSearchCV`
  > å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢
  - å‚æ•°
    - estimatorï¼šä¼°è®¡å™¨å¯¹è±¡
    - param_gridï¼šä¼°è®¡å™¨å‚æ•°(dict){â€œn_neighborsâ€:[1,3,5]}
    - cvï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
    - fitï¼šè¾“å…¥è®­ç»ƒæ•°æ®
    - scoreï¼šå‡†ç¡®ç‡
  - ç»“æœåˆ†æï¼š
    - best_score_:åœ¨äº¤å‰éªŒè¯ä¸­æµ‹è¯•çš„æœ€å¥½ç»“æœ
    - best_estimator_ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
    - cv_results_:æ¯æ¬¡äº¤å‰éªŒè¯åçš„æµ‹è¯•é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ


##### 3.1.1.4.4. å­¦ä¹ æ›²çº¿

- æŸ¥çœ‹æŸä¸ªå‚æ•°ä¸åŒå–å€¼çš„scoreå˜åŒ–
- ä¾‹ï¼šéšæœºæ£®æ—çš„ n_estimators


#### 3.1.1.5. å†³ç­–æ ‘ä¸éšæœºæ£®æ—

##### 3.1.1.5.1. å†³ç­–æ ‘

> ä¼ä¸šè¿‡ç¨‹ä¸­ä½¿ç”¨è¾ƒå¤š

- åŸç†ï¼šä¿¡æ¯è®ºåŸºç¡€
  - ä¿¡æ¯ç†µçš„è®¡ç®—
  - æ¡ä»¶ç†µçš„è®¡ç®—
    > ![](./image/Information_entropy.jpg)
  - ä¿¡æ¯å¢ç›Šçš„è®¡ç®—
    > ä¿¡æ¯å¢ç›Šï¼šå½“å¾—çŸ¥ä¸€ä¸ªç‰¹å¾æ¡ä»¶ä¹‹åï¼Œå‡å°‘çš„ä¿¡æ¯ç†µçš„å¤§å°
    > ![](./image/Information_gain.jpg)

- ä¿¡æ¯å¢ç›Šå¤§çš„ä½œä¸ºæœ€å¼€å§‹çš„åˆ†ç±»ã€‚
  > ä¿¡æ¯å¢ç›Šè¶Šå¤§ï¼Œè¶Šæœ‰å¯èƒ½å¾—å‡ºç»“æœã€‚å¤§çš„æ”¾å‰é¢æœ‰åˆ©äºä¸è¿›è¡Œå¤šä½™çš„åˆ¤æ–­
  > ![](./image/Decision_tree.jpg)

- ç®—æ³•(äº†è§£)ï¼š
  > è¿™æ˜¯åœ¨sklearnä¸­å¯ä»¥é€‰æ‹©åˆ’åˆ†çš„åŸåˆ™
  > å¯¹äºé«˜ç»´æ•°æ®æˆ–è€…å™ªéŸ³å¾ˆå¤šçš„æ•°æ®ï¼Œä¿¡æ¯ç†µå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼ŒåŸºå°¼ç³»æ•°åœ¨è¿™ç§æƒ…å†µä¸‹æ•ˆæœå¾€å¾€æ¯”è¾ƒå¥½
  - ID3
    - ä¿¡æ¯å¢ç›Š æœ€å¤§çš„å‡†åˆ™ã€‚
  - C4.5
    - ä¿¡æ¯å¢ç›Šæ¯” æœ€å¤§çš„å‡†åˆ™
  - CART 
    - å›å½’æ ‘: å¹³æ–¹è¯¯å·® æœ€å° ã€‚
      - api:sklearn.tree.DecisionTreeRegressor
    - åˆ†ç±»æ ‘: åŸºå°¼ç³»æ•°   æœ€å°çš„å‡†åˆ™ 
- api:`sklearn.tree.DecisionTreeClassifier(criterion=â€™giniâ€™,Â max_depth=None,random_state=None)`
  - ä½¿ç”¨
    > å†³ç­–æ ‘åˆ†ç±»å™¨<br>
    > è¶…å‚æ•°æ”¾åœ¨éšæœºæ£®æ—
    - decision_path:è¿”å›å†³ç­–æ ‘çš„è·¯å¾„
    ```python
    DecisionTreeClassifier(
    *,
    criterion='gini', # é»˜è®¤æ˜¯â€™giniâ€™ç³»æ•°ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µâ€™entropyâ€™
    splitter='best',
    max_depth=None, # æ ‘çš„æ·±åº¦æœ€å¤§å¤§å°
    min_samples_split=2, # æ ·æœ¬æ•°å¤§äºä¸¤ä¸ªæ—¶æ‰ä¼šåˆ†å‰
    min_samples_leaf=1, # æ ·æœ¬æ•°å¤§äº1ä¸ªæ—¶ï¼ŒèŠ‚ç‚¹æ‰ä¼šè¢«ç•™ä¸‹
    min_weight_fraction_leaf=0.0,
    max_features=None, # é™åˆ¶åˆ†ææ—¶è€ƒè™‘çš„ç‰¹å¾ä¸ªæ•°ï¼Œè¶…è¿‡é™åˆ¶ä¸ªæ•°çš„ç‰¹å¾éƒ½ä¼šè¢«èˆå¼ƒ
    random_state=None, # éšæœºæ•°ç§å­
    max_leaf_nodes=None,
    min_impurity_decrease=0.0, # æ¯”è¾ƒéš¾è°ƒï¼Œä¸æ¨èã€‚ é™åˆ¶ä¿¡æ¯å¢ç›Šçš„å¤§å°ï¼Œä¿¡æ¯å¢ç›Šå°äºè®¾å®šæ•°å€¼çš„åˆ†æä¸ä¼šå‘ç”Ÿ
    min_impurity_split=None,
    class_weight=None,
    presort='deprecated',
    ccp_alpha=0.0,
    )
    ```
  - æŸ¥çœ‹featureçš„é‡è¦æ€§:`feature_importances_`
  - æŸ¥çœ‹å­¦ä¹ æ›²çº¿ï¼ˆå…¶å®å°±æ˜¯è°ƒæ•´max_depthï¼ŒæŸ¥çœ‹ä¸åŒmax_depthçš„æ­£ç¡®ç‡ï¼‰
  - æ¨¡å‹ä¿å­˜
    - 1ã€sklearn.tree.export_graphviz()Â è¯¥å‡½æ•°èƒ½å¤Ÿå¯¼å‡ºDOTæ ¼å¼
      ```
      tree.export_graphviz(estimator,out_file='tree.dotâ€™,feature_names=[â€˜â€™,â€™â€™]) 
      ```
    - 2ã€å·¥å…·:(èƒ½å¤Ÿå°†dotæ–‡ä»¶è½¬æ¢ä¸ºpdfã€png)
      ```
      å®‰è£…graphviz
      ubuntu:sudo apt-get install graphviz                    Mac:brew install graphviz 

      è¿è¡Œå‘½ä»¤
      ç„¶åæˆ‘ä»¬è¿è¡Œè¿™ä¸ªå‘½ä»¤
      $ dot -Tpng tree.dot -o tree.png
      ```

- ç¤ºä¾‹ï¼š
  - æ³°å¦å°¼å…‹å·å­˜æ´»ç‡

- ä¼˜ç¼ºç‚¹ï¼š
  - ä¼˜ç‚¹ï¼š
    - ç®€å•çš„ç†è§£å’Œè§£é‡Šï¼Œæ ‘æœ¨å¯è§†åŒ–ã€‚
    - éœ€è¦å¾ˆå°‘çš„æ•°æ®å‡†å¤‡ï¼Œå…¶ä»–æŠ€æœ¯é€šå¸¸éœ€è¦æ•°æ®å½’ä¸€åŒ–ï¼Œ
  - ç¼ºç‚¹ï¼š
    - å†³ç­–æ ‘å­¦ä¹ è€…å¯ä»¥åˆ›å»ºä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿æ•°æ®çš„è¿‡äºå¤æ‚çš„æ ‘ï¼Œ è¿™è¢«ç§°ä¸ºè¿‡æ‹Ÿåˆã€‚ 
    - å†³ç­–æ ‘å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºæ•°æ®çš„å°å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘è¢«ç”Ÿæˆ 
  - æ”¹è¿›ï¼š
    - å‡æcartç®—æ³•
      - åˆ›å»ºapiå¯¹è±¡æ—¶è®¾ç½®å‚æ•°ï¼š
        ```python
        min_samples_split=2, # æ ·æœ¬æ•°å¤§äºä¸¤ä¸ªæ—¶æ‰ä¼šåˆ†å‰
        min_samples_leaf=1, # æ ·æœ¬æ•°å¤§äº1ä¸ªæ—¶ï¼ŒèŠ‚ç‚¹æ‰ä¼šè¢«ç•™ä¸‹
        ```
    - éšæœºæ£®æ—


##### 3.1.1.5.2. éšæœºæ£®æ—

- é›†æˆå­¦ä¹ æ–¹æ³•ï¼š
  ```
  é›†æˆå­¦ä¹ é€šè¿‡å»ºç«‹å‡ ä¸ªæ¨¡å‹ç»„åˆçš„æ¥è§£å†³å•ä¸€é¢„æµ‹é—®é¢˜ã€‚
  å®ƒçš„å·¥ä½œåŸç†æ˜¯ç”Ÿæˆå¤šä¸ªåˆ†ç±»å™¨/æ¨¡å‹ï¼Œå„è‡ªç‹¬ç«‹åœ°å­¦ä¹ å’Œä½œå‡ºé¢„æµ‹ã€‚
  è¿™äº›é¢„æµ‹æœ€åç»“åˆæˆå•é¢„æµ‹ï¼Œå› æ­¤ä¼˜äºä»»ä½•ä¸€ä¸ªå•åˆ†ç±»çš„åšå‡ºé¢„æµ‹ã€‚
  ```
  - è£…è¢‹æ³•ï¼ˆBaggingï¼‰:æ„å»ºå¤šä¸ªç›¸äº’ç‹¬ç«‹çš„è¯„ä¼°å™¨ï¼Œç„¶åå¯¹å…¶é¢„æµ‹è¿›è¡Œå¹³å‡æˆ–å¤šæ•°è¡¨å†³åŸåˆ™æ¥å†³å®šé›†æˆè¯„ä¼°å™¨çš„ç»“æœã€‚ä»£è¡¨æ¨¡å‹å°±æ˜¯éšæœºæ£®æ—
  - æå‡æ³•ï¼ˆBoostingï¼‰:ï¼ŒåŸºè¯„ä¼°å™¨æ˜¯ç›¸å…³çš„ï¼Œæ˜¯æŒ‰é¡ºåºä¸€ä¸€æ„å»ºçš„ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç»“åˆå¼±è¯„ä¼°å™¨çš„åŠ›é‡ä¸€æ¬¡æ¬¡å¯¹éš¾ä»¥è¯„ä¼°çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œæ„æˆä¸€ä¸ªå¼ºè¯„ä¼°å™¨ã€‚æå‡æ³•çš„ä»£è¡¨æ¨¡å‹æœ‰Adaboostå’Œæ¢¯åº¦æå‡æ ‘ã€‚
  - stacking
  > ![ensemble-learning](./image/ensemble-learning.png) 

- éšæœºæ£®æ—ï¼š
  - åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œéšæœºæ£®æ—æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå†³ç­–æ ‘çš„åˆ†ç±»å™¨ï¼Œå¹¶ä¸”å…¶è¾“å‡ºçš„ç±»åˆ«æ˜¯ç”±ä¸ªåˆ«æ ‘è¾“å‡ºçš„ç±»åˆ«çš„**ä¼—æ•°**è€Œå®šã€‚

- éšæœºæ£®æ—åˆ›å»ºè¿‡ç¨‹ï¼š
  1. å•æ£µæ ‘åˆ›å»ºè¿‡ç¨‹
    1. éšæœºåœ¨Mä¸ªç‰¹å¾ä¸­é€‰å‡ºmä¸ªç‰¹å¾,måº”è¿œå°äºM
    2. éšæœºä»Nä¸ªæ ·æœ¬ä¸­é€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼Œé‡å¤Næ¬¡ã€‚éšæœºæ”¾å›æŠ½æ ·(bootstrapæŠ½æ ·),æ ·æœ¬æœ‰å¯èƒ½é‡å¤
    3. ä½¿ç”¨æŠ½å–çš„æ ·æœ¬ï¼Œè®­ç»ƒæ¨¡å‹,å¹¶ç”¨æœªæŠ½åˆ°çš„æ ·æœ¬ä½œé¢„æµ‹ï¼Œè¯„ä¼°å…¶è¯¯å·®ã€‚
  2. é‡å¤æ“ä½œï¼Œå»ºç«‹æŒ‡å®šæ•°é‡çš„å†³ç­–æ ‘

- é—®é¢˜ï¼š
  - ä¸ºä»€ä¹ˆè¦éšæœºæŠ½æ ·è®­ç»ƒé›†ï¼Ÿã€€ã€€
    ```
    å¦‚æœä¸è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œæ¯æ£µæ ‘çš„è®­ç»ƒé›†éƒ½ä¸€æ ·ï¼Œé‚£ä¹ˆæœ€ç»ˆè®­ç»ƒå‡ºçš„æ ‘åˆ†ç±»ç»“æœä¹Ÿæ˜¯å®Œå…¨ä¸€æ ·çš„ 
    ```
  - ä¸ºä»€ä¹ˆè¦æœ‰æ”¾å›åœ°æŠ½æ ·ï¼Ÿ
    ```
    å¦‚æœä¸æ˜¯æœ‰æ”¾å›çš„æŠ½æ ·ï¼Œé‚£ä¹ˆæ¯æ£µæ ‘çš„è®­ç»ƒæ ·æœ¬éƒ½æ˜¯ä¸åŒçš„ï¼Œéƒ½æ˜¯æ²¡æœ‰äº¤é›†çš„ï¼Œè¿™æ ·æ¯æ£µæ ‘éƒ½æ˜¯â€œæœ‰åçš„â€ï¼Œéƒ½æ˜¯ç»å¯¹â€œç‰‡é¢çš„â€ï¼ˆå½“ç„¶è¿™æ ·è¯´å¯èƒ½ä¸å¯¹ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯æ£µæ ‘è®­ç»ƒå‡ºæ¥éƒ½æ˜¯æœ‰å¾ˆå¤§çš„å·®å¼‚çš„ï¼›è€Œéšæœºæ£®æ—æœ€ååˆ†ç±»å–å†³äºå¤šæ£µæ ‘ï¼ˆå¼±åˆ†ç±»å™¨ï¼‰çš„æŠ•ç¥¨è¡¨å†³ã€‚
    ```
  - ä»€ä¹ˆæ˜¯è¢‹å¤–é›†
    ```
    bootstrapå‚æ•°é»˜è®¤Trueï¼Œä»£è¡¨é‡‡ç”¨è¿™ç§æœ‰æ”¾å›çš„éšæœºæŠ½æ ·æŠ€æœ¯ã€‚
    å½“nè¶³å¤Ÿå¤§æ—¶ï¼Œè¿™ä¸ªæ¦‚ç‡æ”¶æ•›äº1-(1/e)ï¼Œçº¦ç­‰äº0.632ã€‚
    å› æ­¤ï¼Œä¼šæœ‰çº¦37%çš„è®­ç»ƒæ•°æ®è¢«æµªè´¹æ‰ï¼Œæ²¡æœ‰å‚ä¸å»ºæ¨¡ï¼Œè¿™äº›æ•°æ®è¢«ç§°ä¸ºè¢‹å¤–æ•°æ®(out of bag dataï¼Œç®€å†™ä¸ºoob)ã€‚
    é™¤äº†æˆ‘ä»¬æœ€å¼€å§‹å°±åˆ’åˆ†å¥½çš„æµ‹è¯•é›†ä¹‹å¤–ï¼Œè¿™äº›æ•°æ®ä¹Ÿå¯ä»¥è¢«ç”¨æ¥ä½œä¸ºé›†æˆç®—æ³•çš„æµ‹è¯•é›†

    åœ¨ä½¿ç”¨éšæœºæ£®æ—æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¸åˆ’åˆ†æµ‹è¯•é›†å’Œè®­ç»ƒé›†ï¼Œåªéœ€è¦ç”¨è¢‹å¤–æ•°æ®æ¥æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹å³å¯ã€‚
    å½“ç„¶ï¼Œè¿™ä¹Ÿä¸æ˜¯ç»å¯¹çš„ï¼Œå½“nå’Œn_estimatorséƒ½ä¸å¤Ÿå¤§çš„æ—¶å€™ï¼Œ å¾ˆå¯èƒ½å°±æ²¡æœ‰æ•°æ®æ‰è½åœ¨è¢‹å¤–ï¼Œè‡ªç„¶ä¹Ÿå°±æ— æ³•ä½¿ç”¨oobæ•°æ®æ¥æµ‹è¯•æ¨¡å‹äº†ã€‚
    å¦‚æœå¸Œæœ›ç”¨è¢‹å¤–æ•°æ®æ¥æµ‹è¯•ï¼Œåˆ™éœ€è¦åœ¨å®ä¾‹åŒ–æ—¶å°±å°†oob_scoreè¿™ä¸ªå‚æ•°è°ƒæ•´ä¸ºTrueã€‚
    ```

- api:
  ```python
  classÂ sklearn.ensemble.RandomForestClassifier(
    n_estimators=10, #  æ£®æ—é‡Œçš„æ ‘æœ¨æ•°é‡ã€‚æ¨èï¼š120,200,300,500,800,1200
                    # n_estimatorsæ˜¯æ£®æ—ä¸­æ ‘æœ¨çš„æ•°é‡ï¼Œå³åŸºè¯„ä¼°å™¨çš„æ•°é‡ã€‚
                    # è¿™ä¸ªå‚æ•°å¯¹éšæœºæ£®æ—æ¨¡å‹çš„ç²¾ç¡®æ€§å½±å“æ˜¯å•è°ƒçš„ï¼Œn_estimatorsè¶Šå¤§ï¼Œæ¨¡å‹çš„æ•ˆæœå¾€å¾€è¶Šå¥½ã€‚
                    # ä½†æ˜¯ç›¸åº”çš„ï¼Œä»»ä½•æ¨¡å‹éƒ½æœ‰å†³ç­–è¾¹ç•Œï¼Œn_estimatorsè¾¾åˆ°ä¸€å®šçš„ç¨‹åº¦ä¹‹åï¼Œéšæœºæ£®æ—çš„ç²¾ç¡®æ€§å¾€å¾€ä¸åœ¨ä¸Šå‡æˆ–å¼€å§‹æ³¢åŠ¨ï¼Œ
                    # å¹¶ä¸”ï¼Œn_estimatorsè¶Šå¤§ï¼Œéœ€è¦çš„è®¡ç®—é‡å’Œå†…å­˜ä¹Ÿè¶Šå¤§ï¼Œè®­ç»ƒçš„æ—¶é—´ä¹Ÿä¼šè¶Šæ¥è¶Šé•¿ã€‚
    *,
    criterion=â€™giniâ€™, # åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•
    max_depth=None, #å¯é€‰ï¼ˆé»˜è®¤=æ— ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦  
    min_samples_split=2,
    min_samples_leaf=1,
    min_weight_fraction_leaf=0.0,
    max_features='auto', # æ¯æ£µå†³ç­–æ ‘é€‰å–çš„æœ€å¤§çš„ç‰¹å¾æ•°é‡ï¼ˆmçš„æœ€å¤§å€¼ï¼‰
      # æŸ¥æ–‡æ¡£å³å¯
      # - If int, then consider `max_features` features at each split.
      # - If float, then `max_features` is a fraction and
      #   `int(max_features * n_features)` features are considered at each
      #   split.
      # - If "auto", then `max_features=sqrt(n_features)`.
      # - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
      # - If "log2", then `max_features=log2(n_features)`.
      # - If None, then `max_features=n_features`.
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_impurity_split=None,
    bootstrap=True, # æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶ä½¿ç”¨æ”¾å›æŠ½æ · 
    oob_score=False, # æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ•°æ®
    n_jobs=None,
    random_state=None, 
          # random_stateç›¸åŒæ—¶ï¼ŒæŠ½æ ·ä¼šç›¸åŒï¼Œç”Ÿæˆçš„æ£®æ—ä¹Ÿç›¸åŒ
          # éšæœºæ£®æ—çš„é‡è¦å±æ€§ä¹‹ä¸€ï¼šestimatorsï¼ŒæŸ¥çœ‹æ£®æ—ä¸­æ ‘çš„çŠ¶å†µ
          # rfc.estimators_[0].random_state
          # éšæœºæ£®æ—ä¸­çš„random_stateæ§åˆ¶çš„æ˜¯ç”Ÿæˆæ£®æ—çš„æ¨¡å¼ï¼Œè€Œéè®©ä¸€ä¸ªæ£®æ—ä¸­åªæœ‰ä¸€æ£µæ ‘ 
          # æ¯æ£µæ ‘random_stateä¸åŒ
          # é€šè¿‡rfc.estimators_[0].random_state.random_stateæŸ¥çœ‹
    verbose=0,
    warm_start=False,
    class_weight=None,
    ccp_alpha=0.0,
    max_samples=None,
  )


  # æŸ¥çœ‹å„ä¸ªfeatureçš„é‡è¦åº¦
  [*zip(wine.feature_names ,rfc.feature_importances_)]
  #æ¯ä¸€ä¸ªæ ·æœ¬å¯¹åº”æ ‡ç­¾çš„æ¦‚ç‡
  rfc.predict_proba(Xtest)
  ```

- ä¼˜ç‚¹ï¼š
  - åœ¨å½“å‰æ‰€æœ‰ç®—æ³•ä¸­ï¼Œå…·æœ‰æå¥½çš„å‡†ç¡®ç‡
  - èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿è¡Œåœ¨å¤§æ•°æ®é›†ä¸Š
  - èƒ½å¤Ÿå¤„ç†å…·æœ‰é«˜ç»´ç‰¹å¾çš„è¾“å…¥æ ·æœ¬ï¼Œè€Œä¸”ä¸éœ€è¦é™ç»´
  - èƒ½å¤Ÿè¯„ä¼°å„ä¸ªç‰¹å¾åœ¨åˆ†ç±»é—®é¢˜ä¸Šçš„é‡è¦æ€§
  - å¯¹äºç¼ºçœå€¼é—®é¢˜ä¹Ÿèƒ½å¤Ÿè·å¾—å¾ˆå¥½å¾—ç»“æœ


- éšæœºæ£®æ—å›å½’ç³»
  - api: sklearn.ensemble.RandomForestRegressor

### 3.1.2. å›å½’

#### 3.1.2.1. çº¿æ€§å›å½’

> çœ‹æœºå™¨å­¦ä¹ æ–‡æ¡£

- api:
  - æ­£è§„æ–¹ç¨‹ï¼šsklearn.linear_model.LinearRegression()
    - æ™®é€šæœ€å°äºŒä¹˜æ³•çº¿æ€§å›å½’
    - coef_ï¼šå›å½’ç³»æ•°
  - æ¢¯åº¦ä¸‹é™ï¼šsklearn.linear_model.SGDRegressor( )
    - é€šè¿‡ä½¿ç”¨SGDæœ€å°åŒ–çº¿æ€§æ¨¡å‹
    - coef_ï¼šå›å½’ç³»æ•°
- å›å½’è¯„ä¼°API:sklearn.metrics.mean_squared_error
  > å‡æ–¹è¯¯å·®å›å½’æŸå¤±
  - y_true:çœŸå®å€¼
  - y_pred:é¢„æµ‹å€¼
    > æ³¨ï¼šçœŸå®å€¼ï¼Œé¢„æµ‹å€¼ä¸ºæ ‡å‡†åŒ–ä¹‹å‰çš„å€¼
  - return:æµ®ç‚¹æ•°ç»“æœ

- ä¸¤ç§å›å½’æ¯”è¾ƒï¼š
  > ![](./image/regression.jpg)
- LinearRegressionä¸SGDRegressorè¯„ä¼°
  - ç‰¹ç‚¹ï¼šçº¿æ€§å›å½’å™¨æ˜¯æœ€ä¸ºç®€å•ã€æ˜“ç”¨çš„å›å½’æ¨¡å‹ã€‚
  - ä»æŸç§ç¨‹åº¦ä¸Šé™åˆ¶äº†ä½¿ç”¨ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œåœ¨ä¸çŸ¥é“ç‰¹å¾ä¹‹ é—´å…³ç³»çš„å‰æä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨çº¿æ€§å›å½’å™¨ä½œä¸ºå¤§å¤šæ•° ç³»ç»Ÿçš„é¦–è¦é€‰æ‹©ã€‚
    - å°è§„æ¨¡æ•°æ®ï¼šLinearRegression(ä¸èƒ½è§£å†³æ‹Ÿåˆé—®é¢˜)ä»¥åŠå…¶å®ƒ
    - å¤§è§„æ¨¡æ•°æ®ï¼šSGDRegressor
  
- çº¿æ€§å›å½’é—®é¢˜ï¼š
  - æ¬ æ‹Ÿåˆ
  - è¿‡æ‹Ÿåˆ
  - è§£å†³ï¼š
    - è¿‡æ»¤å¼ï¼šé€‰æ‹©åœ°æ–¹å·®ç‰¹å¾
    - åµŒå…¥å¼ï¼šå†³ç­–æ ‘ï¼Œç¥ç»ç½‘ç»œï¼Œæ­£åˆ™åŒ–ï¼ˆå²­å›å½’ï¼‰

#### 3.1.2.2. å²­å›å½’

- L2æ­£åˆ™åŒ–ï¼šå²­å›å½’ï¼Œå¸¦æœ‰æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’
- api:sklearn.linear_model.Ridge
  > å…·æœ‰l2æ­£åˆ™åŒ–çš„çº¿æ€§æœ€å°äºŒä¹˜æ³•
  - alpha(0~1):æ­£åˆ™åŒ–åŠ›åº¦
    > **é¢è¯•é—®é¢˜ï¼š**å›å½’è¿‡æ‹Ÿåˆè°ƒä¼˜æ–¹å¼ï¼Ÿ<br>
    > æ­£åˆ™åŒ–ï¼Œå²­å›å½’ï¼Œè°ƒæ•´alphaå‚æ•°å¤§å°
  - coef_:å›å½’ç³»æ•°

> å²­å›å½’ï¼šå›å½’å¾—åˆ°çš„å›å½’ç³»æ•°æ›´ç¬¦åˆå®é™…ï¼Œæ›´å¯é ã€‚å¦å¤–ï¼Œèƒ½è®© ä¼°è®¡å‚æ•°çš„æ³¢åŠ¨èŒƒå›´å˜å°ï¼Œå˜çš„æ›´ç¨³å®šã€‚åœ¨å­˜åœ¨ç—…æ€æ•°æ®åå¤šçš„ç ” ç©¶ä¸­æœ‰è¾ƒå¤§çš„å®ç”¨ä»·å€¼ã€‚

#### 3.1.2.3. sklearnæ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

> æ–‡ä»¶æ ¼å¼ï¼špkl
- ä¿å­˜ï¼šjoblib.dump(rf,"test.pkl")
- åŠ è½½ï¼šestimator = joblib.load("test.pkl")

#### 3.1.2.4. é€»è¾‘å›å½’

> ç”¨äºäºŒåˆ†ç±»

- åº”ç”¨ï¼š
  - å¹¿å‘Šç‚¹å‡»ç‡ 
  - åˆ¤æ–­ç”¨æˆ·çš„æ€§åˆ«
  - é¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šè´­ä¹°ç»™å®šçš„å•†å“ç±» 
  - åˆ¤æ–­ä¸€æ¡è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„
- ä¼˜ç‚¹ï¼šé€‚åˆéœ€è¦å¾—åˆ°ä¸€ä¸ªåˆ†ç±»æ¦‚ç‡çš„åœºæ™¯
- ç¼ºç‚¹ï¼šå½“ç‰¹å¾ç©ºé—´å¾ˆå¤§æ—¶ï¼Œé€»è¾‘å›å½’çš„æ€§èƒ½ä¸æ˜¯å¾ˆå¥½ ï¼ˆçœ‹ç¡¬ä»¶èƒ½åŠ›ï¼‰
- åŸç†ï¼šsigmoidå‡½æ•°ï¼Œå°†å˜é‡æ˜ å°„åˆ°0-1ä¹‹é—´
  > ![](./image/sigmoid.jpg)

- api: sklearn.linear_model.LogisticRegression
  ```py
  LogisticRegression(
    penalty='l2',
    *,
    dual=False,
    tol=0.0001,
    C=1.0, # å­¦ä¹ ç‡
    fit_intercept=True,
    intercept_scaling=1,
    class_weight=None,
    random_state=None,
    solver='lbfgs',
    max_iter=100,
    multi_class='auto',
    verbose=0,
    warm_start=False,
    n_jobs=None,
    l1_ratio=None,
  )
  ```
- å¤šåˆ†ç±»é—®é¢˜ï¼šsoftmax-æ–¹æ³•ï¼Œå°†åœ¨åé¢ç¥ç»ç½‘ç»œç®—æ³•ä¸­ä»‹ç»

- ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ï¼š
  - åˆ¤åˆ«æ¨¡å‹:å¦‚æœ´ç´ è´å¶æ–¯ç®—æ³•ï¼Œéšé©¬å¯å¤«æ¨¡å‹ï¼Œä¸€å¼€å§‹éœ€è¦ä»æ•°æ®ä¸­æ±‚å¾—ä¸€äº›æ¦‚ç‡
  - ç”Ÿæˆæ¨¡å‹:å¦‚é€»è¾‘å›å½’ï¼Œå†³ç­–æ ‘ç­‰ç­‰ï¼Œä¸éœ€è¦äº‹å…ˆå¤„ç†æ•°æ®

#### 3.1.2.5. ç¥ç»ç½‘ç»œ

### 3.1.3. æ ‡æ³¨

#### 3.1.3.1. éšé©¬å°”å¯å¤«æ¨¡å‹

## 3.2. æ— ç›‘ç£å­¦ä¹ 

### 3.2.1. èšç±»

#### 3.2.1.1. k-means

- æ— ç›‘ç£å­¦ä¹ ï¼šåªæœ‰ç‰¹å¾å€¼ï¼Œæ²¡æœ‰ç›®æ ‡å€¼
- k-means:èšç±»
- è¿‡ç¨‹ï¼š
  ![](./image/k-means-1.jpg)
- api:`sklearn.cluster.KMeans`
  - n_clusters:å¼€å§‹çš„èšç±»ä¸­å¿ƒæ•°é‡
  - init:åˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸º'k-means ++'
  - labels_:é»˜è®¤æ ‡è®°çš„ç±»å‹ï¼Œå¯ä»¥å’ŒçœŸå®å€¼æ¯”è¾ƒï¼ˆä¸æ˜¯å€¼æ¯”è¾ƒï¼‰
- è¯„ä¼°æ ‡å‡†ï¼š
  > ![](./image/k-means-2.jpg)
  - å¦‚æœã€–ğ‘ ğ‘ã€—_ğ‘– å°äº0ï¼Œè¯´æ˜ğ‘_ğ‘– çš„å¹³å‡è·ç¦»å¤§äºæœ€è¿‘çš„å…¶ä»–ç°‡ã€‚ èšç±»æ•ˆæœä¸å¥½
  - å¦‚æœã€–ğ‘ ğ‘ã€—_ğ‘– è¶Šå¤§ï¼Œè¯´æ˜ğ‘_ğ‘– çš„å¹³å‡è·ç¦»å°äºæœ€è¿‘çš„å…¶ä»–ç°‡ã€‚ èšç±»æ•ˆæœå¥½
  - è½®å»“ç³»æ•°çš„å€¼æ˜¯ä»‹äº [-1,1] ï¼Œè¶Šè¶‹è¿‘äº1ä»£è¡¨å†…èšåº¦å’Œåˆ†ç¦»åº¦éƒ½ç›¸å¯¹è¾ƒä¼˜ 
- è¯„ä¼°apiï¼š `sklearn.metrics.silhouette_score`
  > è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°
  - Xï¼šç‰¹å¾å€¼
  - labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼

- ç¼ºç‚¹ï¼š
  - å®¹æ˜“æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£(å¤šæ¬¡èšç±»)
  - éœ€è¦é¢„å…ˆè®¾å®šç°‡çš„æ•°é‡(k-means++è§£å†³)


