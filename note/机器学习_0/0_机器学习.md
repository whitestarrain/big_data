# 1. 开始

- 发展：
  - 人工智能：最早出现的概念，指一些自动化的程序
  - 机器学习：对数据进行自动分析获得规律
  - 深度学习：机器学习中较为复杂的算法，比如图像识别等
    > ![](./img/history.jpg)
- 影响因素；
  - 硬件计算水平
  - 数据量多少
  - 算法发展
- 应用：

  - 自然语言处理
  - 图像识别
  - 传统预测

- 机器学习库和框架

  - 前面部分使用：scikit learn
  - 后面部分使用：tensorflow(用的最火)
  - 框架列表：
    > ![](./img/ML_frame.jpg)

- 书籍推荐

  > ![](./img/books.jpg)

- 数据集：

  > ![](./img/data-set.jpg)

- 数据集结构：

  - 特征值，即 x
  - 目标值，即 y

- 基本流程：
  > ![]("./img/Basic_process.jpg")

# 2. scikit-learn

## 2.1. 数据抽取

### 2.1.1. 字典数据抽取

#### 2.1.1.1. DictVectorizer

将字典转换为矩阵

```py
# 导包
from sklearn.feature_extraction import DictVectorizer

def main():
    # 实例化数据抽取对象
    # 默认sparse为true，会将矩阵转换为sparse
    dictVector = DictVectorizer(sparse=False)

    # 调用fit_transform
    rdata=dictVector.fit_transform(
        [
          {
              "name": 1,
              "value": "value1"
          },
          {
              "name": 2,
              "value": "value2"
          },
          {
              "name": 3,
              "value": "value3"
          }
        ]
    )
    print(dictVector.get_feature_names())
    print((rdata))
    print(type(rdata))
    """
    输出：
    ['name', 'value=value1', 'value=value2', 'value=value3']
    [[1. 1. 0. 0.]
    [2. 0. 1. 0.]
    [3. 0. 0. 1.]]
    <class 'numpy.ndarray'>

    数字为值的第一列为name;字符串为值的value会转换为第二，三，四列，true为1，false为0(one-hot编码)
    默认会通过sparse压缩，所以不必担心内存

    sparse矩阵：
      (0, 0)        1.0
      (0, 1)        1.0
      (1, 0)        2.0
      (1, 2)        1.0
      (2, 0)        3.0
      (2, 3)        1.0
    """

if __name__ == "__main__":
    main()

```

### 2.1.2. 文本特征抽取

#### 2.1.2.1. Count

> 统计单词出现频率

```py
# 导入特征抽取类
from sklearn.feature_extraction.text import CountVectorizer

def main():
    cv = CountVectorizer()
    # 实例化
    rdata = cv.fit_transform(
        ["life is short,I use python", "life is too long,i don't use python"])
        # 传入两篇文章
    print(rdata)
    print(cv.get_feature_names())
    print((rdata.toarray()))
    pass

if __name__ == "__main__":
    main()
"""
结果：
  (0, 2)        1
  (0, 1)        1
  (0, 5)        1
  (0, 7)        1
  (0, 4)        1
  (1, 2)        1
  (1, 1)        1
  (1, 7)        1
  (1, 4)        1
  (1, 6)        1
  (1, 3)        1
  (1, 0)        1
['don', 'is', 'life', 'long', 'python', 'short', 'too', 'use']
[[0 1 1 0 1 1 0 1]
 [1 1 1 1 1 0 1 1]]

1. 统计文章中所有出现的词，重复词之看作一次，作为feature_names
2. 一行对应一篇文章，对每篇文章，统计出词的出现次数
3. 单个字母不统计
4. 中文默认不支持特征抽取，因为不能自动分词，除非用空格划开。可以使用 jieba 进行分词
"""
```

#### 2.1.2.2. tf-idf

> 已过时

- tf(term frequency):单词频率
- idf(inverse document frequency):逆文档频率

  > `log(总文档数量/改词出现的文档数+1)`<br>
  > 比如`我们`，`今天`这些常用词不应该计入考虑，可以通过逆文档频率判断重要性

- 计算公式
  > ![](./img/tf-idf.jpg)

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def main():
    str1 = "1、今天很残酷，明天更残酷，后天很美好， 但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    str2 = "2、我们看到的从很远星系来的光是在几百万年之前发出的， 这样当我们看到宇宙时，我们是在看它的过去。"
    str3 = "3、如果只用一种方式了解某样事物，你就不会真正了解它。 了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"
    str1_cut = jieba.cut(str1)
    str2_cut = jieba.cut(str2)
    str3_cut = jieba.cut(str3)
    str1_s = " ".join(str1_cut)
    str2_s = " ".join(str2_cut)
    str3_s = " ".join(str3_cut)
    tifid = TfidfVectorizer()
    rdata = tifid.fit_transform([str1_s, str2_s, str3_s])
    print(tifid.get_feature_names())
    print(rdata.toarray())
    print(rdata)

if __name__ == "__main__":
    main()

"""
['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '这样']
[[0.         0.         0.21821789 0.         0.         0.
  0.43643578 0.         0.         0.         0.         0.
  0.21821789 0.         0.21821789 0.         0.         0.
  0.         0.21821789 0.21821789 0.         0.43643578 0.
  0.21821789 0.         0.43643578 0.21821789 0.         0.
  0.         0.21821789 0.21821789 0.         0.         0.        ]
 [0.         0.         0.         0.2410822  0.         0.
  0.         0.2410822  0.2410822  0.2410822  0.         0.
  0.         0.         0.         0.         0.         0.2410822
  0.55004769 0.         0.         0.         0.         0.2410822
  0.         0.         0.         0.         0.48216441 0.
  0.         0.         0.         0.         0.2410822  0.2410822 ]
 [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891
  0.         0.         0.         0.         0.15698297 0.15698297
  0.         0.15698297 0.         0.15698297 0.15698297 0.
  0.1193896  0.         0.         0.15698297 0.         0.
  0.         0.15698297 0.         0.         0.         0.31396594
  0.15698297 0.         0.         0.15698297 0.         0.        ]]

  # 一个数组对应一篇文章，数大小对应词在该篇文章中的的重要性
"""
```

## 2.2. 数据预处理

### 2.2.1. 概述

- 定义：通过特定的统计方法（数学方法）将数据转换成算法要求的数据

- 处理：

  - 数值型数据：标准缩放：
    - 1、归一化
    - 2、标准化
    - 3、缺失值(pandas 和 numpy 都可以进行处理)
  - 类别型数据：one-hot 编码 （字典类型数据抽取）
  - 时间类型：时间的切分(根据情况处理字符串)

- api：`sklearn.preprocessing`

### 2.2.2. 归一化 Normalization

> 不怎么使用

- 特点：通过对原始数据进行变换把数据映射到一定范围(默认为[0,1])之间
- 目的：避免因数值而影响特征的。
  > 比如身高和体重 与 健康程度的关系，因为本身就不是同一范围，直接使用的话，所占权也不同
- 缺点：受异常点影响打（特别是最大最小值受影响时），鲁棒性较差。**只适合传统精确小数据**

  - 鲁棒性 Robustness，也就是健壮性

- 公式：

  > ![](./img/Normalization.jpg)

- api:`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - MinMaxScalar.fit_transform(X)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array

```py
from sklearn.preprocessing import MinMaxScaler

def main():
    mm = MinMaxScaler()
    # 默认就是 mm = MinMaxScaler(feature_range=(0,1))
    rdata = mm.fit_transform( [[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])
    print(mm.feature_range)
    print(rdata)
    pass

if __name__ == "__main__":
    main()
```

### 2.2.3. 标准化 standardization

- 特点：通过对原始数据进行变换把数据变换到均值为 0,方差为 1 范围内
  > 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景
- 公式：

  > ![](./img/standardization.jpg)

- api:`sklearn.preprocessing.StandardScaler`
  - 处理之后每列来说所有数据都聚集在
    - 均值:0 附近
    - 方差为 1
  - StandardScaler.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array
  - StandardScaler.mean\_
    - 原始数据中每列特征的平均值
  - StandardScaler.std\_
    - 原始数据每列特征的方差

```py
from sklearn.preprocessing import StandardScaler

def main():
    stdv = StandardScaler()
    rdata = stdv.fit_transform( [[ 1., -1., 3.], [ 2., 4., 2.], [ 4., 6., -1.]])
    print(rdata)

if __name__ == "__main__":
    main()
```

### 2.2.4. 缺失值处理

> 单纯用 numpy 和 pandas 就能完成

- 插补：可以通过缺失值每行或者每列的平均值、中位数来填充
- api:`sklearn.preprocessing.Imputer`
  - Imputer(missing_values='NaN', strategy='mean', axis=0)
    - 完成缺失值插补
  - Imputer.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array

## 2.3. 数据降维

- 维度：特征的数量

### 2.3.1. 特征选择

#### 2.3.1.1. 说明

- 原因：

  - 冗余：部分特征的相关度高，容易消耗计算性能
  - 噪声：部分特征对预测结果有负影响

- 定义：
  ```
  特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，
  特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯
  定比选择前小，毕竟我们只选择了其中的一部分特征。
  ```
- 主要方法:
  - **Filter(过滤式):VarianceThreshold**
    > variance:方差<br>
    > threshold:入口；门槛；开始；极限；临界值
  - **Embedded(嵌入式)：正则化、决策树**:后面算法时再讲
  - Wrapper(包裹式)：基本不用
- 其他方法：
  - **神经网络**，之后再说
  - 线性判别分析 LDA（基本不用）

#### 2.3.1.2. VarianceThreshold

- api:`sklearn.feature_selection.VarianceThreshold`
  - VarianceThreshold(threshold = 0.0)
    > 默认 threshold=0.0。要根据实际情况取值
    - 删除所有低方差特征
  - Variance.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：训练集差异低于 threshold 的特征将被删除。
    - 默认值是保留所有非零方差特征，即删除所有样本
    - 中具有相同值的特征。

```py
from sklearn.feature_selection import VarianceThreshold

def main():
    vt = VarianceThreshold()
    rdata = vt.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])
    print(rdata)

if __name__ == "__main__":
    main()
    """
    结果：
    [[2 0]
    [1 4]
    [1 1]]

    即删除了一样的列
    """
```

### 2.3.2. PCA(主成分分析)

#### 2.3.2.1. 说明

> 当特征成百上千时，就需要考虑了

> **原理自己查**

- 本质：PCA 是一种分析、简化数据集的技术。
- 目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
  > 比如 feature1 和 featrue2 间为线性关系，那么就可以删除其中一个
- 作用：可以削减回归分析或者聚类分析中特征的数量
- api:`sklearn.decomposition.PCA`
  - PCA(n_components=None)
    > n_components:<br>
    > 小数，0~1 的百分比，表示信息的损失量，一般为 0.9~0.95<br>
    > 整数：减少到的特征数量。**一般不用**
    - 将数据分解为较低维数空间
  - PCA.fit_transform(X)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后指定维度的 array

```py
from sklearn.decomposition import PCA

def main():
    pca = PCA(n_components=0.9)
    rdata = pca.fit_transform([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])
    print(rdata)

if __name__ == "__main__":
    main()
```

#### 2.3.2.2. 降维案例

[数据](https://www.kaggle.com/c/instacart-market-basket-analysis/data)

## 2.4. 数据集

> scikit-learn有内置数据集

- 可将数据集划分为划分为：
  - 训练数据集
  - 测试数据集

> ![](./img/data_set.jpg)

- api:
  - 获取：
    - sklearn.datasets
      > 加载获取流行数据集
      - `datasets.load_*()`
        > 获取小规模数据集，数据包含在datasets里
      - `datasets.fetch_*(data_home=None)`
        > 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/<br>
        > subset: 'train'或者'test','all'，可选，选择要加载的数据集. 训练集的“训练”，测试集的“测试”，两者的“全部”
    - 数据格式；
      - load*和fetch*返回的数据类型datasets.base.Bunch(字典格式)
      - data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
      - target：目标值数组，是 n_samples 的一维 numpy.ndarray 数组
      - DESCR：数据描述
      - feature_names：特征名,新闻数据，手写数字、回归数据集没有
      - target_names：标签名,回归数据集没有
  - 划分：`sklearn.model_selection.train_test_split(*arrays, **options)`
    - x:  数据集的特征值
    - y:  数据集的标签值
    - test_size      测试集的大小，一般为float
    - random_state        随机数种子,不同的种子会造成不同的随机 采样结果。相同的种子采样结果相同。
    - return  训练集特征值，测试集特征值，训练标签，测试标签 (默认随机取)	
  - 清楚：
    - `datasets.clear_data_home(data_home=None)`
      > 清除目录下的数据

## 2.5. 转换器与估计器

### 2.5.1. 转换器

- fit():输入数据，但不进行处理。但会计算些平均值和方差，或者词列表等前提操作，这些值会存储到处理对象中，比如StandScaler
  > **所以不能fit完一个数据，再transform另一个数据，会乱套**
- transform():数据处理。根据fit()得到的结果，再进行处理
- fit_transform():输入数据，直接转换。**一般使用这个即可，上面两个基本不用**
  > 等于 fit()+transform()


### 2.5.2. 估计器

> 实现算法api

- 用于分类的估计器
  - sklearn.neighbors	k-近邻算法
  - sklearn.naive_bayes      贝叶斯
  - sklearn.linear_model.LogisticRegression     逻辑回归
- 用于回归的估计器
  - sklearn.linear_model.LinearRegression     线性回归
  - sklearn.linear_model.Ridge      岭回归 

## 2.6. api总结

### 2.6.1. api总结

- 数据抽取：
  - 字典类型数据抽取:`sklearn.feature_extraction.DictVectorizer`
  - 文本特征数据抽取:
    - count:`sklearn.feature_extraction.text.CountVectorizer`
    - tf-idf:`sklearn.feature_extraction.text.TfidfVectorizer`
- 数据预处理
  - 归一化：`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - 标准化:`sklearn.preprocessing.StandardScaler`
  - 缺失值:`sklearn.preprocessing.Imputer`
- 数据降维
  - 特征选择:`sklearn.feature_selection.VarianceThreshold`
  - PCA:`sklearn.decomposition.PCA`


### 2.6.2. api大致使用流程

![](./img/sklearn_api_line.jpg)

# 3. 基本算法

## 3.1. 监督学习

### 3.1.1. 分类

#### 3.1.1.1. k-近邻算法

> 基本不咋用

- 看《算法图解》
- 需要做标准化处理
- api:sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
  - n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数 
  - algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)

- 优点：
  - 简单，易于理解，易于实现，无需估计参数，无需训练
- 缺点：
  - 懒惰算法，对测试样本分类时的计算量大，内存开销大
  - 必须指定K值，K值选择不当则分类精度不能保证


#### 3.1.1.2. 朴素贝叶斯分类

- 特点
  - 无法传入参数进行调整，训练集影响大
  - 不需要调参
  - 训练集误差会大大影响结果

- 优点：
  - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
  - 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
  - 分类准确度高，速度快
- 缺点：
  - 由于使用了样本属性独立性的假设，所以如果样本属性有关联时 其效果不好

> 神经网络的效果要比朴素贝叶斯要好

#### 3.1.1.3. 决策树与随机森林

#### 3.1.1.4. 逻辑回归

#### 3.1.1.5. 神经网络

### 3.1.2. 回归

#### 3.1.2.1. 线性回归

#### 3.1.2.2. 岭回归

### 3.1.3. 标注

#### 3.1.3.1. 隐马尔可夫模型

## 3.2. 无监督学习

### 3.2.1. 聚类

#### 3.2.1.1. k-means
