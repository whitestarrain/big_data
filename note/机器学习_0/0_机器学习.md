# 1. å¼€å§‹

- å‘å±•ï¼š
  - äººå·¥æ™ºèƒ½ï¼šæœ€æ—©å‡ºç°çš„æ¦‚å¿µï¼ŒæŒ‡ä¸€äº›è‡ªåŠ¨åŒ–çš„ç¨‹åº
  - æœºå™¨å­¦ä¹ ï¼šå¯¹æ•°æ®è¿›è¡Œè‡ªåŠ¨åˆ†æè·å¾—è§„å¾‹
  - æ·±åº¦å­¦ä¹ ï¼šæœºå™¨å­¦ä¹ ä¸­è¾ƒä¸ºå¤æ‚çš„ç®—æ³•ï¼Œæ¯”å¦‚å›¾åƒè¯†åˆ«ç­‰
    > ![](./img/history.jpg)
- å½±å“å› ç´ ï¼›
  - ç¡¬ä»¶è®¡ç®—æ°´å¹³
  - æ•°æ®é‡å¤šå°‘
  - ç®—æ³•å‘å±•
- åº”ç”¨ï¼š

  - è‡ªç„¶è¯­è¨€å¤„ç†
  - å›¾åƒè¯†åˆ«
  - ä¼ ç»Ÿé¢„æµ‹

- æœºå™¨å­¦ä¹ åº“å’Œæ¡†æ¶

  - å‰é¢éƒ¨åˆ†ä½¿ç”¨ï¼šscikit learn
  - åé¢éƒ¨åˆ†ä½¿ç”¨ï¼štensorflow(ç”¨çš„æœ€ç«)
  - æ¡†æ¶åˆ—è¡¨ï¼š
    > ![](./img/ML_frame.jpg)

- ä¹¦ç±æ¨è

  > ![](./img/books.jpg)

- æ•°æ®é›†ï¼š

  > ![](./img/data-set.jpg)

- æ•°æ®é›†ç»“æ„ï¼š

  - ç‰¹å¾å€¼ï¼Œå³ x
  - ç›®æ ‡å€¼ï¼Œå³ y

- åŸºæœ¬æµç¨‹ï¼š
  > ![]("./img/Basic_process.jpg")

# 2. scikit-learn

## 2.1. ä¼˜ç¼ºç‚¹

- sklearn
  - æœ‰ç‚¹ï¼šå°è£…å¥½ï¼Œå»ºç«‹æ¨¡å‹ç®€å•ï¼Œé¢„æµ‹ç®€å•
  - ç¼ºç‚¹ï¼šç®—æ³•çš„è¿‡ç¨‹ï¼Œéƒ¨åˆ†å‚æ•°éƒ½åœ¨å†…éƒ¨è‡ªè¡Œä¼˜åŒ–
- tensorflow:å°è£…é«˜ä½çš„apiéƒ½æœ‰ï¼Œå¯ä»¥è‡ªå·±å®ç°ç®—æ³•

## 2.2. æ•°æ®æŠ½å–

### 2.2.1. å­—å…¸æ•°æ®æŠ½å–

#### 2.2.1.1. DictVectorizer

å°†å­—å…¸è½¬æ¢ä¸ºçŸ©é˜µ

```py
# å¯¼åŒ…
from sklearn.feature_extraction import DictVectorizer

def main():
    # å®ä¾‹åŒ–æ•°æ®æŠ½å–å¯¹è±¡
    # é»˜è®¤sparseä¸ºtrueï¼Œä¼šå°†çŸ©é˜µè½¬æ¢ä¸ºsparse
    dictVector = DictVectorizer(sparse=False)

    # è°ƒç”¨fit_transform
    rdata=dictVector.fit_transform(
        [
          {
              "name": 1,
              "value": "value1"
          },
          {
              "name": 2,
              "value": "value2"
          },
          {
              "name": 3,
              "value": "value3"
          }
        ]
    )
    print(dictVector.get_feature_names())
    print((rdata))
    print(type(rdata))
    """
    è¾“å‡ºï¼š
    ['name', 'value=value1', 'value=value2', 'value=value3']
    [[1. 1. 0. 0.]
    [2. 0. 1. 0.]
    [3. 0. 0. 1.]]
    <class 'numpy.ndarray'>

    æ•°å­—ä¸ºå€¼çš„ç¬¬ä¸€åˆ—ä¸ºname;å­—ç¬¦ä¸²ä¸ºå€¼çš„valueä¼šè½¬æ¢ä¸ºç¬¬äºŒï¼Œä¸‰ï¼Œå››åˆ—ï¼Œtrueä¸º1ï¼Œfalseä¸º0(one-hotç¼–ç )
    é»˜è®¤ä¼šé€šè¿‡sparseå‹ç¼©ï¼Œæ‰€ä»¥ä¸å¿…æ‹…å¿ƒå†…å­˜

    sparseçŸ©é˜µï¼š
      (0, 0)        1.0
      (0, 1)        1.0
      (1, 0)        2.0
      (1, 2)        1.0
      (2, 0)        3.0
      (2, 3)        1.0
    """

if __name__ == "__main__":
    main()

```

### 2.2.2. æ–‡æœ¬ç‰¹å¾æŠ½å–

#### 2.2.2.1. Count

> ç»Ÿè®¡å•è¯å‡ºç°é¢‘ç‡

```py
# å¯¼å…¥ç‰¹å¾æŠ½å–ç±»
from sklearn.feature_extraction.text import CountVectorizer

def main():
    cv = CountVectorizer()
    # å®ä¾‹åŒ–
    rdata = cv.fit_transform(
        ["life is short,I use python", "life is too long,i don't use python"])
        # ä¼ å…¥ä¸¤ç¯‡æ–‡ç« 
    print(rdata)
    print(cv.get_feature_names())
    print((rdata.toarray()))
    pass

if __name__ == "__main__":
    main()
"""
ç»“æœï¼š
  (0, 2)        1
  (0, 1)        1
  (0, 5)        1
  (0, 7)        1
  (0, 4)        1
  (1, 2)        1
  (1, 1)        1
  (1, 7)        1
  (1, 4)        1
  (1, 6)        1
  (1, 3)        1
  (1, 0)        1
['don', 'is', 'life', 'long', 'python', 'short', 'too', 'use']
[[0 1 1 0 1 1 0 1]
 [1 1 1 1 1 0 1 1]]

1. ç»Ÿè®¡æ–‡ç« ä¸­æ‰€æœ‰å‡ºç°çš„è¯ï¼Œé‡å¤è¯ä¹‹çœ‹ä½œä¸€æ¬¡ï¼Œä½œä¸ºfeature_names
2. ä¸€è¡Œå¯¹åº”ä¸€ç¯‡æ–‡ç« ï¼Œå¯¹æ¯ç¯‡æ–‡ç« ï¼Œç»Ÿè®¡å‡ºè¯çš„å‡ºç°æ¬¡æ•°
3. å•ä¸ªå­—æ¯ä¸ç»Ÿè®¡
4. ä¸­æ–‡é»˜è®¤ä¸æ”¯æŒç‰¹å¾æŠ½å–ï¼Œå› ä¸ºä¸èƒ½è‡ªåŠ¨åˆ†è¯ï¼Œé™¤éç”¨ç©ºæ ¼åˆ’å¼€ã€‚å¯ä»¥ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯
"""
```

#### 2.2.2.2. tf-idf

> å·²è¿‡æ—¶

- tf(term frequency):å•è¯é¢‘ç‡
- idf(inverse document frequency):é€†æ–‡æ¡£é¢‘ç‡

  > `log(æ€»æ–‡æ¡£æ•°é‡/æ”¹è¯å‡ºç°çš„æ–‡æ¡£æ•°+1)`<br>
  > æ¯”å¦‚`æˆ‘ä»¬`ï¼Œ`ä»Šå¤©`è¿™äº›å¸¸ç”¨è¯ä¸åº”è¯¥è®¡å…¥è€ƒè™‘ï¼Œå¯ä»¥é€šè¿‡é€†æ–‡æ¡£é¢‘ç‡åˆ¤æ–­é‡è¦æ€§

- è®¡ç®—å…¬å¼
  > ![](./img/tf-idf.jpg)

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def main():
    str1 = "1ã€ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œ ä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚"
    str2 = "2ã€æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œ è¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚"
    str3 = "3ã€å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚ äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"
    str1_cut = jieba.cut(str1)
    str2_cut = jieba.cut(str2)
    str3_cut = jieba.cut(str3)
    str1_s = " ".join(str1_cut)
    str2_s = " ".join(str2_cut)
    str3_s = " ".join(str3_cut)
    tifid = TfidfVectorizer()
    rdata = tifid.fit_transform([str1_s, str2_s, str3_s])
    print(tifid.get_feature_names())
    print(rdata.toarray())
    print(rdata)

if __name__ == "__main__":
    main()

"""
['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿™æ ·']
[[0.         0.         0.21821789 0.         0.         0.
  0.43643578 0.         0.         0.         0.         0.
  0.21821789 0.         0.21821789 0.         0.         0.
  0.         0.21821789 0.21821789 0.         0.43643578 0.
  0.21821789 0.         0.43643578 0.21821789 0.         0.
  0.         0.21821789 0.21821789 0.         0.         0.        ]
 [0.         0.         0.         0.2410822  0.         0.
  0.         0.2410822  0.2410822  0.2410822  0.         0.
  0.         0.         0.         0.         0.         0.2410822
  0.55004769 0.         0.         0.         0.         0.2410822
  0.         0.         0.         0.         0.48216441 0.
  0.         0.         0.         0.         0.2410822  0.2410822 ]
 [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891
  0.         0.         0.         0.         0.15698297 0.15698297
  0.         0.15698297 0.         0.15698297 0.15698297 0.
  0.1193896  0.         0.         0.15698297 0.         0.
  0.         0.15698297 0.         0.         0.         0.31396594
  0.15698297 0.         0.         0.15698297 0.         0.        ]]

  # ä¸€ä¸ªæ•°ç»„å¯¹åº”ä¸€ç¯‡æ–‡ç« ï¼Œæ•°å¤§å°å¯¹åº”è¯åœ¨è¯¥ç¯‡æ–‡ç« ä¸­çš„çš„é‡è¦æ€§
"""
```

## 2.3. æ•°æ®é¢„å¤„ç†

### 2.3.1. æ¦‚è¿°

- å®šä¹‰ï¼šé€šè¿‡ç‰¹å®šçš„ç»Ÿè®¡æ–¹æ³•ï¼ˆæ•°å­¦æ–¹æ³•ï¼‰å°†æ•°æ®è½¬æ¢æˆç®—æ³•è¦æ±‚çš„æ•°æ®

- å¤„ç†ï¼š

  - æ•°å€¼å‹æ•°æ®ï¼šæ ‡å‡†ç¼©æ”¾ï¼š
    - 1ã€å½’ä¸€åŒ–
    - 2ã€æ ‡å‡†åŒ–
    - 3ã€ç¼ºå¤±å€¼(pandas å’Œ numpy éƒ½å¯ä»¥è¿›è¡Œå¤„ç†)
  - ç±»åˆ«å‹æ•°æ®ï¼šone-hot ç¼–ç  ï¼ˆå­—å…¸ç±»å‹æ•°æ®æŠ½å–ï¼‰
  - æ—¶é—´ç±»å‹ï¼šæ—¶é—´çš„åˆ‡åˆ†(æ ¹æ®æƒ…å†µå¤„ç†å­—ç¬¦ä¸²)

- apiï¼š`sklearn.preprocessing`

### 2.3.2. å½’ä¸€åŒ– Normalization

> ä¸æ€ä¹ˆä½¿ç”¨

- ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®æ˜ å°„åˆ°ä¸€å®šèŒƒå›´(é»˜è®¤ä¸º[0,1])ä¹‹é—´
- ç›®çš„ï¼šé¿å…å› æ•°å€¼è€Œå½±å“ç‰¹å¾çš„ã€‚
  > æ¯”å¦‚èº«é«˜å’Œä½“é‡ ä¸ å¥åº·ç¨‹åº¦çš„å…³ç³»ï¼Œå› ä¸ºæœ¬èº«å°±ä¸æ˜¯åŒä¸€èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨çš„è¯ï¼Œæ‰€å æƒä¹Ÿä¸åŒ
- ç¼ºç‚¹ï¼šå—å¼‚å¸¸ç‚¹å½±å“æ‰“ï¼ˆç‰¹åˆ«æ˜¯æœ€å¤§æœ€å°å€¼å—å½±å“æ—¶ï¼‰ï¼Œé²æ£’æ€§è¾ƒå·®ã€‚**åªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®**

  - é²æ£’æ€§ Robustnessï¼Œä¹Ÿå°±æ˜¯å¥å£®æ€§

- å…¬å¼ï¼š

  > ![](./img/Normalization.jpg)

- api:`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - MinMaxScalar.fit_transform(X)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array

```py
from sklearn.preprocessing import MinMaxScaler

def main():
    mm = MinMaxScaler()
    # é»˜è®¤å°±æ˜¯ mm = MinMaxScaler(feature_range=(0,1))
    rdata = mm.fit_transform( [[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])
    print(mm.feature_range)
    print(rdata)
    pass

if __name__ == "__main__":
    main()
```

### 2.3.3. æ ‡å‡†åŒ– standardization

- ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º 0,æ–¹å·®ä¸º 1 èŒƒå›´å†…
  > åœ¨å·²æœ‰æ ·æœ¬è¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹æ¯”è¾ƒç¨³å®šï¼Œé€‚åˆç°ä»£å˜ˆæ‚å¤§æ•°æ®åœºæ™¯
- å…¬å¼ï¼š

  > ![](./img/standardization.jpg)

- api:`sklearn.preprocessing.StandardScaler`
  - å¤„ç†ä¹‹åæ¯åˆ—æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨
    - å‡å€¼:0 é™„è¿‘
    - æ–¹å·®ä¸º 1
  - StandardScaler.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array
  - StandardScaler.mean\_
    - åŸå§‹æ•°æ®ä¸­æ¯åˆ—ç‰¹å¾çš„å¹³å‡å€¼
  - StandardScaler.std\_
    - åŸå§‹æ•°æ®æ¯åˆ—ç‰¹å¾çš„æ–¹å·®

```py
from sklearn.preprocessing import StandardScaler

def main():
    stdv = StandardScaler()
    rdata = stdv.fit_transform( [[ 1., -1., 3.], [ 2., 4., 2.], [ 4., 6., -1.]])
    print(rdata)

if __name__ == "__main__":
    main()
```

### 2.3.4. ç¼ºå¤±å€¼å¤„ç†

> å•çº¯ç”¨ numpy å’Œ pandas å°±èƒ½å®Œæˆ

- æ’è¡¥ï¼šå¯ä»¥é€šè¿‡ç¼ºå¤±å€¼æ¯è¡Œæˆ–è€…æ¯åˆ—çš„å¹³å‡å€¼ã€ä¸­ä½æ•°æ¥å¡«å……
- api:`sklearn.preprocessing.Imputer`
  - Imputer(missing_values='NaN',Â strategy='mean',Â axis=0)
    - å®Œæˆç¼ºå¤±å€¼æ’è¡¥
  - Imputer.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„ array

## 2.4. æ•°æ®é™ç»´

- ç»´åº¦ï¼šç‰¹å¾çš„æ•°é‡

### 2.4.1. ç‰¹å¾é€‰æ‹©

#### 2.4.1.1. è¯´æ˜

- åŸå› ï¼š

  - å†—ä½™ï¼šéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆè€—è®¡ç®—æ€§èƒ½
  - å™ªå£°ï¼šéƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰è´Ÿå½±å“

- å®šä¹‰ï¼š
  ```
  ç‰¹å¾é€‰æ‹©å°±æ˜¯å•çº¯åœ°ä»æå–åˆ°çš„æ‰€æœ‰ç‰¹å¾ä¸­é€‰æ‹©éƒ¨åˆ†ç‰¹å¾ä½œä¸ºè®­ç»ƒé›†ç‰¹å¾ï¼Œ
  ç‰¹å¾åœ¨é€‰æ‹©å‰å’Œé€‰æ‹©åå¯ä»¥æ”¹å˜å€¼ã€ä¹Ÿä¸æ”¹å˜å€¼ï¼Œä½†æ˜¯é€‰æ‹©åçš„ç‰¹å¾ç»´æ•°è‚¯
  å®šæ¯”é€‰æ‹©å‰å°ï¼Œæ¯•ç«Ÿæˆ‘ä»¬åªé€‰æ‹©äº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†ç‰¹å¾ã€‚
  ```
- ä¸»è¦æ–¹æ³•:
  - **Filter(è¿‡æ»¤å¼):VarianceThreshold**
    > variance:æ–¹å·®<br>
    > threshold:å…¥å£ï¼›é—¨æ§›ï¼›å¼€å§‹ï¼›æé™ï¼›ä¸´ç•Œå€¼
  - **Embedded(åµŒå…¥å¼)ï¼šæ­£åˆ™åŒ–ã€å†³ç­–æ ‘**:åé¢ç®—æ³•æ—¶å†è®²
  - Wrapper(åŒ…è£¹å¼)ï¼šåŸºæœ¬ä¸ç”¨
- å…¶ä»–æ–¹æ³•ï¼š
  - **ç¥ç»ç½‘ç»œ**ï¼Œä¹‹åå†è¯´
  - çº¿æ€§åˆ¤åˆ«åˆ†æ LDAï¼ˆåŸºæœ¬ä¸ç”¨ï¼‰

#### 2.4.1.2. VarianceThreshold

- api:`sklearn.feature_selection.VarianceThreshold`
  - VarianceThreshold(threshold = 0.0)
    > é»˜è®¤ threshold=0.0ã€‚è¦æ ¹æ®å®é™…æƒ…å†µå–å€¼
    - åˆ é™¤æ‰€æœ‰ä½æ–¹å·®ç‰¹å¾
  - Variance.fit_transform(X,y)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè®­ç»ƒé›†å·®å¼‚ä½äº threshold çš„ç‰¹å¾å°†è¢«åˆ é™¤ã€‚
    - é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾ï¼Œå³åˆ é™¤æ‰€æœ‰æ ·æœ¬
    - ä¸­å…·æœ‰ç›¸åŒå€¼çš„ç‰¹å¾ã€‚

```py
from sklearn.feature_selection import VarianceThreshold

def main():
    vt = VarianceThreshold()
    rdata = vt.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])
    print(rdata)

if __name__ == "__main__":
    main()
    """
    ç»“æœï¼š
    [[2 0]
    [1 4]
    [1 1]]

    å³åˆ é™¤äº†ä¸€æ ·çš„åˆ—
    """
```

### 2.4.2. PCA(ä¸»æˆåˆ†åˆ†æ)

#### 2.4.2.1. è¯´æ˜

> å½“ç‰¹å¾æˆç™¾ä¸Šåƒæ—¶ï¼Œå°±éœ€è¦è€ƒè™‘äº†

> **åŸç†è‡ªå·±æŸ¥**

- æœ¬è´¨ï¼šPCA æ˜¯ä¸€ç§åˆ†æã€ç®€åŒ–æ•°æ®é›†çš„æŠ€æœ¯ã€‚
- ç›®çš„ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯ã€‚
  > æ¯”å¦‚ feature1 å’Œ featrue2 é—´ä¸ºçº¿æ€§å…³ç³»ï¼Œé‚£ä¹ˆå°±å¯ä»¥åˆ é™¤å…¶ä¸­ä¸€ä¸ª
- ä½œç”¨ï¼šå¯ä»¥å‰Šå‡å›å½’åˆ†ææˆ–è€…èšç±»åˆ†æä¸­ç‰¹å¾çš„æ•°é‡
- api:`sklearn.decomposition.PCA`
  - PCA(n_components=None)
    > n_components:<br>
    > å°æ•°ï¼Œ0~1 çš„ç™¾åˆ†æ¯”ï¼Œè¡¨ç¤ºä¿¡æ¯çš„æŸå¤±é‡ï¼Œä¸€èˆ¬ä¸º 0.9~0.95<br>
    > æ•´æ•°ï¼šå‡å°‘åˆ°çš„ç‰¹å¾æ•°é‡ã€‚**ä¸€èˆ¬ä¸ç”¨**
    - å°†æ•°æ®åˆ†è§£ä¸ºè¾ƒä½ç»´æ•°ç©ºé—´
  - PCA.fit_transform(X)
    - X:numpy array æ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åæŒ‡å®šç»´åº¦çš„ array

```py
from sklearn.decomposition import PCA

def main():
    pca = PCA(n_components=0.9)
    rdata = pca.fit_transform([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])
    print(rdata)

if __name__ == "__main__":
    main()
```

#### 2.4.2.2. é™ç»´æ¡ˆä¾‹

[æ•°æ®](https://www.kaggle.com/c/instacart-market-basket-analysis/data)

## 2.5. æ•°æ®é›†

> scikit-learnæœ‰å†…ç½®æ•°æ®é›†

- å¯å°†æ•°æ®é›†åˆ’åˆ†ä¸ºåˆ’åˆ†ä¸ºï¼š
  - è®­ç»ƒæ•°æ®é›†
  - æµ‹è¯•æ•°æ®é›†

> ![](./img/data_set.jpg)

- api:
  - è·å–ï¼š
    - sklearn.datasets
      > åŠ è½½è·å–æµè¡Œæ•°æ®é›†
      - `datasets.load_*()`
        > è·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œ
      - `datasets.fetch_*(data_home=None)`
        > è·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½ï¼Œå‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯data_homeï¼Œè¡¨ç¤ºæ•°æ®é›†ä¸‹è½½çš„ç›®å½•,é»˜è®¤æ˜¯ ~/scikit_learn_data/<br>
        > subset: 'train'æˆ–è€…'test','all'ï¼Œå¯é€‰ï¼Œé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†. è®­ç»ƒé›†çš„â€œè®­ç»ƒâ€ï¼Œæµ‹è¯•é›†çš„â€œæµ‹è¯•â€ï¼Œä¸¤è€…çš„â€œå…¨éƒ¨â€
    - æ•°æ®æ ¼å¼ï¼›
      - load*å’Œfetch*è¿”å›çš„æ•°æ®ç±»å‹datasets.base.Bunch(å­—å…¸æ ¼å¼)
      - dataï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯ [n_samples * n_features] çš„äºŒç»´ numpy.ndarray æ•°ç»„
      - targetï¼šç›®æ ‡å€¼æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
      - DESCRï¼šæ•°æ®æè¿°
      - feature_namesï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
      - target_namesï¼šæ ‡ç­¾å,å›å½’æ•°æ®é›†æ²¡æœ‰
  - åˆ’åˆ†ï¼š`sklearn.model_selection.train_test_split(*arrays,Â **options)`
    - x:  æ•°æ®é›†çš„ç‰¹å¾å€¼
    - y:  æ•°æ®é›†çš„æ ‡ç­¾å€¼
    - test_size      æµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸ºfloat
    - random_state        éšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœº é‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚
    - return  è®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒæ ‡ç­¾ï¼Œæµ‹è¯•æ ‡ç­¾ (é»˜è®¤éšæœºå–)	
  - æ¸…æ¥šï¼š
    - `datasets.clear_data_home(data_home=None)`
      > æ¸…é™¤ç›®å½•ä¸‹çš„æ•°æ®

## 2.6. è½¬æ¢å™¨ä¸ä¼°è®¡å™¨

### 2.6.1. è½¬æ¢å™¨

> ç‰¹å¾å·¥ç¨‹ï¼Œæ•°æ®å¤„ç†æ‰€ç”¨api

- é€šç”¨ï¼š
  - fit():è¾“å…¥æ•°æ®ï¼Œä½†ä¸è¿›è¡Œå¤„ç†ã€‚ä½†ä¼šè®¡ç®—äº›å¹³å‡å€¼å’Œæ–¹å·®ï¼Œæˆ–è€…è¯åˆ—è¡¨ç­‰å‰ææ“ä½œï¼Œè¿™äº›å€¼ä¼šå­˜å‚¨åˆ°å¤„ç†å¯¹è±¡ä¸­ï¼Œæ¯”å¦‚StandScaler
    > **æ‰€ä»¥ä¸èƒ½fitå®Œä¸€ä¸ªæ•°æ®ï¼Œå†transformå¦ä¸€ä¸ªæ•°æ®ï¼Œä¼šä¹±å¥—**
  - transform():æ•°æ®å¤„ç†ã€‚æ ¹æ®fit()å¾—åˆ°çš„ç»“æœï¼Œå†è¿›è¡Œå¤„ç†
  - fit_transform():è¾“å…¥æ•°æ®ï¼Œç›´æ¥è½¬æ¢ã€‚**ä¸€èˆ¬ä½¿ç”¨è¿™ä¸ªå³å¯ï¼Œä¸Šé¢ä¸¤ä¸ªåŸºæœ¬ä¸ç”¨**
    > ç­‰äº fit()+transform()


### 2.6.2. ä¼°è®¡å™¨

- å®šä¹‰ï¼šåœ¨sklearnä¸­ï¼Œä¼°è®¡å™¨(estimator)æ˜¯ä¸€ä¸ªé‡è¦çš„è§’è‰²ï¼Œåˆ†ç±»å™¨å’Œå›å½’å™¨éƒ½å±äºestimatorï¼Œæ˜¯ä¸€ç±»å®ç°äº†ç®—æ³•çš„API
  > å®ç°ç®—æ³•api

- ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨
  - sklearn.neighbors	k-è¿‘é‚»ç®—æ³•
  - sklearn.naive_bayes      è´å¶æ–¯
  - sklearn.linear_model.LogisticRegression     é€»è¾‘å›å½’
- ç”¨äºå›å½’çš„ä¼°è®¡å™¨
  - sklearn.linear_model.LinearRegression     çº¿æ€§å›å½’
  - sklearn.linear_model.Ridge      å²­å›å½’ 

- é€šç”¨ï¼š
  - fit()ï¼šè¿›è¡Œè®­ç»ƒ
  - prediec():é¢„ä¼°ç»“æœ
  - score():å‡†ç¡®ç‡

## 2.7. apiæ€»ç»“

### 2.7.1. apiæ€»ç»“

- æ•°æ®æŠ½å–ï¼š
  - å­—å…¸ç±»å‹æ•°æ®æŠ½å–:`sklearn.feature_extraction.DictVectorizer`
  - æ–‡æœ¬ç‰¹å¾æ•°æ®æŠ½å–:
    - count:`sklearn.feature_extraction.text.CountVectorizer`
    - tf-idf:`sklearn.feature_extraction.text.TfidfVectorizer`
- æ•°æ®é¢„å¤„ç†
  - å½’ä¸€åŒ–ï¼š`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - æ ‡å‡†åŒ–:`sklearn.preprocessing.StandardScaler`
  - ç¼ºå¤±å€¼:`sklearn.preprocessing.Imputer`
- æ•°æ®é™ç»´
  - ç‰¹å¾é€‰æ‹©:`sklearn.feature_selection.VarianceThreshold`
  - PCA:`sklearn.decomposition.PCA`


### 2.7.2. apiå¤§è‡´ä½¿ç”¨æµç¨‹

![](./img/sklearn_api_line.jpg)

# 3. åŸºæœ¬ç®—æ³•

> æ­¤å¤„åªä¼šè®²è§£åŸºæœ¬æ¦‚å¿µå’Œapiä½¿ç”¨ã€‚ç®—æ³•å®ç°ä¸åšæ€»ç»“

## 3.1. ç›‘ç£å­¦ä¹ 

### 3.1.1. åˆ†ç±»

#### 3.1.1.1. k-è¿‘é‚»ç®—æ³•

> åŸºæœ¬ä¸å’‹ç”¨

- çœ‹ã€Šç®—æ³•å›¾è§£ã€‹
- éœ€è¦åšæ ‡å‡†åŒ–å¤„ç†
- api:sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
  - n_neighborsï¼šint,å¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œk_neighborsæŸ¥è¯¢é»˜è®¤ä½¿ç”¨çš„é‚»å±…æ•° 
  - algorithmï¼š{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼šâ€˜ball_treeâ€™å°†ä¼šä½¿ç”¨ BallTreeï¼Œâ€˜kd_treeâ€™å°†ä½¿ç”¨ KDTreeã€‚â€˜autoâ€™å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•ã€‚ (ä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡)

- ä¼˜ç‚¹ï¼š
  - ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€ä¼°è®¡å‚æ•°ï¼Œæ— éœ€è®­ç»ƒ
- ç¼ºç‚¹ï¼š
  - æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
  - å¿…é¡»æŒ‡å®šKå€¼ï¼ŒKå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯

```py
import numpy as np
import pandas as pd
from sklearn import neighbors
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# è·å–æ•°æ®
iris_data = load_iris()

# æ•°æ®æè¿°
print(iris_data.DESCR)

# æŸ¥çœ‹éƒ¨åˆ†æ•°æ®
print(iris_data.data[:5, :])

# å½’ä¸€åŒ–
mm = MinMaxScaler()
x_data = mm.fit_transform(iris_data.data)

# æƒé‡å¤„ç†(å°è¯•)
# x_data[:,[0]] = x_data[:,[0]]/(1-0.7826)
# x_data[:,[1]] = x_data[:,[0]]/(1+0.4194)
# x_data[:,[2]] = x_data[:,[0]]/(1-0.9490)

# æ•°æ®åˆ’åˆ†
x_train, x_test, y_train, y_test = train_test_split(
    x_data, iris_data.target, test_size=0.25)

# å®ä¾‹åŒ–knnå¯¹è±¡
knn = neighbors.KNeighborsClassifier(n_neighbors=5,)

# å°è¯•æ·»åŠ æƒé‡
# knn = neighbors.KNeighborsClassifier(n_neighbors=5, metric="wminkowski", metric_params={
#                                      "w": [0.7826, -0.4194, 0.9490, 0.9565]})

# åŠ è½½æ•°æ®
knn.fit(x_train, y_train)

# é¢„æµ‹å€¼
knn.predict(x_test)

# å‡†ç¡®ç‡
knn.score(x_test, y_test)
```


#### 3.1.1.2. æœ´ç´ è´å¶æ–¯åˆ†ç±»

[è®²è§£](https://zhuanlan.zhihu.com/p/26262151)

```py
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

news = fetch_20newsgroups(subset='all')
x_train,x_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25)

# æ•°æ®å¤„ç†
tf = TfidfVectorizer()
x_train = tf.fit_transform(x_train)
x_test = tf.transform(x_test) # æ³¨æ„ï¼Œè¿™é‡Œç”¨trainsformï¼Œæ²¡æœ‰fitï¼Œè¦ä»¥trainä¸ºæ ‡å‡†

# åˆ›å»ºæœ´ç´ è´å¶æ–¯apiå¯¹è±¡
mlt = MultinomialNB(alpha=1.0)

# è¿›è¡Œè®­ç»ƒ
mlt.fit(x_train,y_train)

# é¢„æµ‹å€¼
mlt.predict(x_test)

# å‡†ç¡®ç‡
mlt.score(x_test,y_test)
```

- ç‰¹ç‚¹
  - æ— æ³•ä¼ å…¥å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œè®­ç»ƒé›†å½±å“å¤§
  - ä¸éœ€è¦è°ƒå‚
  - è®­ç»ƒé›†è¯¯å·®ä¼šå¤§å¤§å½±å“ç»“æœ

- ä¼˜ç‚¹ï¼š
  - æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
  - å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
  - åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«
- ç¼ºç‚¹ï¼š
  - ç”±äºä½¿ç”¨äº†æ ·æœ¬å±æ€§ç‹¬ç«‹æ€§çš„å‡è®¾ï¼Œæ‰€ä»¥å¦‚æœæ ·æœ¬å±æ€§æœ‰å…³è”æ—¶ å…¶æ•ˆæœä¸å¥½

> ç¥ç»ç½‘ç»œçš„æ•ˆæœè¦æ¯”æœ´ç´ è´å¶æ–¯è¦å¥½

#### 3.1.1.3. åˆ†ç±»æ¨¡å‹è¯„ä¼°

> åœ¨åˆ†ç±»ä»»åŠ¡ä¸‹ï¼Œé¢„æµ‹ç»“æœ(Predicted Condition)ä¸æ­£ç¡®æ ‡è®°(True Condition)ä¹‹é—´å­˜åœ¨å››ç§ä¸åŒçš„ç»„åˆï¼Œæ„æˆæ··æ·†çŸ©é˜µ(é€‚ç”¨äºå¤šåˆ†ç±»)

![](./img/confusion_matrix.jpg)


- æ··æ·†çŸ©é˜µ
  - ç²¾ç¡®ç‡:é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹æ ·æœ¬ä¸­çœŸå®ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å¾—å‡†ï¼‰
    > ![](./img/precision.jpg)
  - å¬å›ç‡:çœŸå®ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­é¢„æµ‹ç»“æœä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼ˆæŸ¥çš„å…¨ï¼Œå¯¹æ­£æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ï¼‰
    > ![](./img/recall.jpg)
  - å…¶ä»–åˆ†ç±»æ ‡å‡†ï¼ŒF1-scoreï¼Œåæ˜ äº†æ¨¡å‹çš„ç¨³å¥å‹
    > ![](./img/f1-score.jpg)

- api:`sklearn.metrics.classification_report `
  - y_trueï¼šçœŸå®ç›®æ ‡å€¼ 
  - y_predï¼šä¼°è®¡å™¨é¢„æµ‹ç›®æ ‡å€¼ 
  - target_namesï¼šç›®æ ‡ç±»åˆ«åç§° 
  - returnï¼šæ¯ä¸ªç±»åˆ«ç²¾ç¡®ç‡ä¸å¬å›ç‡

#### 3.1.1.4. æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

##### 3.1.1.4.1. äº¤å‰éªŒè¯

> ä¸€èˆ¬å’Œç½‘æ ¼æœç´¢æ­é…

- ç›®çš„ï¼šäº¤å‰éªŒè¯ï¼šä¸ºäº†è®©è¢«è¯„ä¼°çš„æ¨¡å‹æ›´åŠ å‡†ç¡®å¯ä¿¡
- è¿‡ç¨‹ï¼š
  ```
  äº¤å‰éªŒè¯ï¼šå°†æ‹¿åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼š
  å°†æ•°æ®åˆ†æˆ5ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä½œä¸ºéªŒè¯é›†ã€‚
  ç„¶åç»è¿‡5æ¬¡(ç»„)çš„æµ‹è¯•ï¼Œæ¯æ¬¡éƒ½æ›´æ¢ä¸åŒçš„éªŒè¯é›†ã€‚
  å³å¾—åˆ°5ç»„æ¨¡å‹çš„ç»“æœï¼Œå–å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆç»“æœã€‚åˆç§°5æŠ˜äº¤å‰éªŒè¯ã€‚
  ```
  > ![](./img/cross_validation.jpg)


##### 3.1.1.4.2. ç½‘æ ¼æœç´¢

> ä¸€èˆ¬ä¸äº¤å‰éªŒè¯æ­é…

- è¯´æ˜ï¼š
  ```
  é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œ
  è¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„
  åˆã€‚æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»º
  ç«‹æ¨¡å‹ã€‚
  ```
- è¿‡ç¨‹ï¼š
  - å°±æ˜¯ä¸€ä¸ªç©·ä¸¾ã€‚æ¯ä¸ªå‚æ•°å€¼éƒ½ç”¨äº¤å‰éªŒè¯å¾—åˆ°æ¨¡å‹
  - å‚æ•°é—´ç›¸äº’ç»„åˆ
  - é€‰å–æœ€å¥½çš„æ¨¡å‹

- api:`sklearn.model_selection.GridSearchCV`
  > å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢
  - å‚æ•°
    - estimatorï¼šä¼°è®¡å™¨å¯¹è±¡
    - param_gridï¼šä¼°è®¡å™¨å‚æ•°(dict){â€œn_neighborsâ€:[1,3,5]}
    - cvï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
    - fitï¼šè¾“å…¥è®­ç»ƒæ•°æ®
    - scoreï¼šå‡†ç¡®ç‡
  - ç»“æœåˆ†æï¼š
    - best_score_:åœ¨äº¤å‰éªŒè¯ä¸­æµ‹è¯•çš„æœ€å¥½ç»“æœ
    - best_estimator_ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
    - cv_results_:æ¯æ¬¡äº¤å‰éªŒè¯åçš„æµ‹è¯•é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ


#### 3.1.1.5. å†³ç­–æ ‘ä¸éšæœºæ£®æ—

##### 3.1.1.5.1. å†³ç­–æ ‘

> ä¼ä¸šè¿‡ç¨‹ä¸­ä½¿ç”¨è¾ƒå¤š

- åŸç†ï¼šä¿¡æ¯è®ºåŸºç¡€
  - ä¿¡æ¯ç†µçš„è®¡ç®—
  - æ¡ä»¶ç†µçš„è®¡ç®—
    > ![](./img/Information_entropy.jpg)
  - ä¿¡æ¯å¢ç›Šçš„è®¡ç®—
    > ä¿¡æ¯å¢ç›Šï¼šå½“å¾—çŸ¥ä¸€ä¸ªç‰¹å¾æ¡ä»¶ä¹‹åï¼Œå‡å°‘çš„ä¿¡æ¯ç†µçš„å¤§å°
    > ![](./img/Information_gain.jpg)

- ä¿¡æ¯å¢ç›Šå¤§çš„ä½œä¸ºæœ€å¼€å§‹çš„åˆ†ç±»ã€‚
  > ä¿¡æ¯å¢ç›Šè¶Šå¤§ï¼Œè¶Šæœ‰å¯èƒ½å¾—å‡ºç»“æœã€‚å¤§çš„æ”¾å‰é¢æœ‰åˆ©äºä¸è¿›è¡Œå¤šä½™çš„åˆ¤æ–­
  > ![](./img/Decision_tree.jpg)

- ç®—æ³•(äº†è§£)ï¼š
  > åœ¨sklearnä¸­å¯ä»¥é€‰æ‹©åˆ’åˆ†çš„åŸåˆ™
  - ID3
    - ä¿¡æ¯å¢ç›Š æœ€å¤§çš„å‡†åˆ™
  - C4.5
    - ä¿¡æ¯å¢ç›Šæ¯” æœ€å¤§çš„å‡†åˆ™
  - CART 
    - å›å½’æ ‘: å¹³æ–¹è¯¯å·® æœ€å° 
    - åˆ†ç±»æ ‘: åŸºå°¼ç³»æ•°   æœ€å°çš„å‡†åˆ™ 
- api:`sklearn.tree.DecisionTreeClassifier(criterion=â€™giniâ€™,Â max_depth=None,random_state=None)`
  - ä½¿ç”¨
    > å†³ç­–æ ‘åˆ†ç±»å™¨<br>
    > è¶…å‚æ•°æ”¾åœ¨éšæœºæ£®æ—
    - decision_path:è¿”å›å†³ç­–æ ‘çš„è·¯å¾„
    ```python
    DecisionTreeClassifier(
    *,
    criterion='gini', # é»˜è®¤æ˜¯â€™giniâ€™ç³»æ•°ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µâ€™entropyâ€™
    splitter='best',
    max_depth=None, # æ ‘çš„æ·±åº¦æœ€å¤§å¤§å°
    min_samples_split=2, # æ ·æœ¬æ•°å¤§äºä¸¤ä¸ªæ—¶æ‰ä¼šåˆ†å‰
    min_samples_leaf=1, # æ ·æœ¬æ•°å¤§äº1ä¸ªæ—¶ï¼ŒèŠ‚ç‚¹æ‰ä¼šè¢«ç•™ä¸‹
    min_weight_fraction_leaf=0.0,
    max_features=None,
    random_state=None, # éšæœºæ•°ç§å­
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_impurity_split=None,
    class_weight=None,
    presort='deprecated',
    ccp_alpha=0.0,
    )
    ```
  - æ¨¡å‹ä¿å­˜
    - 1ã€sklearn.tree.export_graphviz()Â è¯¥å‡½æ•°èƒ½å¤Ÿå¯¼å‡ºDOTæ ¼å¼
      ```
      tree.export_graphviz(estimator,out_file='tree.dotâ€™,feature_names=[â€˜â€™,â€™â€™]) 
      ```
    - 2ã€å·¥å…·:(èƒ½å¤Ÿå°†dotæ–‡ä»¶è½¬æ¢ä¸ºpdfã€png)
      ```
      å®‰è£…graphviz
      ubuntu:sudo apt-get install graphviz                    Mac:brew install graphviz 

      è¿è¡Œå‘½ä»¤
      ç„¶åæˆ‘ä»¬è¿è¡Œè¿™ä¸ªå‘½ä»¤
      $ dot -Tpng tree.dot -o tree.png
      ```

- ç¤ºä¾‹ï¼š
  - æ³°å¦å°¼å…‹å·å­˜æ´»ç‡

- ä¼˜ç¼ºç‚¹ï¼š
  - ä¼˜ç‚¹ï¼š
    - ç®€å•çš„ç†è§£å’Œè§£é‡Šï¼Œæ ‘æœ¨å¯è§†åŒ–ã€‚
    - éœ€è¦å¾ˆå°‘çš„æ•°æ®å‡†å¤‡ï¼Œå…¶ä»–æŠ€æœ¯é€šå¸¸éœ€è¦æ•°æ®å½’ä¸€åŒ–ï¼Œ
  - ç¼ºç‚¹ï¼š
    - å†³ç­–æ ‘å­¦ä¹ è€…å¯ä»¥åˆ›å»ºä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿æ•°æ®çš„è¿‡äºå¤æ‚çš„æ ‘ï¼Œ è¿™è¢«ç§°ä¸ºè¿‡æ‹Ÿåˆã€‚ 
    - å†³ç­–æ ‘å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºæ•°æ®çš„å°å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘è¢«ç”Ÿæˆ 
  - æ”¹è¿›ï¼š
    - å‡æcartç®—æ³•
      - åˆ›å»ºapiå¯¹è±¡æ—¶è®¾ç½®å‚æ•°ï¼š
        ```python
        min_samples_split=2, # æ ·æœ¬æ•°å¤§äºä¸¤ä¸ªæ—¶æ‰ä¼šåˆ†å‰
        min_samples_leaf=1, # æ ·æœ¬æ•°å¤§äº1ä¸ªæ—¶ï¼ŒèŠ‚ç‚¹æ‰ä¼šè¢«ç•™ä¸‹
        ```
    - éšæœºæ£®æ—

##### 3.1.1.5.2. éšæœºæ£®æ—

- é›†æˆå­¦ä¹ æ–¹æ³•ï¼š
  ```
  é›†æˆå­¦ä¹ é€šè¿‡å»ºç«‹å‡ ä¸ªæ¨¡å‹ç»„åˆçš„æ¥è§£å†³å•ä¸€é¢„æµ‹é—®é¢˜ã€‚
  å®ƒçš„å·¥ä½œåŸç†æ˜¯ç”Ÿæˆå¤šä¸ªåˆ†ç±»å™¨/æ¨¡å‹ï¼Œå„è‡ªç‹¬ç«‹åœ°å­¦ä¹ å’Œä½œå‡ºé¢„æµ‹ã€‚
  è¿™äº›é¢„æµ‹æœ€åç»“åˆæˆå•é¢„æµ‹ï¼Œå› æ­¤ä¼˜äºä»»ä½•ä¸€ä¸ªå•åˆ†ç±»çš„åšå‡ºé¢„æµ‹ã€‚
  ```
- éšæœºæ£®æ—ï¼š
  ```
  åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œéšæœºæ£®æ—æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå†³ç­–æ ‘çš„åˆ†ç±»å™¨ï¼Œå¹¶ä¸”å…¶è¾“å‡ºçš„ç±»åˆ«æ˜¯ç”±ä¸ªåˆ«æ ‘è¾“å‡ºçš„ç±»åˆ«çš„ä¼—æ•°è€Œå®šã€‚
  ```

- éšæœºæ£®æ—åˆ›å»ºè¿‡ç¨‹ï¼š
  1. å•æ£µæ ‘åˆ›å»ºè¿‡ç¨‹
    1. éšæœºåœ¨Mä¸ªç‰¹å¾ä¸­é€‰å‡ºmä¸ªç‰¹å¾,måº”è¿œå°äºM
    2. éšæœºä»Nä¸ªæ ·æœ¬ä¸­é€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼Œé‡å¤Næ¬¡ã€‚éšæœºæ”¾å›æŠ½æ ·(bootstrapæŠ½æ ·),æ ·æœ¬æœ‰å¯èƒ½é‡å¤
    3. ä½¿ç”¨æŠ½å–çš„æ ·æœ¬ï¼Œè®­ç»ƒæ¨¡å‹,å¹¶ç”¨æœªæŠ½åˆ°çš„æ ·æœ¬ä½œé¢„æµ‹ï¼Œè¯„ä¼°å…¶è¯¯å·®ã€‚
  2. é‡å¤æ“ä½œï¼Œå»ºç«‹æŒ‡å®šæ•°é‡çš„å†³ç­–æ ‘

- é—®é¢˜ï¼š
  - ä¸ºä»€ä¹ˆè¦éšæœºæŠ½æ ·è®­ç»ƒé›†ï¼Ÿã€€ã€€
    ```
    å¦‚æœä¸è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œæ¯æ£µæ ‘çš„è®­ç»ƒé›†éƒ½ä¸€æ ·ï¼Œé‚£ä¹ˆæœ€ç»ˆè®­ç»ƒå‡ºçš„æ ‘åˆ†ç±»ç»“æœä¹Ÿæ˜¯å®Œå…¨ä¸€æ ·çš„ 
    ```
  - ä¸ºä»€ä¹ˆè¦æœ‰æ”¾å›åœ°æŠ½æ ·ï¼Ÿ
    ```
    å¦‚æœä¸æ˜¯æœ‰æ”¾å›çš„æŠ½æ ·ï¼Œé‚£ä¹ˆæ¯æ£µæ ‘çš„è®­ç»ƒæ ·æœ¬éƒ½æ˜¯ä¸åŒçš„ï¼Œéƒ½æ˜¯æ²¡æœ‰äº¤é›†çš„ï¼Œè¿™æ ·æ¯æ£µæ ‘éƒ½æ˜¯â€œæœ‰åçš„â€ï¼Œéƒ½æ˜¯ç»å¯¹â€œç‰‡é¢çš„â€ï¼ˆå½“ç„¶è¿™æ ·è¯´å¯èƒ½ä¸å¯¹ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯æ£µæ ‘è®­ç»ƒå‡ºæ¥éƒ½æ˜¯æœ‰å¾ˆå¤§çš„å·®å¼‚çš„ï¼›è€Œéšæœºæ£®æ—æœ€ååˆ†ç±»å–å†³äºå¤šæ£µæ ‘ï¼ˆå¼±åˆ†ç±»å™¨ï¼‰çš„æŠ•ç¥¨è¡¨å†³ã€‚
    ```
- api:
  ```py
  classÂ sklearn.ensemble.RandomForestClassifier(
    n_estimators=10, #  æ£®æ—é‡Œçš„æ ‘æœ¨æ•°é‡ã€‚æ¨èï¼š120,200,300,500,800,1200
    *,
    criterion=â€™giniâ€™, # åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•
    max_depth=None, #å¯é€‰ï¼ˆé»˜è®¤=æ— ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦  
    min_samples_split=2,
    min_samples_leaf=1,
    min_weight_fraction_leaf=0.0,
    max_features='auto', # æ¯æ£µå†³ç­–æ ‘é€‰å–çš„æœ€å¤§çš„ç‰¹å¾æ•°é‡ï¼ˆmçš„æœ€å¤§å€¼ï¼‰
      # æŸ¥æ–‡æ¡£å³å¯
      # - If int, then consider `max_features` features at each split.
      # - If float, then `max_features` is a fraction and
      #   `int(max_features * n_features)` features are considered at each
      #   split.
      # - If "auto", then `max_features=sqrt(n_features)`.
      # - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
      # - If "log2", then `max_features=log2(n_features)`.
      # - If None, then `max_features=n_features`.
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_impurity_split=None,
    bootstrap=True, # æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶ä½¿ç”¨æ”¾å›æŠ½æ · 
    oob_score=False,
    n_jobs=None,
    random_state=None,
    verbose=0,
    warm_start=False,
    class_weight=None,
    ccp_alpha=0.0,
    max_samples=None,
  )
  ```

- ä¼˜ç‚¹ï¼š
  - åœ¨å½“å‰æ‰€æœ‰ç®—æ³•ä¸­ï¼Œå…·æœ‰æå¥½çš„å‡†ç¡®ç‡
  - èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿è¡Œåœ¨å¤§æ•°æ®é›†ä¸Š
  - èƒ½å¤Ÿå¤„ç†å…·æœ‰é«˜ç»´ç‰¹å¾çš„è¾“å…¥æ ·æœ¬ï¼Œè€Œä¸”ä¸éœ€è¦é™ç»´
  - èƒ½å¤Ÿè¯„ä¼°å„ä¸ªç‰¹å¾åœ¨åˆ†ç±»é—®é¢˜ä¸Šçš„é‡è¦æ€§
  - å¯¹äºç¼ºçœå€¼é—®é¢˜ä¹Ÿèƒ½å¤Ÿè·å¾—å¾ˆå¥½å¾—ç»“æœ


### 3.1.2. å›å½’

#### 3.1.2.1. çº¿æ€§å›å½’

> çœ‹æœºå™¨å­¦ä¹ æ–‡æ¡£

- api:
  - æ­£è§„æ–¹ç¨‹ï¼šsklearn.linear_model.LinearRegression()
    - æ™®é€šæœ€å°äºŒä¹˜æ³•çº¿æ€§å›å½’
    - coef_ï¼šå›å½’ç³»æ•°
  - æ¢¯åº¦ä¸‹é™ï¼šsklearn.linear_model.SGDRegressor( )
    - é€šè¿‡ä½¿ç”¨SGDæœ€å°åŒ–çº¿æ€§æ¨¡å‹
    - coef_ï¼šå›å½’ç³»æ•°
- å›å½’è¯„ä¼°API:sklearn.metrics.mean_squared_error
  > å‡æ–¹è¯¯å·®å›å½’æŸå¤±
  - y_true:çœŸå®å€¼
  - y_pred:é¢„æµ‹å€¼
    > æ³¨ï¼šçœŸå®å€¼ï¼Œé¢„æµ‹å€¼ä¸ºæ ‡å‡†åŒ–ä¹‹å‰çš„å€¼
  - return:æµ®ç‚¹æ•°ç»“æœ

- ä¸¤ç§å›å½’æ¯”è¾ƒï¼š
  > ![](./img/regression.jpg)
- LinearRegressionä¸SGDRegressorè¯„ä¼°
  - ç‰¹ç‚¹ï¼šçº¿æ€§å›å½’å™¨æ˜¯æœ€ä¸ºç®€å•ã€æ˜“ç”¨çš„å›å½’æ¨¡å‹ã€‚
  - ä»æŸç§ç¨‹åº¦ä¸Šé™åˆ¶äº†ä½¿ç”¨ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œåœ¨ä¸çŸ¥é“ç‰¹å¾ä¹‹ é—´å…³ç³»çš„å‰æä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨çº¿æ€§å›å½’å™¨ä½œä¸ºå¤§å¤šæ•° ç³»ç»Ÿçš„é¦–è¦é€‰æ‹©ã€‚
    - å°è§„æ¨¡æ•°æ®ï¼šLinearRegression(ä¸èƒ½è§£å†³æ‹Ÿåˆé—®é¢˜)ä»¥åŠå…¶å®ƒ
    - å¤§è§„æ¨¡æ•°æ®ï¼šSGDRegressor
  
- çº¿æ€§å›å½’é—®é¢˜ï¼š
  - æ¬ æ‹Ÿåˆ
  - è¿‡æ‹Ÿåˆ
  - è§£å†³ï¼š
    - è¿‡æ»¤å¼ï¼šé€‰æ‹©åœ°æ–¹å·®ç‰¹å¾
    - åµŒå…¥å¼ï¼šå†³ç­–æ ‘ï¼Œç¥ç»ç½‘ç»œï¼Œæ­£åˆ™åŒ–ï¼ˆå²­å›å½’ï¼‰

#### 3.1.2.2. å²­å›å½’

- L2æ­£åˆ™åŒ–ï¼šå²­å›å½’ï¼Œå¸¦æœ‰æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’
- api:sklearn.linear_model.Ridge
  > å…·æœ‰l2æ­£åˆ™åŒ–çš„çº¿æ€§æœ€å°äºŒä¹˜æ³•
  - alpha(0~1):æ­£åˆ™åŒ–åŠ›åº¦
    > **é¢è¯•é—®é¢˜ï¼š**å›å½’è¿‡æ‹Ÿåˆè°ƒä¼˜æ–¹å¼ï¼Ÿ<br>
    > æ­£åˆ™åŒ–ï¼Œå²­å›å½’ï¼Œè°ƒæ•´alphaå‚æ•°å¤§å°
  - coef_:å›å½’ç³»æ•°

> å²­å›å½’ï¼šå›å½’å¾—åˆ°çš„å›å½’ç³»æ•°æ›´ç¬¦åˆå®é™…ï¼Œæ›´å¯é ã€‚å¦å¤–ï¼Œèƒ½è®© ä¼°è®¡å‚æ•°çš„æ³¢åŠ¨èŒƒå›´å˜å°ï¼Œå˜çš„æ›´ç¨³å®šã€‚åœ¨å­˜åœ¨ç—…æ€æ•°æ®åå¤šçš„ç ” ç©¶ä¸­æœ‰è¾ƒå¤§çš„å®ç”¨ä»·å€¼ã€‚

#### sklearnæ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

> æ–‡ä»¶æ ¼å¼ï¼špkl
- ä¿å­˜ï¼šjoblib.dump(rf,"test.pkl")
- åŠ è½½ï¼šestimator = joblib.load("test.pkl")

#### 3.1.2.3. é€»è¾‘å›å½’

> ç”¨äºäºŒåˆ†ç±»

- åº”ç”¨ï¼š
  - å¹¿å‘Šç‚¹å‡»ç‡ 
  - åˆ¤æ–­ç”¨æˆ·çš„æ€§åˆ«
  - é¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šè´­ä¹°ç»™å®šçš„å•†å“ç±» 
  - åˆ¤æ–­ä¸€æ¡è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„
- ä¼˜ç‚¹ï¼šé€‚åˆéœ€è¦å¾—åˆ°ä¸€ä¸ªåˆ†ç±»æ¦‚ç‡çš„åœºæ™¯
- ç¼ºç‚¹ï¼šå½“ç‰¹å¾ç©ºé—´å¾ˆå¤§æ—¶ï¼Œé€»è¾‘å›å½’çš„æ€§èƒ½ä¸æ˜¯å¾ˆå¥½ ï¼ˆçœ‹ç¡¬ä»¶èƒ½åŠ›ï¼‰
- åŸç†ï¼šsigmoidå‡½æ•°ï¼Œå°†å˜é‡æ˜ å°„åˆ°0-1ä¹‹é—´
  > ![](./img/sigmoid.jpg)

- api: sklearn.linear_model.LogisticRegression
  ```py
  LogisticRegression(
    penalty='l2',
    *,
    dual=False,
    tol=0.0001,
    C=1.0, # å­¦ä¹ ç‡
    fit_intercept=True,
    intercept_scaling=1,
    class_weight=None,
    random_state=None,
    solver='lbfgs',
    max_iter=100,
    multi_class='auto',
    verbose=0,
    warm_start=False,
    n_jobs=None,
    l1_ratio=None,
  )
  ```
- å¤šåˆ†ç±»é—®é¢˜ï¼šsoftmax-æ–¹æ³•ï¼Œå°†åœ¨åé¢ç¥ç»ç½‘ç»œç®—æ³•ä¸­ä»‹ç»

- ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ï¼š
  - åˆ¤åˆ«æ¨¡å‹:å¦‚æœ´ç´ è´å¶æ–¯ç®—æ³•ï¼Œéšé©¬å¯å¤«æ¨¡å‹ï¼Œä¸€å¼€å§‹éœ€è¦ä»æ•°æ®ä¸­æ±‚å¾—ä¸€äº›æ¦‚ç‡
  - ç”Ÿæˆæ¨¡å‹:å¦‚é€»è¾‘å›å½’ï¼Œå†³ç­–æ ‘ç­‰ç­‰ï¼Œä¸éœ€è¦äº‹å…ˆå¤„ç†æ•°æ®

#### 3.1.2.4. ç¥ç»ç½‘ç»œ

### 3.1.3. æ ‡æ³¨

#### 3.1.3.1. éšé©¬å°”å¯å¤«æ¨¡å‹

## 3.2. æ— ç›‘ç£å­¦ä¹ 

### 3.2.1. èšç±»

#### 3.2.1.1. k-means

- æ— ç›‘ç£å­¦ä¹ ï¼šåªæœ‰ç‰¹å¾å€¼ï¼Œæ²¡æœ‰ç›®æ ‡å€¼
- k-means:èšç±»
- è¿‡ç¨‹ï¼š
  ![](./img/k-means-1.jpg)
- api:`sklearn.cluster.KMeans`
  - n_clusters:å¼€å§‹çš„èšç±»ä¸­å¿ƒæ•°é‡
  - init:åˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸º'k-means ++'
  - labels_:é»˜è®¤æ ‡è®°çš„ç±»å‹ï¼Œå¯ä»¥å’ŒçœŸå®å€¼æ¯”è¾ƒï¼ˆä¸æ˜¯å€¼æ¯”è¾ƒï¼‰
- è¯„ä¼°æ ‡å‡†ï¼š
  > ![](./img/k-means-2.jpg)
  - å¦‚æœã€–ğ‘ ğ‘ã€—_ğ‘– å°äº0ï¼Œè¯´æ˜ğ‘_ğ‘– çš„å¹³å‡è·ç¦»å¤§äºæœ€è¿‘çš„å…¶ä»–ç°‡ã€‚ èšç±»æ•ˆæœä¸å¥½
  - å¦‚æœã€–ğ‘ ğ‘ã€—_ğ‘– è¶Šå¤§ï¼Œè¯´æ˜ğ‘_ğ‘– çš„å¹³å‡è·ç¦»å°äºæœ€è¿‘çš„å…¶ä»–ç°‡ã€‚ èšç±»æ•ˆæœå¥½
  - è½®å»“ç³»æ•°çš„å€¼æ˜¯ä»‹äº [-1,1] ï¼Œè¶Šè¶‹è¿‘äº1ä»£è¡¨å†…èšåº¦å’Œåˆ†ç¦»åº¦éƒ½ç›¸å¯¹è¾ƒä¼˜ 
- è¯„ä¼°apiï¼š `sklearn.metrics.silhouette_score`
  > è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°
  - Xï¼šç‰¹å¾å€¼
  - labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼

- ç¼ºç‚¹ï¼š
  - å®¹æ˜“æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£(å¤šæ¬¡èšç±»)
  - éœ€è¦é¢„å…ˆè®¾å®šç°‡çš„æ•°é‡(k-means++è§£å†³)


