# 1. 开始

## 1.1. 相关

- 发展：
  - 人工智能：最早出现的概念，指一些自动化的程序
  - 机器学习：对数据进行自动分析获得规律
  - 深度学习：机器学习中较为复杂的算法，比如图像识别等
    > ![](./image/history.jpg)
- 影响因素；
  - 硬件计算水平
  - 数据量多少
  - 算法发展
- 应用：

  - 自然语言处理
  - 图像识别
  - 传统预测

- 机器学习库和框架

  - 前面部分使用：scikit learn
  - 后面部分使用：tensorflow(用的最火)
  - 框架列表：
    > ![](./image/ML_frame.jpg)

- 书籍推荐

  > ![](./image/books.jpg)

- 数据集：

  > ![](./image/data-set.jpg)

- 数据集结构：

  - 特征值，即 x
  - 目标值，即 y

- 基本流程：
  > ![](./image/Basic_process.jpg)


## 1.2. 误差来源

看那个台湾的那个

一个集成模型(f)在未知数据集(D)上的泛化误差E(f;D)，由方差(var)，偏差(bais)和噪声(ε)共同决定。

![](./image/wucha.png)

![](./image/model-1.1.png)


# 2. scikit-learn

## 2.1. 优缺点

- sklearn
  - 有点：封装好，建立模型简单，预测简单
  - 缺点：算法的过程，部分参数都在内部自行优化
  - 注意：sklearn当中特征矩阵必须是二维
- tensorflow:封装高低的api都有，可以自己实现算法

## 2.2. 数据抽取

### 2.2.1. 字典数据抽取

#### 2.2.1.1. DictVectorizer

将字典转换为矩阵

```py
# 导包
from sklearn.feature_extraction import DictVectorizer

def main():
    # 实例化数据抽取对象
    # 默认sparse为true，会将矩阵转换为sparse
    dictVector = DictVectorizer(sparse=False)

    # 调用fit_transform
    rdata=dictVector.fit_transform(
        [
          {
              "name": 1,
              "value": "value1"
          },
          {
              "name": 2,
              "value": "value2"
          },
          {
              "name": 3,
              "value": "value3"
          }
        ]
    )
    print(dictVector.get_feature_names())
    print((rdata))
    print(type(rdata))
    """
    输出：
    ['name', 'value=value1', 'value=value2', 'value=value3']
    [[1. 1. 0. 0.]
    [2. 0. 1. 0.]
    [3. 0. 0. 1.]]
    <class 'numpy.ndarray'>

    数字为值的第一列为name;字符串为值的value会转换为第二，三，四列，true为1，false为0(one-hot编码)
    默认会通过sparse压缩，所以不必担心内存

    sparse矩阵：
      (0, 0)        1.0
      (0, 1)        1.0
      (1, 0)        2.0
      (1, 2)        1.0
      (2, 0)        3.0
      (2, 3)        1.0
    """

if __name__ == "__main__":
    main()

```

### 2.2.2. 文本特征抽取

#### 2.2.2.1. Count

> 统计单词出现频率

```py
# 导入特征抽取类
from sklearn.feature_extraction.text import CountVectorizer

def main():
    cv = CountVectorizer()
    # 实例化
    rdata = cv.fit_transform(
        ["life is short,I use python", "life is too long,i don't use python"])
        # 传入两篇文章
    print(rdata)
    print(cv.get_feature_names())
    print((rdata.toarray()))
    pass

if __name__ == "__main__":
    main()
"""
结果：
  (0, 2)        1
  (0, 1)        1
  (0, 5)        1
  (0, 7)        1
  (0, 4)        1
  (1, 2)        1
  (1, 1)        1
  (1, 7)        1
  (1, 4)        1
  (1, 6)        1
  (1, 3)        1
  (1, 0)        1
['don', 'is', 'life', 'long', 'python', 'short', 'too', 'use']
[[0 1 1 0 1 1 0 1]
 [1 1 1 1 1 0 1 1]]

1. 统计文章中所有出现的词，重复词之看作一次，作为feature_names
2. 一行对应一篇文章，对每篇文章，统计出词的出现次数
3. 单个字母不统计
4. 中文默认不支持特征抽取，因为不能自动分词，除非用空格划开。可以使用 jieba 进行分词
"""
```

#### 2.2.2.2. tf-idf

> 已过时

- tf(term frequency):单词频率
- idf(inverse document frequency):逆文档频率

  > `log(总文档数量/改词出现的文档数+1)`<br>
  > 比如`我们`，`今天`这些常用词不应该计入考虑，可以通过逆文档频率判断重要性

- 计算公式
  > ![](./image/tf-idf.jpg)

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def main():
    str1 = "1、今天很残酷，明天更残酷，后天很美好， 但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    str2 = "2、我们看到的从很远星系来的光是在几百万年之前发出的， 这样当我们看到宇宙时，我们是在看它的过去。"
    str3 = "3、如果只用一种方式了解某样事物，你就不会真正了解它。 了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"
    str1_cut = jieba.cut(str1)
    str2_cut = jieba.cut(str2)
    str3_cut = jieba.cut(str3)
    str1_s = " ".join(str1_cut)
    str2_s = " ".join(str2_cut)
    str3_s = " ".join(str3_cut)
    tifid = TfidfVectorizer()
    rdata = tifid.fit_transform([str1_s, str2_s, str3_s])
    print(tifid.get_feature_names())
    print(rdata.toarray())
    print(rdata)

if __name__ == "__main__":
    main()

"""
['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '这样']
[[0.         0.         0.21821789 0.         0.         0.
  0.43643578 0.         0.         0.         0.         0.
  0.21821789 0.         0.21821789 0.         0.         0.
  0.         0.21821789 0.21821789 0.         0.43643578 0.
  0.21821789 0.         0.43643578 0.21821789 0.         0.
  0.         0.21821789 0.21821789 0.         0.         0.        ]
 [0.         0.         0.         0.2410822  0.         0.
  0.         0.2410822  0.2410822  0.2410822  0.         0.
  0.         0.         0.         0.         0.         0.2410822
  0.55004769 0.         0.         0.         0.         0.2410822
  0.         0.         0.         0.         0.48216441 0.
  0.         0.         0.         0.         0.2410822  0.2410822 ]
 [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891
  0.         0.         0.         0.         0.15698297 0.15698297
  0.         0.15698297 0.         0.15698297 0.15698297 0.
  0.1193896  0.         0.         0.15698297 0.         0.
  0.         0.15698297 0.         0.         0.         0.31396594
  0.15698297 0.         0.         0.15698297 0.         0.        ]]

  # 一个数组对应一篇文章，数大小对应词在该篇文章中的的重要性
"""
```




## 2.3. 数据预处理

### 2.3.1. 概述

- 定义：通过特定的统计方法（数学方法）将数据转换成算法要求的数据

- 处理：

  - 数值型数据：标准缩放：
    - 1、归一化
    - 2、标准化
    - 3、缺失值(pandas 和 numpy 都可以进行处理)
  - 类别型数据：one-hot 编码 （字典类型数据抽取）
  - 时间类型：时间的切分(根据情况处理字符串)

- api：`sklearn.preprocessing`

### 2.3.2. 归一化 Normalization

> 不怎么使用

- 特点：通过对原始数据进行变换把数据映射到一定范围(默认为[0,1])之间
- 目的：避免因数值而影响特征的。
  > 比如身高和体重 与 健康程度的关系，因为本身就不是同一范围，直接使用的话，所占权也不同
- 缺点：受异常点影响打（特别是最大最小值受影响时），鲁棒性较差。**只适合传统精确小数据**

  - 鲁棒性 Robustness，也就是健壮性

- 公式：

  > ![](./image/Normalization.jpg)

- api:`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - MinMaxScalar.fit_transform(X)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array

```py
from sklearn.preprocessing import MinMaxScaler

def main():
    mm = MinMaxScaler()
    # 默认就是 mm = MinMaxScaler(feature_range=(0,1))
    rdata = mm.fit_transform( [[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])
    print(mm.feature_range)
    print(rdata)
    pass

if __name__ == "__main__":
    main()
```

### 2.3.3. 标准化 standardization

- 特点：通过对原始数据进行变换把数据变换到均值为 0,方差为 1 范围内
  > 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景
- 公式：

  > ![](./image/standardization.jpg)

- api:`sklearn.preprocessing.StandardScaler`
  - 处理之后每列来说所有数据都聚集在
    - 均值:0 附近
    - 方差为 1
  - StandardScaler.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array
  - StandardScaler.mean\_
    - 原始数据中每列特征的平均值
  - StandardScaler.std\_
    - 原始数据每列特征的方差

```py
from sklearn.preprocessing import StandardScaler

def main():
    stdv = StandardScaler()
    rdata = stdv.fit_transform( [[ 1., -1., 3.], [ 2., 4., 2.], [ 4., 6., -1.]])
    print(rdata)

if __name__ == "__main__":
    main()
```

```
大多数机器学习算法中，会选择StandardScaler来进行特征缩放，
因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，
神经网络这些算法中，StandardScaler往往是最好的选择。
MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，
比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。
建议先试试看StandardScaler，效果不好换MinMaxScaler
```



### 2.3.4. 缺失值处理

> 单纯用 numpy 和 pandas 就能完成

- 插补：可以通过缺失值每行或者每列的平均值、中位数来填充
- api:`sklearn.preprocessing.Imputer`
  - Imputer(missing_values='NaN', strategy='mean', axis=0)
    - 完成缺失值插补
    - strategy:
      - mean:默认均值
      - median:中位数
      - constant：0
      - most_frequent:众数
    - fill_value：当参数startegy为”constant"的时候可用，可输入字符串或数字表示要填充的值，常用0
    - copy=true: 默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中

  - Imputer.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的 array

> **Imputer类已经过时，0.22版本会被移除，下面使用了sklearn.impute.SimpleImputer替换了原代码中的Imputer**


### 2.3.5. 数值型-->数字型

- sklearn.preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值
  - inverse_transform():复原
- sklearn.preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值
  > OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息
- sklearn.preprocessing.OneHotEncoder：独热编码，创建哑变量**重要※**
  - categories='auto'：自动分组
- sklearn.preprocessing.Binarizer:根据阈值将数据二值化（将特征值设置为0或1）
  - 大于阈值的值映射为1，而小于或等于阈值的值映射为0。
  - 默认阈值为0时，特征中所有的正值都映射到1
- sklearn.preprocessing.KBinsDiscretizer:可以将连续型变量划分为多个分类变量的类，能够将连续型变量排序后按顺序分箱后编码
  - 比如对年龄进行独热等宽分箱
    - "uniform"：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为(特征.max() - 特征.min())/(n_bins)
    - "quantile"：表示等位分箱，即每个特征中的每个箱内的样本数量都相同
    - "kmeans"：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同

## 2.4. 特征工程

- 维度：特征的数量

### 2.4.1. 特征选择

#### 2.4.1.1. 说明

- 原因：

  - 冗余：部分特征的相关度高，容易消耗计算性能
  - 噪声：部分特征对预测结果有负影响

- 定义：
  ```
  特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，
  特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯
  定比选择前小，毕竟我们只选择了其中的一部分特征。
  ```
- 主要方法:
  - **Filter(过滤式):VarianceThreshold**
    > variance:方差<br>
    > threshold:入口；门槛；开始；极限；临界值
  - **Embedded(嵌入式)：正则化、决策树**:后面算法时再讲
  - Wrapper(包裹式)：基本不用
- 其他方法：
  - **神经网络**，之后再说
  - 线性判别分析 LDA（基本不用）

#### 2.4.1.2. 过滤式

##### 2.4.1.2.1. 方差过滤：VarianceThreshold

- api:`sklearn.feature_selection.VarianceThreshold`
  - VarianceThreshold(threshold = 0.0)
    > 默认 threshold=0.0。要根据实际情况取值
    - 删除所有低方差特征
  - Variance.fit_transform(X,y)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：训练集差异低于 threshold 的特征将被删除。
    - 默认值是保留所有非零方差特征，即删除所有样本
    - 中具有相同值的特征。

```py
from sklearn.feature_selection import VarianceThreshold

def main():
    vt = VarianceThreshold()
    rdata = vt.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])
    print(rdata)

if __name__ == "__main__":
    main()
    """
    结果：
    [[2 0]
    [1 4]
    [1 1]]

    即删除了一样的列
    """
```

##### 2.4.1.2.2. 相关性过滤

###### 2.4.1.2.2.1. 卡方过滤

```
卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。
卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。
再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，
我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。
```

- api:
  ```py
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2
  
  #假设需要300个特征
  X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)
  X_fschi.shape
  ```

###### 2.4.1.2.2.2. 互信息过滤

> 了解 

```
互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。
互信息法不返回p值或F值类似的统计量，它返回 每个特征与目标之间的互信息量的估计 ，这个估计量在[0,1]之间取值，
为0则表示两个变量独立，为1则表示两个变量完全相关。
使用互信息法选取数据特征。
```
- api
  ```python
  from sklearn.feature_selection import mutual_info_classif as MIC
  result = MIC(X_fsvar,y)#互信息法
  ```


#### 2.4.1.3. 嵌入式

##### 2.4.1.3.1. 介绍和api

- 嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行

- 步骤：
  - 在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。
    - 这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，
    - 比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献
  - 我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。
- 其他：
  - 嵌入法引入了算法来挑选特征，因此其计算速度也会和应用的算法有很大的关系。
  - 如果采用计算量很大，计算缓慢的算法，嵌入法本身也会非常耗时耗力。
  - 并且，在选择完毕之后，我们还是需要自己来评估模型。

![](./image/2020-11-07-10-56-52.png)


- api:主要是threshold（阈值）的选择
  ```py
  class sklearn.feature_selection.SelectFromModel(
    estimator, # 使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚项的模型都可以使用
    threshold=None, # 特征重要性的阈值，重要性低于这个阈值的特征都将被删除
    prefit=False, # 默认False，判断是否将实例化后的模型直接传递给构造函数。
                  # 如果为True，则必须直接调用fit和transform，不能使用fit_transform，
                  # 并且SelectFromModel不能与cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。
    norm_order=1, # k可输入非零整数，正无穷，负无穷，默认值为1。在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数
    ax_features=None #在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置threshold = -np.inf
  )
  ```

##### 2.4.1.3.2. ※通过学习曲线进行选择(通用)※

- 通过宽泛的学习曲线进行选择
  ```py
  # RFC_:随机森林对象
  RFC_.fit(X,y).feature_importances_
  
  threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20) # 从0到最高特征重要性，linspace得到20个。作为阈值查看分数
  
  score = []
  for i in threshold:
      X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)  # 筛选出特征
      once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()  # 5折交叉验证
      score.append(once)  # 得到分数
  plt.plot(threshold,score)
  plt.show()
  ```

- 通过细化的学习曲线进行选择
  ```py
  score2 = []
  for i in np.linspace(0,0.00134,20):
      X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)
      once = cross_val_score(RFC_,X_embedded,y,cv=5).mean()
      score2.append(once)
  plt.figure(figsize=[20,5])
  plt.plot(np.linspace(0,0.00134,20),score2)
  plt.xticks(np.linspace(0,0.00134,20))
  plt.show()
  ```


#### 2.4.1.4. 包装式

- 包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，
  - 比如coef_属性或feature_importances_属性来完成特征选择
  - 从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征
  - 区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的
  - 包装法的效果是所有特征选择方法中最利于提升模型表现的，它可以使用很少的特征达到很优秀的效果。
  - 除此之外，在特征数目相同时，包装法和嵌入法的效果能够匹敌，不过它比嵌入法算得更见缓慢，所以也不适用于太大型的数据。相比之下，包装法是最能保证模型效果的特征选择方法。

![](./image/2020-11-07-11-15-06.png)

- 最典型的目标函数是递归特征消除法（Recursive feature elimination, 简写为RFE）。
  - 它是一种贪婪的优化算法，旨在找到性能最佳的特征子集。
  - 它反复创建模型，并在每次迭代时保留最佳特征或剔除最差特征，
  - 下一次迭代时，它会使用上一次建模中没有被选中的特征来构建下一个模型，直到所有特征都耗尽为止。
  - 然后，它根据自己保留或剔除特征的顺序来对特征进行排名，最终选出一个最佳子集。

- api:
  ass sklearn . feature _ selection . RFE ( estimator , n features to select = None , step = 1 , verbose

### 2.4.2. 特征降维 

#### 2.4.2.1. 概念

在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，
并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵


降维算法的计算量很大，运行比较缓慢，但无论如何，它们的功能无可替代


- 和特征选择区别：
  - 特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。
  - 而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。
  - 通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。
  - 以PCA为代表的降维算法因此是特征创造（feature creation，或feature construction）的一种。
  - 可以想见，PCA一般不适用于探索特征和标签之间的关系的模型（如线性回归），因为无法解释的新特征和标签之间的关系不具有意义

#### 2.4.2.2. PCA(主成分分析)


> 当特征成百上千时，就需要考虑了

```
PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。
降维完成之后，PCA找到的每个新特征向量就叫做“主成分”，而被丢弃的特征向量被认为信息量很少，这些信息很可能就是噪音。
```

> **原理自己查**

- 本质：PCA 是一种分析、简化数据集的技术。
- 目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
  > 比如 feature1 和 featrue2 间为线性关系，那么就可以删除其中一个
- 作用：可以削减回归分析或者聚类分析中特征的数量
- api:`sklearn.decomposition.PCA`
  - PCA
    - n_components=None
      > mle:缺点计算量大
      > 小数，0~1 的百分比，表示信息的损失量，一般为 0.9~0.95<br>
      > 整数：减少到的特征数量。**一般不用**
    - svd_solver
      > 在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选 "auto"是默认值
      - "auto"：基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生
      - "full"：从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况
      - "arpack"：可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。
      - "randomized"：适合特征矩阵巨大，计算量庞大的情况。

  - PCA.fit_transform(X)
    - X:numpy array 格式的数据[n_samples,n_features]
    - 返回值：转换后指定维度的 array
  - 属性：
    - explained_variance_，查看降维后每个新特征向量上所带的信息量大小 （可解释性方差的大小）
    - **explained_variance_ratio**，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比又叫做可解释方差贡献率
      > 降维前要看看
    - explained_variance_ratio_.sum() 保留信息总和
    - components_:新特征空间
  - PCA.inverse_transform()
    > 数据恢复

```py
from sklearn.decomposition import PCA

def main():
    pca = PCA(n_components=0.9)
    pca.fit([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])
    print(pca.explained_variance_ratio)  # 当有必要时，才进行转换。贡献度都差不多，就别降维了
    rdata = pca.transform();
    print(rdata)

if __name__ == "__main__":
    main()
```

#### 2.4.2.3. SVD

```
SVD使用奇异值分解来找出空间V，其中Σ也是一个对角矩阵，不过它对角线上的元素是奇异值，这也是SVD中用来衡量特征上的信息量的指标。
```

##### 2.4.2.3.1. 降维案例

[数据](https://www.kaggle.com/c/instacart-market-basket-analysis/data)



## 2.5. 数据集

> scikit-learn有内置数据集

- 可将数据集划分为划分为：
  - 训练数据集
  - 测试数据集

> ![](./image/data_set.jpg)

- api:
  - 获取：
    - sklearn.datasets
      > 加载获取流行数据集
      - `datasets.load_*()`
        > 获取小规模数据集，数据包含在datasets里
      - `datasets.fetch_*(data_home=None)`
        > 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/<br>
        > subset: 'train'或者'test','all'，可选，选择要加载的数据集. 训练集的“训练”，测试集的“测试”，两者的“全部”
    - 数据格式；
      - load*和fetch*返回的数据类型datasets.base.Bunch(字典格式)
      - data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
      - target：目标值数组，是 n_samples 的一维 numpy.ndarray 数组
      - DESCR：数据描述
      - feature_names：特征名,新闻数据，手写数字、回归数据集没有
      - target_names：标签名,回归数据集没有
  - 划分：`sklearn.model_selection.train_test_split(*arrays, **options)`
    - x:  数据集的特征值
    - y:  数据集的标签值
    - test_size      测试集的大小，一般为float
    - random_state        随机数种子,不同的种子会造成不同的随机 采样结果。相同的种子采样结果相同。
    - return  训练集特征值，测试集特征值，训练标签，测试标签 (默认随机取)	
  - 清楚：
    - `datasets.clear_data_home(data_home=None)`
      > 清除目录下的数据

## 2.6. 转换器与估计器

### 2.6.1. 转换器

> 特征工程，数据处理所用api

- 通用：
  - fit():输入数据，但不进行处理。但会计算些平均值和方差，或者词列表等前提操作，这些值会存储到处理对象中，比如StandScaler
    > **所以不能fit完一个数据，再transform另一个数据，会乱套**
  - transform():数据处理。根据fit()得到的结果，再进行处理
  - fit_transform():输入数据，直接转换。**一般使用这个即可，上面两个基本不用**
    > 等于 fit()+transform()


### 2.6.2. 估计器

- 定义：在sklearn中，估计器(estimator)是一个重要的角色，分类器和回归器都属于estimator，是一类实现了算法的API
  > 实现算法api

- 用于分类的估计器
  - sklearn.neighbors	k-近邻算法
  - sklearn.naive_bayes      贝叶斯
  - sklearn.linear_model.LogisticRegression     逻辑回归
- 用于回归的估计器
  - sklearn.linear_model.LinearRegression     线性回归
  - sklearn.linear_model.Ridge      岭回归 

- 通用：
  - fit()：进行训练
  - prediec():预估结果
  - score():准确率

## 2.7. api总结

### 2.7.1. api总结

- 数据抽取：
  - 字典类型数据抽取:`sklearn.feature_extraction.DictVectorizer`
  - 文本特征数据抽取:
    - count:`sklearn.feature_extraction.text.CountVectorizer`
    - tf-idf:`sklearn.feature_extraction.text.TfidfVectorizer`
- 数据预处理
  - 归一化：`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)`
  - 标准化:`sklearn.preprocessing.StandardScaler`
  - 缺失值:`sklearn.preprocessing.Imputer`
- 数据降维
  - 特征选择:`sklearn.feature_selection.VarianceThreshold`
  - PCA:`sklearn.decomposition.PCA`


### 2.7.2. api大致使用流程

![](./image/sklearn_api_line.jpg)

# 3. 基本算法

> 此处只会讲解基本概念和api使用。算法实现不做总结

## 3.1. 监督学习

### 3.1.1. 分类

#### 3.1.1.1. k-近邻算法

> 基本不咋用

- 看《算法图解》
- 需要做标准化处理
- api:sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
  - n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数 
  - algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)

- 优点：
  - 简单，易于理解，易于实现，无需估计参数，无需训练
- 缺点：
  - 懒惰算法，对测试样本分类时的计算量大，内存开销大
  - 必须指定K值，K值选择不当则分类精度不能保证

  ```py
  import numpy as np
  import pandas as pd
  from sklearn import neighbors
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler

  # 获取数据
  iris_data = load_iris()

  # 数据描述
  print(iris_data.DESCR)

  # 查看部分数据
  print(iris_data.data[:5, :])

  # 归一化
  mm = MinMaxScaler()
  x_data = mm.fit_transform(iris_data.data)

  # 权重处理(尝试)
  # x_data[:,[0]] = x_data[:,[0]]/(1-0.7826)
  # x_data[:,[1]] = x_data[:,[0]]/(1+0.4194)
  # x_data[:,[2]] = x_data[:,[0]]/(1-0.9490)

  # 数据划分
  x_train, x_test, y_train, y_test = train_test_split(
      x_data, iris_data.target, test_size=0.25)

  # 实例化knn对象
  knn = neighbors.KNeighborsClassifier(n_neighbors=5,)

  # 尝试添加权重
  # knn = neighbors.KNeighborsClassifier(n_neighbors=5, metric="wminkowski", metric_params={
  #                                      "w": [0.7826, -0.4194, 0.9490, 0.9565]})

  # 加载数据
  knn.fit(x_train, y_train)

  # 预测值
  knn.predict(x_test)

  # 准确率
  knn.score(x_test, y_test)
  ```


#### 3.1.1.2. 朴素贝叶斯分类

[讲解](https://zhuanlan.zhihu.com/p/26262151)

  ```py
  import numpy as np
  import pandas as pd
  from sklearn.datasets import fetch_20newsgroups
  from sklearn.model_selection import train_test_split
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.naive_bayes import MultinomialNB

  news = fetch_20newsgroups(subset='all')
  x_train,x_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25)

  # 数据处理
  tf = TfidfVectorizer()
  x_train = tf.fit_transform(x_train)
  x_test = tf.transform(x_test) # 注意，这里用trainsform，没有fit，要以train为标准

  # 创建朴素贝叶斯api对象
  mlt = MultinomialNB(alpha=1.0)

  # 进行训练
  mlt.fit(x_train,y_train)

  # 预测值
  mlt.predict(x_test)

  # 准确率
  mlt.score(x_test,y_test)
  ```

- 特点
  - 无法传入参数进行调整，训练集影响大
  - 不需要调参
  - 训练集误差会大大影响结果

- 优点：
  - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
  - 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
  - 分类准确度高，速度快
- 缺点：
  - 由于使用了样本属性独立性的假设，所以如果样本属性有关联时 其效果不好

> 神经网络的效果要比朴素贝叶斯要好

#### 3.1.1.3. 分类模型评估

> 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

![](./image/confusion_matrix.jpg)


- 混淆矩阵
  - 精确率:预测结果为正例样本中真实为正例的比例（查得准）
    > ![](./image/precision.jpg)
  - 召回率:真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）
    > ![](./image/recall.jpg)
  - 其他分类标准，F1-score，反映了模型的稳健型
    > ![](./image/f1-score.jpg)

- api:`sklearn.metrics.classification_report `
  - y_true：真实目标值 
  - y_pred：估计器预测目标值 
  - target_names：目标类别名称 
  - return：每个类别精确率与召回率

#### 3.1.1.4. 模型选择与调优

##### 3.1.1.4.1. 交叉验证

> 一般和网格搜索搭配。

- 目的：交叉验证：为了让被评估的模型更加准确可信
- 过程：
  ```
  交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：
  将数据分成5份，其中一份作为验证集。
  然后经过5次(组)的测试，每次都更换不同的验证集。
  即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。
  ```
  > ![](./image/cross_validation.jpg)

- api:
  - `sklearn.model_selection.cross_val_score(estimator,data,target,cv,[scoring])`:获取cv折交叉验证的分数
  - `sklearn.model_selection.cross_val_predict(estimator,data,target,cv,[scoring])`:获取cv折交叉验证的分数
  ```python
  # 例
  regressor = DecisionTreeRegressor(random_state=0)
  score_arr = cross_val_score(regressor, boston.data, boston.target, cv=10,
                    scoring = "neg_mean_squared_error")
  ```


##### 3.1.1.4.2. 网格搜索

> 一般与交叉验证搭配

- 说明：
  ```
  通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），
  这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组
  合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建
  立模型。
  ```
- 过程：
  - 就是一个穷举。每个参数值都用交叉验证得到模型
  - 参数间相互组合
  - 选取最好的模型

##### 3.1.1.4.3. 两者搭配

- api:`sklearn.model_selection.GridSearchCV`
  > 对估计器的指定参数值进行详尽搜索
  - 参数
    - estimator：估计器对象
    - param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
    - cv：指定几折交叉验证
    - fit：输入训练数据
    - score：准确率
  - 结果分析：
    - best_score_:在交叉验证中测试的最好结果
    - best_estimator_：最好的参数模型
    - cv_results_:每次交叉验证后的测试集准确率结果和训练集准确率结果


##### 3.1.1.4.4. 学习曲线

- 查看某个参数不同取值的score变化
- 例：随机森林的 n_estimators


#### 3.1.1.5. 决策树与随机森林

##### 3.1.1.5.1. 决策树

> 企业过程中使用较多

- 原理：信息论基础
  - 信息熵的计算
  - 条件熵的计算
    > ![](./image/Information_entropy.jpg)
  - 信息增益的计算
    > 信息增益：当得知一个特征条件之后，减少的信息熵的大小
    > ![](./image/Information_gain.jpg)

- 信息增益大的作为最开始的分类。
  > 信息增益越大，越有可能得出结果。大的放前面有利于不进行多余的判断
  > ![](./image/Decision_tree.jpg)

- 算法(了解)：
  > 这是在sklearn中可以选择划分的原则
  > 对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较好
  - ID3
    - 信息增益 最大的准则。
  - C4.5
    - 信息增益比 最大的准则
  - CART 
    - 回归树: 平方误差 最小 。
      - api:sklearn.tree.DecisionTreeRegressor
    - 分类树: 基尼系数   最小的准则 
- api:`sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)`
  - 使用
    > 决策树分类器<br>
    > 超参数放在随机森林
    - decision_path:返回决策树的路径
    ```python
    DecisionTreeClassifier(
    *,
    criterion='gini', # 默认是’gini’系数，也可以选择信息增益的熵’entropy’
    splitter='best',
    max_depth=None, # 树的深度最大大小
    min_samples_split=2, # 样本数大于两个时才会分叉
    min_samples_leaf=1, # 样本数大于1个时，节点才会被留下
    min_weight_fraction_leaf=0.0,
    max_features=None, # 限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃
    random_state=None, # 随机数种子
    max_leaf_nodes=None,
    min_impurity_decrease=0.0, # 比较难调，不推荐。 限制信息增益的大小，信息增益小于设定数值的分枝不会发生
    min_impurity_split=None,
    class_weight=None,
    presort='deprecated',
    ccp_alpha=0.0,
    )
    ```
  - 查看feature的重要性:`feature_importances_`
  - 查看学习曲线（其实就是调整max_depth，查看不同max_depth的正确率）
  - 模型保存
    - 1、sklearn.tree.export_graphviz() 该函数能够导出DOT格式
      ```
      tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[‘’,’’]) 
      ```
    - 2、工具:(能够将dot文件转换为pdf、png)
      ```
      安装graphviz
      ubuntu:sudo apt-get install graphviz                    Mac:brew install graphviz 

      运行命令
      然后我们运行这个命令
      $ dot -Tpng tree.dot -o tree.png
      ```

- 示例：
  - 泰坦尼克号存活率

- 优缺点：
  - 优点：
    - 简单的理解和解释，树木可视化。
    - 需要很少的数据准备，其他技术通常需要数据归一化，
  - 缺点：
    - 决策树学习者可以创建不能很好地推广数据的过于复杂的树， 这被称为过拟合。 
    - 决策树可能不稳定，因为数据的小变化可能会导致完全不同的树被生成 
  - 改进：
    - 减枝cart算法
      - 创建api对象时设置参数：
        ```python
        min_samples_split=2, # 样本数大于两个时才会分叉
        min_samples_leaf=1, # 样本数大于1个时，节点才会被留下
        ```
    - 随机森林


##### 3.1.1.5.2. 随机森林

- 集成学习方法：
  ```
  集成学习通过建立几个模型组合的来解决单一预测问题。
  它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。
  这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。
  ```
  - 装袋法（Bagging）:构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。代表模型就是随机森林
  - 提升法（Boosting）:，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。
  - stacking
  > ![ensemble-learning](./image/ensemble-learning.png) 

- 随机森林：
  - 在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的**众数**而定。

- 随机森林创建过程：
  1. 单棵树创建过程
    1. 随机在M个特征中选出m个特征,m应远小于M
    2. 随机从N个样本中选择一个样本，重复N次。随机放回抽样(bootstrap抽样),样本有可能重复
    3. 使用抽取的样本，训练模型,并用未抽到的样本作预测，评估其误差。
  2. 重复操作，建立指定数量的决策树

- 问题：
  - 为什么要随机抽样训练集？　　
    ```
    如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的 
    ```
  - 为什么要有放回地抽样？
    ```
    如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。
    ```
  - 什么是袋外集
    ```
    bootstrap参数默认True，代表采用这种有放回的随机抽样技术。
    当n足够大时，这个概率收敛于1-(1/e)，约等于0.632。
    因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。
    除了我们最开始就划分好的测试集之外，这些数据也可以被用来作为集成算法的测试集

    在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。
    当然，这也不是绝对的，当n和n_estimators都不够大的时候， 很可能就没有数据掉落在袋外，自然也就无法使用oob数据来测试模型了。
    如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True。
    ```

- api:
  ```python
  class sklearn.ensemble.RandomForestClassifier(
    n_estimators=10, #  森林里的树木数量。推荐：120,200,300,500,800,1200
                    # n_estimators是森林中树木的数量，即基评估器的数量。
                    # 这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。
                    # 但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，
                    # 并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。
    *,
    criterion=’gini’, # 分割特征的测量方法
    max_depth=None, #可选（默认=无）树的最大深度  
    min_samples_split=2,
    min_samples_leaf=1,
    min_weight_fraction_leaf=0.0,
    max_features='auto', # 每棵决策树选取的最大的特征数量（m的最大值）
      # 查文档即可
      # - If int, then consider `max_features` features at each split.
      # - If float, then `max_features` is a fraction and
      #   `int(max_features * n_features)` features are considered at each
      #   split.
      # - If "auto", then `max_features=sqrt(n_features)`.
      # - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
      # - If "log2", then `max_features=log2(n_features)`.
      # - If None, then `max_features=n_features`.
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_impurity_split=None,
    bootstrap=True, # 是否在构建树时使用放回抽样 
    oob_score=False, # 是否使用袋外数据
    n_jobs=None,
    random_state=None, 
          # random_state相同时，抽样会相同，生成的森林也相同
          # 随机森林的重要属性之一：estimators，查看森林中树的状况
          # rfc.estimators_[0].random_state
          # 随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树 
          # 每棵树random_state不同
          # 通过rfc.estimators_[0].random_state.random_state查看
    verbose=0,
    warm_start=False,
    class_weight=None,
    ccp_alpha=0.0,
    max_samples=None,
  )


  # 查看各个feature的重要度
  [*zip(wine.feature_names ,rfc.feature_importances_)]
  #每一个样本对应标签的概率
  rfc.predict_proba(Xtest)
  ```

- 优点：
  - 在当前所有算法中，具有极好的准确率
  - 能够有效地运行在大数据集上
  - 能够处理具有高维特征的输入样本，而且不需要降维
  - 能够评估各个特征在分类问题上的重要性
  - 对于缺省值问题也能够获得很好得结果


- 随机森林回归系
  - api: sklearn.ensemble.RandomForestRegressor

### 3.1.2. 回归

#### 3.1.2.1. 线性回归

> 看机器学习文档

- api:
  - 正规方程：sklearn.linear_model.LinearRegression()
    - 普通最小二乘法线性回归
    - coef_：回归系数
  - 梯度下降：sklearn.linear_model.SGDRegressor( )
    - 通过使用SGD最小化线性模型
    - coef_：回归系数
- 回归评估API:sklearn.metrics.mean_squared_error
  > 均方误差回归损失
  - y_true:真实值
  - y_pred:预测值
    > 注：真实值，预测值为标准化之前的值
  - return:浮点数结果

- 两种回归比较：
  > ![](./image/regression.jpg)
- LinearRegression与SGDRegressor评估
  - 特点：线性回归器是最为简单、易用的回归模型。
  - 从某种程度上限制了使用，尽管如此，在不知道特征之 间关系的前提下，我们仍然使用线性回归器作为大多数 系统的首要选择。
    - 小规模数据：LinearRegression(不能解决拟合问题)以及其它
    - 大规模数据：SGDRegressor
  
- 线性回归问题：
  - 欠拟合
  - 过拟合
  - 解决：
    - 过滤式：选择地方差特征
    - 嵌入式：决策树，神经网络，正则化（岭回归）

#### 3.1.2.2. 岭回归

- L2正则化：岭回归，带有正则化的线性回归
- api:sklearn.linear_model.Ridge
  > 具有l2正则化的线性最小二乘法
  - alpha(0~1):正则化力度
    > **面试问题：**回归过拟合调优方式？<br>
    > 正则化，岭回归，调整alpha参数大小
  - coef_:回归系数

> 岭回归：回归得到的回归系数更符合实际，更可靠。另外，能让 估计参数的波动范围变小，变的更稳定。在存在病态数据偏多的研 究中有较大的实用价值。

#### 3.1.2.3. sklearn模型的保存与加载

> 文件格式：pkl
- 保存：joblib.dump(rf,"test.pkl")
- 加载：estimator = joblib.load("test.pkl")

#### 3.1.2.4. 逻辑回归

> 用于二分类

- 应用：
  - 广告点击率 
  - 判断用户的性别
  - 预测用户是否会购买给定的商品类 
  - 判断一条评论是正面的还是负面的
- 优点：适合需要得到一个分类概率的场景
- 缺点：当特征空间很大时，逻辑回归的性能不是很好 （看硬件能力）
- 原理：sigmoid函数，将变量映射到0-1之间
  > ![](./image/sigmoid.jpg)

- api: sklearn.linear_model.LogisticRegression
  ```py
  LogisticRegression(
    penalty='l2',
    *,
    dual=False,
    tol=0.0001,
    C=1.0, # 学习率
    fit_intercept=True,
    intercept_scaling=1,
    class_weight=None,
    random_state=None,
    solver='lbfgs',
    max_iter=100,
    multi_class='auto',
    verbose=0,
    warm_start=False,
    n_jobs=None,
    l1_ratio=None,
  )
  ```
- 多分类问题：softmax-方法，将在后面神经网络算法中介绍

- 生成模型和判别模型：
  - 判别模型:如朴素贝叶斯算法，隐马可夫模型，一开始需要从数据中求得一些概率
  - 生成模型:如逻辑回归，决策树等等，不需要事先处理数据

#### 3.1.2.5. 神经网络

### 3.1.3. 标注

#### 3.1.3.1. 隐马尔可夫模型

## 3.2. 无监督学习

### 3.2.1. 聚类

#### 3.2.1.1. k-means

- 无监督学习：只有特征值，没有目标值
- k-means:聚类
- 过程：
  ![](./image/k-means-1.jpg)
- api:`sklearn.cluster.KMeans`
  - n_clusters:开始的聚类中心数量
  - init:初始化方法，默认为'k-means ++'
  - labels_:默认标记的类型，可以和真实值比较（不是值比较）
- 评估标准：
  > ![](./image/k-means-2.jpg)
  - 如果〖𝑠𝑐〗_𝑖 小于0，说明𝑎_𝑖 的平均距离大于最近的其他簇。 聚类效果不好
  - 如果〖𝑠𝑐〗_𝑖 越大，说明𝑎_𝑖 的平均距离小于最近的其他簇。 聚类效果好
  - 轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优 
- 评估api： `sklearn.metrics.silhouette_score`
  > 计算所有样本的平均轮廓系数
  - X：特征值
  - labels：被聚类标记的目标值

- 缺点：
  - 容易收敛到局部最优解(多次聚类)
  - 需要预先设定簇的数量(k-means++解决)


